{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#real data with real Feedback\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gurobipy\n",
    "fixedItemSet=np.array([[1.00000e+00, 4.21669e-01, 1.10000e-05, 1.09020e-02, 3.09585e-01,\n",
    "        2.57833e-01],\n",
    "       [1.00000e+00, 2.95442e-01, 1.40000e-05, 1.35191e-01, 2.92304e-01,\n",
    "        2.77050e-01],\n",
    "       [1.00000e+00, 4.38513e-01, 3.00000e-06, 3.07140e-02, 3.84494e-01,\n",
    "        1.46277e-01],\n",
    "       [1.00000e+00, 3.31830e-01, 2.20000e-05, 1.99040e-02, 4.40390e-01,\n",
    "        2.07855e-01],\n",
    "       [1.00000e+00, 3.06008e-01, 4.50000e-04, 7.70480e-02, 2.30439e-01,\n",
    "        3.86055e-01],\n",
    "       [1.00000e+00, 3.13277e-01, 1.25000e-04, 1.84130e-02, 4.10555e-01,\n",
    "        2.57630e-01],\n",
    "       [1.00000e+00, 3.06008e-01, 4.50000e-04, 7.70480e-02, 2.30439e-01,\n",
    "        3.86055e-01],\n",
    "       [1.00000e+00, 2.49086e-01, 1.00900e-03, 5.14682e-01, 6.77320e-02,\n",
    "        1.67491e-01],\n",
    "       [1.00000e+00, 2.77121e-01, 1.31000e-04, 3.81530e-02, 3.35835e-01,\n",
    "        3.48760e-01],\n",
    "       [1.00000e+00, 3.06008e-01, 4.50000e-04, 7.70480e-02, 2.30439e-01,\n",
    "        3.86055e-01],\n",
    "       [1.00000e+00, 3.75829e-01, 2.50000e-05, 3.30410e-02, 3.49637e-01,\n",
    "        2.41468e-01],\n",
    "       [1.00000e+00, 2.64355e-01, 1.20000e-05, 3.73930e-02, 4.20649e-01,\n",
    "        2.77591e-01],\n",
    "       [1.00000e+00, 2.64355e-01, 1.20000e-05, 3.73930e-02, 4.20649e-01,\n",
    "        2.77591e-01],\n",
    "       [1.00000e+00, 3.06008e-01, 4.50000e-04, 7.70480e-02, 2.30439e-01,\n",
    "        3.86055e-01],\n",
    "       [1.00000e+00, 2.87909e-01, 2.50000e-05, 8.98300e-03, 5.11333e-01,\n",
    "        1.91751e-01],\n",
    "       [1.00000e+00, 3.81149e-01, 1.29000e-04, 6.00380e-02, 2.69129e-01,\n",
    "        2.89554e-01],\n",
    "       [1.00000e+00, 2.97322e-01, 2.50000e-05, 3.49510e-02, 4.13566e-01,\n",
    "        2.54137e-01],\n",
    "       [1.00000e+00, 2.11406e-01, 3.60000e-05, 2.77300e-03, 5.69886e-01,\n",
    "        2.15900e-01],\n",
    "       [1.00000e+00, 2.97750e-01, 1.30000e-05, 1.16030e-02, 5.12182e-01,\n",
    "        1.78452e-01],\n",
    "       [1.00000e+00, 2.81649e-01, 1.73000e-04, 1.95994e-01, 1.51003e-01,\n",
    "        3.71182e-01]])\n",
    "userclick=np.load(r'./Yahoo!/userclick.npy',allow_pickle=True)\n",
    "user=np.argmax([len(userclick[i]) for i in range(len(userclick))])\n",
    "def hwithoutconstraints(action):\n",
    "    t=model.iteration\n",
    "    if type(action)==torch.Tensor:\n",
    "        action=action.detach().numpy()\n",
    "    tmp=np.argwhere(np.array(action)>0.5)\n",
    "    items=tmp.reshape(len(tmp))\n",
    "    if t<args.card:\n",
    "        itemlist=userclick[user][:args.card]\n",
    "    else:\n",
    "        itemlist=userclick[user][(t-args.card):min((t+args.card),len(userclick[user]))]\n",
    "    cnt2=0\n",
    "    for i in items:\n",
    "        cnt2+=( itemlist.count(i))\n",
    "    return cnt2/(args.card)\n",
    "\n",
    "import argparse\n",
    "from copy import deepcopy\n",
    "import torch\n",
    "import gym\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "import argparse\n",
    "import sys\n",
    "\n",
    "import copy\n",
    "import numpy as np\n",
    "from numpy.random import permutation\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "sys.path.insert(0,'/root/gurobi903/linux64/lib/python3.6_utf32/gurobipy')\n",
    "\n",
    "class para:\n",
    "    def __init__(self):\n",
    "        self.batch_size=16\n",
    "        self.wmc=10\n",
    "        self.solDim=20\n",
    "        self.card=5\n",
    "        self.test_batch_size=1000\n",
    "        self.epochs=10\n",
    "        self.lr=0.1\n",
    "        self.momentum=0.5\n",
    "        self.no_cuda='store_true'\n",
    "        self.seed=1\n",
    "        self.log_interval=10\n",
    "        self.hidden_dim=10\n",
    "args=para()\n",
    "args5=para()\n",
    "thre=0.4\n",
    "legalList={}\n",
    "for i in range(20):\n",
    "    legalList[i]=[]\n",
    "for i in range(20):\n",
    "    for j in range(20):\n",
    "        if i!=j and np.linalg.norm(fixedItemSet[i]-fixedItemSet[j])<thre:\n",
    "            legalList[i].append(j)\n",
    "def checkFea(x):\n",
    "    cnt=0\n",
    "    cntT=0\n",
    "    for i in range(len(legalList.keys())):\n",
    "        for j in legalList[i]:\n",
    "            cntT+=1\n",
    "            if x[j]+x[i]>1:\n",
    "                cnt+=1\n",
    "    return cnt/cntT/2+abs(sum(x)-args.card)/args.card/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def solver(theta,MODEL):\n",
    "\n",
    "    import gurobipy\n",
    "    import time\n",
    "    s=time.time()\n",
    "\n",
    "    variables=[]\n",
    "    for i in range(args.solDim):\n",
    "        variables.append(MODEL.addVar(vtype=gurobipy.GRB.BINARY, name='x'+'i'))\n",
    "\n",
    "    MODEL.update()\n",
    "\n",
    "\n",
    "    MODEL.setObjective(np.array(variables).dot(theta), sense=gurobipy.GRB.MAXIMIZE)\n",
    "\n",
    "\n",
    "    cnt=0\n",
    "    for i in range(args.solDim):\n",
    "        if len(legalList)>0:\n",
    "            for j in legalList[i]:\n",
    "                MODEL.addConstr(-variables[i]-variables[j] >= -1, name=str(cnt))\n",
    "                cnt+=1\n",
    "    MODEL.addConstr(sum(variables) == args.card, name=str(cnt))\n",
    "    f=time.time()\n",
    "\n",
    "    MODEL.optimize()\n",
    "    return np.array(MODEL.x)\n",
    "def solver_quad(Q,MODEL):\n",
    "\n",
    "    import gurobipy\n",
    "    import time\n",
    "    s=time.time()\n",
    "\n",
    "    variables=[]\n",
    "    for i in range(args.solDim):\n",
    "        variables.append(MODEL.addVar(vtype=gurobipy.GRB.BINARY, name='x'+'i'))\n",
    "    MODEL.update()\n",
    "\n",
    "    MODEL.setObjective(np.array(variables).dot(Q).dot(np.array(variables)), sense=gurobipy.GRB.MAXIMIZE)\n",
    "\n",
    "    cnt=0\n",
    "    for i in range(args.solDim):\n",
    "        if len(legalList)>0:\n",
    "            for j in legalList[i]:\n",
    "                MODEL.addConstr(-variables[i]-variables[j] >= -1, name=str(cnt))\n",
    "                cnt+=1\n",
    "    MODEL.addConstr(sum(variables) == args.card, name=str(cnt))\n",
    "    f=time.time()\n",
    "\n",
    "    \n",
    "    MODEL.optimize()\n",
    "    return np.array(MODEL.x)\n",
    "def solver_mixed(Q,a,MODEL):\n",
    "    import gurobipy\n",
    "    import time\n",
    "    s=time.time()\n",
    "\n",
    "    variables=[]\n",
    "    for i in range(args.solDim):\n",
    "        variables.append(MODEL.addVar(vtype=gurobipy.GRB.BINARY, name='x'+'i'))\n",
    "\n",
    "    MODEL.update()\n",
    "\n",
    "    MODEL.setObjective(np.array(variables).dot(Q).dot(np.array(variables))+(np.array(variables).dot(a))*(np.array(variables).dot(a)), sense=gurobipy.GRB.MAXIMIZE)\n",
    "\n",
    "    cnt=0\n",
    "    for i in range(args.solDim):\n",
    "        if len(legalList)>0:\n",
    "            for j in legalList[i]:\n",
    "                MODEL.addConstr(-variables[i]-variables[j] >= -1, name=str(cnt))\n",
    "                cnt+=1\n",
    "    MODEL.addConstr(sum(variables) == args.card, name=str(cnt))\n",
    "    f=time.time()\n",
    "    \n",
    "    MODEL.optimize()\n",
    "    return np.array(MODEL.x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#g2a\n",
    "criterion = nn.MSELoss()\n",
    "class g2apara:\n",
    "    def __init__(self):\n",
    "        self.rnn_hidden_dim=8\n",
    "        self.n_agents=args.solDim\n",
    "        self.attention_dim=8\n",
    "        self.cuda=False\n",
    "        self.n_actions=2\n",
    "        self.hard=False\n",
    "args3=g2apara()\n",
    "hids=torch.from_numpy(np.random.random([args.solDim,args3.rnn_hidden_dim])).float()\n",
    "\n",
    "def fanin_init(size, fanin=None):\n",
    "    fanin = fanin or size[0]\n",
    "    v = 1. / np.sqrt(fanin)\n",
    "    return torch.Tensor(size).uniform_(-v, v)\n",
    "\n",
    "class Actorg2a(nn.Module):\n",
    "    def __init__(self, input_shape, args):\n",
    "        super(Actorg2a, self).__init__()\n",
    "\n",
    "        # Encoding\n",
    "        self.encoding = nn.Linear(input_shape, args.rnn_hidden_dim) \n",
    "        self.h = nn.GRUCell(args.rnn_hidden_dim, args.rnn_hidden_dim) \n",
    "\n",
    "        self.hard_bi_GRU = nn.GRU(args.rnn_hidden_dim * 2, args.rnn_hidden_dim, bidirectional=True)\n",
    "        self.hard_encoding = nn.Linear(args.rnn_hidden_dim * 2, 2)  \n",
    "\n",
    "        # Soft\n",
    "        self.q = nn.Linear(args.rnn_hidden_dim, args.attention_dim, bias=False)\n",
    "        self.k = nn.Linear(args.rnn_hidden_dim, args.attention_dim, bias=False)\n",
    "        self.v = nn.Linear(args.rnn_hidden_dim, args.attention_dim)\n",
    "\n",
    "        self.decoding = nn.Linear(args.rnn_hidden_dim + args.attention_dim, args.n_actions)\n",
    "        self.args = args\n",
    "        self.input_shape = input_shape\n",
    "        self.softmax=F.softmax\n",
    "\n",
    "    def forward(self,state, obs=torch.diag(torch.ones(args.solDim)), hidden_state=hids):\n",
    "        size = obs.shape[0]  # batch_size * n_agents\n",
    "        obs_encoding = F.relu(self.encoding(obs))\n",
    "        h_in = hidden_state.reshape(-1, self.args.rnn_hidden_dim)\n",
    "\n",
    "        h_out = self.h(obs_encoding, h_in)  # (batch_size * n_agents, args.rnn_hidden_dim)\n",
    "\n",
    "        if self.args.hard:\n",
    "            h = h_out.reshape(-1, self.args.n_agents, self.args.rnn_hidden_dim) \n",
    "            input_hard = []\n",
    "            for i in range(self.args.n_agents):\n",
    "                h_i = h[:, i]  # (batch_size, rnn_hidden_dim)\n",
    "                h_hard_i = []\n",
    "                for j in range(self.args.n_agents):  \n",
    "                    if j != i:\n",
    "                        h_hard_i.append(torch.cat([h_i, h[:, j]], dim=-1))\n",
    "                \n",
    "                h_hard_i = torch.stack(h_hard_i, dim=0)\n",
    "                input_hard.append(h_hard_i)\n",
    "                \n",
    "            input_hard = torch.stack(input_hard, dim=-2)\n",
    "            \n",
    "            input_hard = input_hard.view(self.args.n_agents - 1, -1, self.args.rnn_hidden_dim * 2)\n",
    "\n",
    "            h_hard = torch.zeros((2 * 1, size, self.args.rnn_hidden_dim))  \n",
    "            if self.args.cuda:\n",
    "                h_hard = h_hard.cuda()\n",
    "            h_hard, _ = self.hard_bi_GRU(input_hard, h_hard)  # (n_agents - 1,batch_size * n_agents,rnn_hidden_dim * 2)\n",
    "            h_hard = h_hard.permute(1, 0, 2)  # (batch_size * n_agents, n_agents - 1, rnn_hidden_dim * 2)\n",
    "            h_hard = h_hard.reshape(-1, self.args.rnn_hidden_dim * 2)  # (batch_size * n_agents * (n_agents - 1), rnn_hidden_dim * 2)\n",
    "\n",
    "            \n",
    "            hard_weights = self.hard_encoding(h_hard)\n",
    "            hard_weights = F.gumbel_softmax(hard_weights, tau=0.01)\n",
    "            # print(hard_weights)\n",
    "            hard_weights = hard_weights[:, 1].view(-1, self.args.n_agents, 1, self.args.n_agents - 1)\n",
    "            hard_weights = hard_weights.permute(1, 0, 2, 3)\n",
    "\n",
    "        else:\n",
    "            hard_weights = torch.ones((self.args.n_agents, size // self.args.n_agents, 1, self.args.n_agents - 1))\n",
    "            if self.args.cuda:\n",
    "                hard_weights = hard_weights.cuda()\n",
    "\n",
    "        # Soft Attention\n",
    "        q = self.q(h_out).reshape(-1, self.args.n_agents, self.args.attention_dim)  # (batch_size, n_agents, args.attention_dim)\n",
    "        k = self.k(h_out).reshape(-1, self.args.n_agents, self.args.attention_dim)  # (batch_size, n_agents, args.attention_dim)\n",
    "        v = F.relu(self.v(h_out)).reshape(-1, self.args.n_agents, self.args.attention_dim)  # (batch_size, n_agents, args.attention_dim)\n",
    "        x = []\n",
    "        for i in range(self.args.n_agents):\n",
    "            q_i = q[:, i].view(-1, 1, self.args.attention_dim)  \n",
    "            k_i = [k[:, j] for j in range(self.args.n_agents) if j != i] \n",
    "            v_i = [v[:, j] for j in range(self.args.n_agents) if j != i] \n",
    "\n",
    "            k_i = torch.stack(k_i, dim=0)  # (n_agents - 1, batch_size, args.attention_dim)\n",
    "            k_i = k_i.permute(1, 2, 0)  \n",
    "            v_i = torch.stack(v_i, dim=0)\n",
    "            v_i = v_i.permute(1, 2, 0)\n",
    "\n",
    "            # (batch_size, 1, attention_dim) * (batch_size, attention_dim，n_agents - 1) = (batch_size, 1，n_agents - 1)\n",
    "            score = torch.matmul(q_i, k_i)\n",
    "\n",
    "            \n",
    "            scaled_score = score / np.sqrt(self.args.attention_dim)\n",
    "\n",
    "            \n",
    "            soft_weight = F.softmax(scaled_score, dim=-1)  # (batch_size，1, n_agents - 1)\n",
    "\n",
    "            \n",
    "            x_i = (v_i * soft_weight * hard_weights[i]).sum(dim=-1)\n",
    "            x.append(x_i)\n",
    "\n",
    "            \n",
    "        x = torch.stack(x, dim=1).reshape(-1, self.args.attention_dim)  # (batch_size * n_agents, args.attention_dim)\n",
    "        final_input = torch.cat([h_out, x], dim=-1)\n",
    "        output = self.softmax(1*self.decoding(final_input))\n",
    "        return torch.cat(([i[0].view(1) for i in output]),0)\n",
    "\n",
    "class Criticg2a(nn.Module):\n",
    "    def __init__(self, nb_actions, hidden1=400, hidden2=300, init_w=3e-3):\n",
    "        super(Criticg2a, self).__init__()\n",
    "        self.fc1 = nn.Linear(nb_actions, hidden1)\n",
    "        self.fc2 = nn.Linear(hidden1, hidden2)\n",
    "        self.fc3 = nn.Linear(hidden2, 1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.init_weights(init_w)\n",
    "\n",
    "    def init_weights(self, init_w):\n",
    "        self.fc1.weight.data = fanin_init(self.fc1.weight.data.size())\n",
    "        self.fc2.weight.data = fanin_init(self.fc2.weight.data.size())\n",
    "        self.fc3.weight.data.uniform_(-init_w, init_w)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc3(out)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Q= np.load('Q24.npy')\n",
    "rewards=[]\n",
    "constraints=[]\n",
    "import gym\n",
    "\n",
    "# https://github.com/openai/gym/blob/master/gym/core.py\n",
    "class Normalizedenv2(gym.ActionWrapper):\n",
    "    \"\"\" Wrap action \"\"\"\n",
    "\n",
    "    def action(self, action):\n",
    "        act_k = (self.action_space.high - self.action_space.low)/ 2.\n",
    "        act_b = (self.action_space.high + self.action_space.low)/ 2.\n",
    "        return act_k * action + act_b\n",
    "\n",
    "    def reverse_action(self, action):\n",
    "        act_k_inv = 2./(self.action_space.high - self.action_space.low)\n",
    "        act_b = (self.action_space.high + self.action_space.low)/ 2.\n",
    "        return act_k_inv * (action - act_b)\n",
    "import os\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "\n",
    "USE_CUDA = torch.cuda.is_available()\n",
    "FLOAT = torch.cuda.FloatTensor if USE_CUDA else torch.FloatTensor\n",
    "\n",
    "\n",
    "def to_numpy(var):\n",
    "    return var.cpu().data.numpy() if USE_CUDA else var.data.numpy()\n",
    "\n",
    "def to_tensor(ndarray, volatile=False, requires_grad=False, dtype=FLOAT):\n",
    "    return Variable(\n",
    "        torch.from_numpy(ndarray.astype(np.float64)), volatile=volatile, requires_grad=requires_grad\n",
    "    ).type(dtype)\n",
    "\n",
    "def soft_update(target, source, tau):\n",
    "    for target_param, param in zip(target.parameters(), source.parameters()):\n",
    "        target_param.data.copy_(\n",
    "            target_param.data * (1.0 - tau) + param.data * tau\n",
    "        )\n",
    "\n",
    "def hard_update(target, source):\n",
    "    for target_param, param in zip(target.parameters(), source.parameters()):\n",
    "            target_param.data.copy_(param.data)\n",
    "\n",
    "def get_output_folder(parent_dir, env2_name):\n",
    "    \"\"\"Return save folder.\n",
    "    Assumes folders in the parent_dir have suffix -run{run\n",
    "    number}. Finds the highest run number and sets the output folder\n",
    "    to that number + 1. This is just convenient so that if you run the\n",
    "    same script multiple times tensorboard can plot all of the results\n",
    "    on the same plots with different names.\n",
    "    Parameters\n",
    "    ----------\n",
    "    parent_dir: str\n",
    "      Path of the directory containing all experiment runs.\n",
    "    Returns\n",
    "    -------\n",
    "    parent_dir/run_dir\n",
    "      Path to this run's save directory.\n",
    "    \"\"\"\n",
    "    os.makedirs(parent_dir, exist_ok=True)\n",
    "    experiment_id = 0\n",
    "    for folder_name in os.listdir(parent_dir):\n",
    "        if not os.path.isdir(os.path.join(parent_dir, folder_name)):\n",
    "            continue\n",
    "        try:\n",
    "            folder_name = int(folder_name.split('-run')[-1])\n",
    "            if folder_name > experiment_id:\n",
    "                experiment_id = folder_name\n",
    "        except:\n",
    "            pass\n",
    "    experiment_id += 1\n",
    "\n",
    "    parent_dir = os.path.join(parent_dir, env2_name)\n",
    "    parent_dir = parent_dir + '-run{}'.format(experiment_id)\n",
    "    os.makedirs(parent_dir, exist_ok=True)\n",
    "    return parent_dir\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.io import savemat\n",
    "class Evaluator(object):\n",
    "\n",
    "    def __init__(self, num_episodes, interval, save_path='', max_episode_length=None):\n",
    "        self.num_episodes = num_episodes\n",
    "        self.max_episode_length = max_episode_length\n",
    "        self.interval = interval\n",
    "        self.save_path = save_path\n",
    "        self.results = np.array([]).reshape(num_episodes,0)\n",
    "\n",
    "    def __call__(self, env2, policy,m, debug=False, visualize=False, save=True):\n",
    "\n",
    "        self.is_training = False\n",
    "        observation = None\n",
    "        result = []\n",
    "\n",
    "        for episode in range(self.num_episodes):\n",
    "\n",
    "            # reset at the start of episode\n",
    "            observation = env2.reset()\n",
    "            episode_steps = 0\n",
    "            episode_reward = 0.\n",
    "\n",
    "            assert observation is not None\n",
    "\n",
    "            # start episode\n",
    "            done = False\n",
    "            while not done:\n",
    "                # basic operation, action ,reward, blablabla ...\n",
    "                action = policy([observation])\n",
    "                m.eval()\n",
    "                observation, reward, done, info = env2.step(action,m)\n",
    "                if self.max_episode_length and episode_steps >= self.max_episode_length -1:\n",
    "                    done = True\n",
    "\n",
    "                # update\n",
    "                episode_reward += reward\n",
    "                episode_steps += 1\n",
    "\n",
    "            result.append(episode_reward)\n",
    "\n",
    "        result = np.array(result).reshape(-1,1)\n",
    "        self.results = np.hstack([self.results, result])\n",
    "\n",
    "        if save:\n",
    "            self.save_results('{}/validate_reward'.format(self.save_path))\n",
    "        return np.mean(result)\n",
    "\n",
    "    def save_results(self, fn):\n",
    "\n",
    "        y = np.mean(self.results, axis=0)\n",
    "        error=np.std(self.results, axis=0)\n",
    "\n",
    "        x = range(0,self.results.shape[1]*self.interval,self.interval)\n",
    "        fig, ax = plt.subplots(1, 1, figsize=(6, 5))\n",
    "        plt.xlabel('Timestep')\n",
    "        plt.ylabel('Average Reward')\n",
    "        ax.errorbar(x, y, yerr=error, fmt='-o')\n",
    "        plt.savefig(fn+'.png')\n",
    "        savemat(fn+'.mat', {'reward':self.results})\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "#from ipdb import set_trace as debug\n",
    "\n",
    "def fanin_init(size, fanin=None):\n",
    "    fanin = fanin or size[0]\n",
    "    v = 1. / np.sqrt(fanin)\n",
    "    return torch.Tensor(size).uniform_(-v, v)\n",
    "\n",
    "class Actor(nn.Module):\n",
    "    def __init__(self, nb_states, nb_actions, hidden1=400, hidden2=300, init_w=3e-3):\n",
    "        super(Actor, self).__init__()\n",
    "        self.fc1 = nn.Linear(nb_states, hidden1)\n",
    "        self.fc2 = nn.Linear(hidden1, hidden2)\n",
    "        self.fc3 = nn.Linear(hidden2, nb_actions)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.init_weights(init_w)\n",
    "        self.sigmoid=nn.Sigmoid()\n",
    "    def init_weights(self, init_w):\n",
    "        self.fc1.weight.data = fanin_init(self.fc1.weight.data.size())\n",
    "        self.fc2.weight.data = fanin_init(self.fc2.weight.data.size())\n",
    "        self.fc3.weight.data.uniform_(-init_w, init_w)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc3(out)\n",
    "        out = self.tanh(out)\n",
    "        out=self.sigmoid(out)\n",
    "        return out\n",
    "\n",
    "class Critic(nn.Module):\n",
    "    def __init__(self, nb_states, nb_actions, hidden1=400, hidden2=300, init_w=3e-3):\n",
    "        super(Critic, self).__init__()\n",
    "        self.fc1 = nn.Linear(nb_states, hidden1)\n",
    "        self.fc2 = nn.Linear(hidden1 +nb_actions, hidden2)\n",
    "        self.fc3 = nn.Linear(hidden2, 1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.init_weights(init_w)\n",
    "\n",
    "    def init_weights(self, init_w):\n",
    "        self.fc1.weight.data = fanin_init(self.fc1.weight.data.size())\n",
    "        self.fc2.weight.data = fanin_init(self.fc2.weight.data.size())\n",
    "        self.fc3.weight.data.uniform_(-init_w, init_w)\n",
    "\n",
    "    def forward(self, xs):\n",
    "        x, a = xs\n",
    "        out = self.fc1(x)\n",
    "        out = self.relu(out)\n",
    "        # debug()\n",
    "        if len(a.shape)==1:\n",
    "            a=a.view([1,a.shape[0]])\n",
    "        print(out.shape,a.shape)\n",
    "        out = self.fc2(torch.cat([out,a],1))\n",
    "        out = self.relu(out)\n",
    "        out = self.fc3(out)\n",
    "        return out\n",
    "\n",
    "from collections import deque, namedtuple\n",
    "import warnings\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.cluster import Birch\n",
    "# [reference] https://github.com/matthiasplappert/keras-rl/blob/master/rl/memory.py\n",
    "\n",
    "# This is to be understood as a transition: Given `state0`, performing `action`\n",
    "# yields `reward` and results in `state1`, which might be `terminal`.\n",
    "Experience = namedtuple('Experience', 'state0, action, reward, state1, terminal1')\n",
    "\n",
    "\n",
    "def sample_batch_indexes(low, high, size):\n",
    "    if high - low >= size:\n",
    "        # We have enough data. Draw without replacement, that is each index is unique in the\n",
    "        # batch. We cannot use `np.random.choice` here because it is horribly inefficient as\n",
    "        # the memory grows. See https://github.com/numpy/numpy/issues/2764 for a discussion.\n",
    "        # `random.sample` does the same thing (drawing without replacement) and is way faster.\n",
    "        try:\n",
    "            r = xrange(low, high)\n",
    "        except NameError:\n",
    "            r = range(low, high)\n",
    "        batch_idxs = random.sample(r, size)\n",
    "    else:\n",
    "        # Not enough data. Help ourselves with sampling from the range, but the same index\n",
    "        # can occur multiple times. This is not good and should be avoided by picking a\n",
    "        # large enough warm-up phase.\n",
    "        warnings.warn('Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!')\n",
    "        batch_idxs = np.random.random_integers(low, high - 1, size=size)\n",
    "    assert len(batch_idxs) == size\n",
    "    return batch_idxs\n",
    "\n",
    "\n",
    "class RingBuffer(object):\n",
    "    def __init__(self, maxlen):\n",
    "        self.maxlen = maxlen\n",
    "        self.start = 0\n",
    "        self.length = 0\n",
    "        self.data = [None for _ in range(maxlen)]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if idx < 0 or idx >= self.length:\n",
    "            raise KeyError()\n",
    "        return self.data[(self.start + idx) % self.maxlen]\n",
    "\n",
    "    def append(self, v):\n",
    "        if self.length < self.maxlen:\n",
    "            # We have space, simply increase the length.\n",
    "            self.length += 1\n",
    "        elif self.length == self.maxlen:\n",
    "            # No space, \"remove\" the first item.\n",
    "            self.start = (self.start + 1) % self.maxlen\n",
    "        else:\n",
    "            # This should never happen.\n",
    "            raise RuntimeError()\n",
    "        self.data[(self.start + self.length - 1) % self.maxlen] = v\n",
    "\n",
    "\n",
    "def zeroed_observation(observation):\n",
    "    if hasattr(observation, 'shape'):\n",
    "        return np.zeros(observation.shape)\n",
    "    elif hasattr(observation, '__iter__'):\n",
    "        out = []\n",
    "        for x in observation:\n",
    "            out.append(zeroed_observation(x))\n",
    "        return out\n",
    "    else:\n",
    "        return 0.\n",
    "\n",
    "\n",
    "class Memory(object):\n",
    "    def __init__(self, window_length, ignore_episode_boundaries=False):\n",
    "        self.window_length = window_length\n",
    "        self.ignore_episode_boundaries = ignore_episode_boundaries\n",
    "\n",
    "        self.recent_observations = deque(maxlen=window_length)\n",
    "        self.recent_terminals = deque(maxlen=window_length)\n",
    "\n",
    "    def sample(self, batch_size, batch_idxs=None):\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def append(self, observation, action, reward, terminal, training=True):\n",
    "        self.recent_observations.append(observation)\n",
    "        self.recent_terminals.append(terminal)\n",
    "\n",
    "    def get_recent_state(self, current_observation):\n",
    "        # This code is slightly complicated by the fact that subsequent observations might be\n",
    "        # from different episodes. We ensure that an experience never spans multiple episodes.\n",
    "        # This is probably not that important in practice but it seems cleaner.\n",
    "        state = [current_observation]\n",
    "        idx = len(self.recent_observations) - 1\n",
    "        for offset in range(0, self.window_length - 1):\n",
    "            current_idx = idx - offset\n",
    "            current_terminal = self.recent_terminals[current_idx - 1] if current_idx - 1 >= 0 else False\n",
    "            if current_idx < 0 or (not self.ignore_episode_boundaries and current_terminal):\n",
    "                # The previously handled observation was terminal, don't add the current one.\n",
    "                # Otherwise we would leak into a different episode.\n",
    "                break\n",
    "            state.insert(0, self.recent_observations[current_idx])\n",
    "        while len(state) < self.window_length:\n",
    "            state.insert(0, zeroed_observation(state[0]))\n",
    "        return state\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {\n",
    "            'window_length': self.window_length,\n",
    "            'ignore_episode_boundaries': self.ignore_episode_boundaries,\n",
    "        }\n",
    "        return config\n",
    "def samplewithclustering(acs,size):\n",
    "    model = Birch(threshold=0.01, n_clusters=20)\n",
    "    model.fit(acs)      \n",
    "    yhat = model.predict(acs)\n",
    "    ydict={}\n",
    "    for i in range(20):\n",
    "        ydict[i]=[]\n",
    "        ydict[i]=ydict[i]+list(np.array(np.argwhere(yhat==i)).reshape(len(np.argwhere(yhat==i))))\n",
    "    ids=[]\n",
    "    for i in range(20):\n",
    "        np.random.shuffle(ydict[i])\n",
    "        ids+=ydict[i][:int(size*len(ydict[i])/len(yhat))]\n",
    "    return ids\n",
    "class SequentialMemory(Memory):\n",
    "    def __init__(self, limit, **kwargs):\n",
    "        super(SequentialMemory, self).__init__(**kwargs)\n",
    "\n",
    "        self.limit = limit\n",
    "\n",
    "        # Do not use deque to implement the memory. This data structure may seem convenient but\n",
    "        # it is way too slow on random access. Instead, we use our own ring buffer implementation.\n",
    "        self.actions = RingBuffer(limit)\n",
    "        self.rewards = RingBuffer(limit)\n",
    "        self.terminals = RingBuffer(limit)\n",
    "        self.observations = RingBuffer(limit)\n",
    "\n",
    "    def sample(self, batch_size, batch_idxs=None):\n",
    "        if batch_idxs is None:\n",
    "            # Draw random indexes such that we have at least a single entry before each\n",
    "            # index.\n",
    "            sample1s=samplewithclustering([list(i) for i in self.actions.data[:(self.actions.start + self.actions.length - 1) % self.actions.maxlen]],batch_size//2)\n",
    "            batch_idxs = sample1s+sample_batch_indexes(0, self.nb_entries - 1, size=batch_size-len(sample1s))\n",
    "        batch_idxs = np.array(batch_idxs) + 1\n",
    "        assert np.min(batch_idxs) >= 1\n",
    "        assert np.max(batch_idxs) < self.nb_entries\n",
    "        assert len(batch_idxs) == batch_size\n",
    "\n",
    "        # Create experiences\n",
    "        experiences = []\n",
    "        for idx in batch_idxs:\n",
    "            terminal0 = self.terminals[idx - 2] if idx >= 2 else False\n",
    "            while terminal0:\n",
    "                # Skip this transition because the env2ironment was reset here. Select a new, random\n",
    "                # transition and use this instead. This may cause the batch to contain the same\n",
    "                # transition twice.\n",
    "                idx = sample_batch_indexes(1, self.nb_entries, size=1)[0]\n",
    "                terminal0 = self.terminals[idx - 2] if idx >= 2 else False\n",
    "            assert 1 <= idx < self.nb_entries\n",
    "\n",
    "            # This code is slightly complicated by the fact that subsequent observations might be\n",
    "            # from different episodes. We ensure that an experience never spans multiple episodes.\n",
    "            # This is probably not that important in practice but it seems cleaner.\n",
    "            state0 = [self.observations[idx - 1]]\n",
    "            for offset in range(0, self.window_length - 1):\n",
    "                current_idx = idx - 2 - offset\n",
    "                current_terminal = self.terminals[current_idx - 1] if current_idx - 1 > 0 else False\n",
    "                if current_idx < 0 or (not self.ignore_episode_boundaries and current_terminal):\n",
    "                    # The previously handled observation was terminal, don't add the current one.\n",
    "                    # Otherwise we would leak into a different episode.\n",
    "                    break\n",
    "                state0.insert(0, self.observations[current_idx])\n",
    "            while len(state0) < self.window_length:\n",
    "                state0.insert(0, zeroed_observation(state0[0]))\n",
    "            action = self.actions[idx - 1]\n",
    "            reward = self.rewards[idx - 1]\n",
    "            terminal1 = self.terminals[idx - 1]\n",
    "\n",
    "            # Okay, now we need to create the follow-up state. This is state0 shifted on timestep\n",
    "            # to the right. Again, we need to be careful to not include an observation from the next\n",
    "            # episode if the last state is terminal.\n",
    "            state1 = [np.copy(x) for x in state0[1:]]\n",
    "            state1.append(self.observations[idx])\n",
    "\n",
    "            assert len(state0) == self.window_length\n",
    "            assert len(state1) == len(state0)\n",
    "            experiences.append(Experience(state0=state0, action=action, reward=reward,\n",
    "                                          state1=state1, terminal1=terminal1))\n",
    "        assert len(experiences) == batch_size\n",
    "        return experiences\n",
    "\n",
    "    def sample_and_split(self, batch_size, batch_idxs=None):\n",
    "        experiences = self.sample(batch_size, batch_idxs)\n",
    "\n",
    "        state0_batch = []\n",
    "        reward_batch = []\n",
    "        action_batch = []\n",
    "        terminal1_batch = []\n",
    "        state1_batch = []\n",
    "        for e in experiences:\n",
    "            state0_batch.append(e.state0)\n",
    "            state1_batch.append(e.state1)\n",
    "            reward_batch.append(e.reward)\n",
    "            action_batch.append(e.action)\n",
    "            terminal1_batch.append(0. if e.terminal1 else 1.)\n",
    "\n",
    "        # Prepare and validate parameters.\n",
    "        state0_batch = np.array(state0_batch).reshape(batch_size,-1)\n",
    "        state1_batch = np.array(state1_batch).reshape(batch_size,-1)\n",
    "        terminal1_batch = np.array(terminal1_batch).reshape(batch_size,-1)\n",
    "        reward_batch = np.array(reward_batch).reshape(batch_size,-1)\n",
    "        action_batch = np.array(action_batch).reshape(batch_size,-1)\n",
    "\n",
    "        return state0_batch, action_batch, reward_batch, state1_batch, terminal1_batch\n",
    "\n",
    "\n",
    "    def append(self, observation, action, reward, terminal, training=True):\n",
    "        super(SequentialMemory, self).append(observation, action, reward, terminal, training=training)\n",
    "\n",
    "        # This needs to be understood as follows: in `observation`, take `action`, obtain `reward`\n",
    "        # and weather the next state is `terminal` or not.\n",
    "        if training:\n",
    "            \n",
    "            self.observations.append(observation)\n",
    "            \n",
    "            self.actions.append(action)\n",
    "            self.rewards.append(reward)\n",
    "            self.terminals.append(terminal)\n",
    "\n",
    "    @property\n",
    "    def nb_entries(self):\n",
    "        return len(self.observations)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super(SequentialMemory, self).get_config()\n",
    "        config['limit'] = self.limit\n",
    "        return config\n",
    "\n",
    "\n",
    "class EpisodeParameterMemory(Memory):\n",
    "    def __init__(self, limit, **kwargs):\n",
    "        super(EpisodeParameterMemory, self).__init__(**kwargs)\n",
    "        self.limit = limit\n",
    "\n",
    "        self.params = RingBuffer(limit)\n",
    "        self.intermediate_rewards = []\n",
    "        self.total_rewards = RingBuffer(limit)\n",
    "\n",
    "    def sample(self, batch_size, batch_idxs=None):\n",
    "        if batch_idxs is None:\n",
    "            batch_idxs = sample_batch_indexes(0, self.nb_entries, size=batch_size)\n",
    "        assert len(batch_idxs) == batch_size\n",
    "\n",
    "        batch_params = []\n",
    "        batch_total_rewards = []\n",
    "        for idx in batch_idxs:\n",
    "            batch_params.append(self.params[idx])\n",
    "            batch_total_rewards.append(self.total_rewards[idx])\n",
    "        return batch_params, batch_total_rewards\n",
    "\n",
    "    def append(self, observation, action, reward, terminal, training=True):\n",
    "        super(EpisodeParameterMemory, self).append(observation, action, reward, terminal, training=training)\n",
    "        if training:\n",
    "            self.intermediate_rewards.append(reward)\n",
    "\n",
    "    def finalize_episode(self, params):\n",
    "        total_reward = sum(self.intermediate_rewards)\n",
    "        self.total_rewards.append(total_reward)\n",
    "        self.params.append(params)\n",
    "        self.intermediate_rewards = []\n",
    "\n",
    "    @property\n",
    "    def nb_entries(self):\n",
    "        return len(self.total_rewards)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super(SequentialMemory, self).get_config()\n",
    "        config['limit'] = self.limit\n",
    "        return config\n",
    "import numpy as np \n",
    "\n",
    "# [reference] https://github.com/matthiasplappert/keras-rl/blob/master/rl/random.py\n",
    "\n",
    "class RandomProcess(object):\n",
    "    def reset_states(self):\n",
    "        pass\n",
    "\n",
    "class AnnealedGaussianProcess(RandomProcess):\n",
    "    def __init__(self, mu, sigma, sigma_min, n_steps_annealing):\n",
    "        self.mu = mu\n",
    "        self.sigma = sigma\n",
    "        self.n_steps = 0\n",
    "\n",
    "        if sigma_min is not None:\n",
    "            self.m = -float(sigma - sigma_min) / float(n_steps_annealing)\n",
    "            self.c = sigma\n",
    "            self.sigma_min = sigma_min\n",
    "        else:\n",
    "            self.m = 0.\n",
    "            self.c = sigma\n",
    "            self.sigma_min = sigma\n",
    "\n",
    "    @property\n",
    "    def current_sigma(self):\n",
    "        sigma = max(self.sigma_min, self.m * float(self.n_steps) + self.c)\n",
    "        return sigma\n",
    "\n",
    "\n",
    "# Based on http://math.stackexchange.com/questions/1287634/implementing-ornstein-uhlenbeck-in-matlab\n",
    "class OrnsteinUhlenbeckProcess(AnnealedGaussianProcess):\n",
    "    def __init__(self, theta, mu=0., sigma=1., dt=1e-2, x0=None, size=1, sigma_min=None, n_steps_annealing=1000):\n",
    "        super(OrnsteinUhlenbeckProcess, self).__init__(mu=mu, sigma=sigma, sigma_min=sigma_min, n_steps_annealing=n_steps_annealing)\n",
    "        self.theta = theta\n",
    "        self.mu = mu\n",
    "        self.dt = dt\n",
    "        self.x0 = x0\n",
    "        self.size = size\n",
    "        self.reset_states()\n",
    "\n",
    "    def sample(self):\n",
    "        x = self.x_prev + self.theta * (self.mu - self.x_prev) * self.dt + self.current_sigma * np.sqrt(self.dt) * np.random.normal(size=self.size)\n",
    "        self.x_prev = x\n",
    "        self.n_steps += 1\n",
    "        return x\n",
    "\n",
    "    def reset_states(self):\n",
    "        self.x_prev = self.x0 if self.x0 is not None else np.zeros(self.size)\n",
    "# from ipdb import set_trace as debug\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "class DDPG(object):\n",
    "    def __init__(self, nb_states, nb_actions, args2):\n",
    "\n",
    "        if args2.seed > 0:\n",
    "            self.seed(args2.seed)\n",
    "\n",
    "        self.nb_states = nb_states\n",
    "        self.nb_actions= nb_actions\n",
    "\n",
    "        # Create Actor and Critic Network\n",
    "        net_cfg = {\n",
    "            'hidden1':args2.hidden1, \n",
    "            'hidden2':args2.hidden2, \n",
    "            'init_w':args2.init_w\n",
    "        }\n",
    "        self.actor = Actor(self.nb_states, self.nb_actions, **net_cfg)\n",
    "        self.actor_target = Actor(self.nb_states, self.nb_actions, **net_cfg)\n",
    "        self.actor_optim  = Adam(self.actor.parameters(), lr=args2.prate)\n",
    "\n",
    "        self.critic = Critic(self.nb_states, self.nb_actions, **net_cfg)\n",
    "        self.critic_target = Critic(self.nb_states, self.nb_actions, **net_cfg)\n",
    "        self.critic_optim  = Adam(self.critic.parameters(), lr=args2.rate)\n",
    "\n",
    "        hard_update(self.actor_target, self.actor) # Make sure target is with the same weight\n",
    "        hard_update(self.critic_target, self.critic)\n",
    "\n",
    "        #Create replay buffer\n",
    "        self.memory = SequentialMemory(limit=args2.rmsize, window_length=args2.window_length)\n",
    "        self.random_process = OrnsteinUhlenbeckProcess(size=nb_actions, theta=args2.ou_theta, mu=args2.ou_mu, sigma=args2.ou_sigma)\n",
    "\n",
    "        # Hyper-parameters\n",
    "        self.batch_size = args2.bsize\n",
    "        self.tau = args2.tau\n",
    "        self.discount = args2.discount\n",
    "        self.depsilon = 1.0 / args2.epsilon\n",
    "\n",
    "        # \n",
    "        self.epsilon = 1.0\n",
    "        self.s_t = None # Most recent state\n",
    "        self.a_t = None # Most recent action\n",
    "        self.is_training = True\n",
    "\n",
    "        # \n",
    "        if USE_CUDA: self.cuda()\n",
    "\n",
    "    def update_policy(self):\n",
    "        # Sample batch\n",
    "        state_batch, action_batch, reward_batch, \\\n",
    "        next_state_batch, terminal_batch = self.memory.sample_and_split(self.batch_size)\n",
    "\n",
    "        # Prepare for the target q batch\n",
    "        next_q_values = self.critic_target([\n",
    "            to_tensor(next_state_batch, volatile=True),\n",
    "            self.actor_target(to_tensor(next_state_batch, volatile=True)),\n",
    "        ])\n",
    "        next_q_values.volatile=False\n",
    "\n",
    "        target_q_batch = to_tensor(reward_batch) + \\\n",
    "            self.discount*to_tensor(terminal_batch.astype(np.float))*next_q_values\n",
    "\n",
    "        # Critic update\n",
    "        self.critic.zero_grad()\n",
    "\n",
    "        q_batch = self.critic([ to_tensor(state_batch), to_tensor(action_batch) ])\n",
    "\n",
    "        value_loss = criterion(q_batch, target_q_batch)\n",
    "        value_loss.backward()\n",
    "        self.critic_optim.step()\n",
    "\n",
    "        # Actor update\n",
    "        self.actor.zero_grad()\n",
    "\n",
    "        policy_loss = -self.critic([\n",
    "            to_tensor(state_batch),\n",
    "            self.actor(to_tensor(state_batch))\n",
    "        ])\n",
    "\n",
    "        policy_loss = policy_loss.mean()\n",
    "        policy_loss.backward()\n",
    "        self.actor_optim.step()\n",
    "\n",
    "        # Target update\n",
    "        soft_update(self.actor_target, self.actor, self.tau)\n",
    "        soft_update(self.critic_target, self.critic, self.tau)\n",
    "\n",
    "    def eval(self):\n",
    "        self.actor.eval()\n",
    "        self.actor_target.eval()\n",
    "        self.critic.eval()\n",
    "        self.critic_target.eval()\n",
    "\n",
    "    def cuda(self):\n",
    "        self.actor.cuda()\n",
    "        self.actor_target.cuda()\n",
    "        self.critic.cuda()\n",
    "        self.critic_target.cuda()\n",
    "\n",
    "    def observe(self, r_t, s_t1, done):\n",
    "        if self.is_training:\n",
    "            #print('----',self.a_t)\n",
    "            self.memory.append(self.s_t, self.a_t, r_t, done)\n",
    "            self.s_t = s_t1\n",
    "\n",
    "    def random_action(self):\n",
    "        action = np.random.uniform(0.,1.,self.nb_actions)\n",
    "        self.a_t = action\n",
    "        return action\n",
    "\n",
    "    def select_action(self, s_t, decay_epsilon=True):\n",
    "        #print('self.actor(to_tensor(np.array([s_t]))):',self.actor(to_tensor(np.array([s_t]))))\n",
    "        action = to_numpy(\n",
    "            self.actor(to_tensor(np.array([s_t])))\n",
    "        ).squeeze(0)\n",
    "        #action += self.is_training*max(self.epsilon, 0)*self.random_process.sample()\n",
    "        action = np.clip(action, 0., 1.)\n",
    "\n",
    "        if decay_epsilon:\n",
    "            self.epsilon -= self.depsilon\n",
    "\n",
    "        self.a_t = action\n",
    "        \n",
    "        return action\n",
    "\n",
    "    def reset(self, obs):\n",
    "        self.s_t = obs\n",
    "        self.random_process.reset_states()\n",
    "\n",
    "    def load_weights(self, output):\n",
    "        if output is None: return\n",
    "\n",
    "        self.actor.load_state_dict(\n",
    "            torch.load('{}/actor.pkl'.format(output))\n",
    "        )\n",
    "\n",
    "        self.critic.load_state_dict(\n",
    "            torch.load('{}/critic.pkl'.format(output))\n",
    "        )\n",
    "\n",
    "\n",
    "    def save_model(self,output):\n",
    "        torch.save(\n",
    "            self.actor.state_dict(),\n",
    "            '{}/actor.pkl'.format(output)\n",
    "        )\n",
    "        torch.save(\n",
    "            self.critic.state_dict(),\n",
    "            '{}/critic.pkl'.format(output)\n",
    "        )\n",
    "\n",
    "    def seed(self,s):\n",
    "        torch.manual_seed(s)\n",
    "        if USE_CUDA:\n",
    "            torch.cuda.manual_seed(s)\n",
    "import pyflann\n",
    "from gym.spaces import Box\n",
    "import numpy as np\n",
    "import itertools\n",
    "\n",
    "class Space:\n",
    "\n",
    "    def __init__(self, low, high, points):\n",
    "\n",
    "        self._low = np.array(low)\n",
    "        self._high = np.array(high)\n",
    "        self._range = self._high - self._low\n",
    "        self._dimensions = len(low)\n",
    "        self.__space = init_uniform_space([0] * self._dimensions,\n",
    "                                          [1] * self._dimensions,\n",
    "                                          points)\n",
    "        self._flann = pyflann.FLANN()\n",
    "        self.rebuild_flann()\n",
    "\n",
    "    def rebuild_flann(self):\n",
    "        self._index = self._flann.build_index(self.__space, algorithm='kdtree')\n",
    "        \n",
    "    def search_point(self, point, k):\n",
    "        p_in = self.import_point(point).reshape(1, -1).astype('float64')\n",
    "        search_res, _ = self._flann.nn_index(p_in, k)\n",
    "        knns = self.__space[search_res]\n",
    "        p_out = []\n",
    "        for p in knns:\n",
    "            p_out.append(self.export_point(p))\n",
    "\n",
    "        if k == 1:\n",
    "            p_out = [p_out]\n",
    "        return np.array(p_out)\n",
    "\n",
    "    def import_point(self, point):\n",
    "        return (point - self._low) / self._range\n",
    "\n",
    "    def export_point(self, point):\n",
    "        return self._low + point * self._range\n",
    "\n",
    "    def get_space(self):\n",
    "        return self.__space\n",
    "\n",
    "    def shape(self):\n",
    "        return self.__space.shape\n",
    "\n",
    "    def get_number_of_actions(self):\n",
    "        return self.shape()[0]\n",
    "\n",
    "    def plot_space(self, additional_points=None):\n",
    "\n",
    "        dims = self._dimensions\n",
    "\n",
    "        if dims > 3:\n",
    "            print(\n",
    "                'Cannot plot a {}-dimensional space. Max 3 dimensions'.format(dims))\n",
    "            return\n",
    "\n",
    "        space = self.get_space()\n",
    "        if additional_points is not None:\n",
    "            for i in additional_points:\n",
    "                space = np.append(space, additional_points, axis=0)\n",
    "\n",
    "        if dims == 1:\n",
    "            for x in space:\n",
    "                plt.plot([x], [0], 'o')\n",
    "\n",
    "            plt.show()\n",
    "        elif dims == 2:\n",
    "            for x, y in space:\n",
    "                plt.plot([x], [y], 'o')\n",
    "\n",
    "            plt.show()\n",
    "        else:\n",
    "            plot_3d_points(space)\n",
    "\n",
    "\n",
    "class Discrete_space(Space):\n",
    "    \"\"\"\n",
    "        Discrete action space with n actions (the integers in the range [0, n))\n",
    "        0, 1, 2, ..., n-2, n-1\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n):  # n: the number of the discrete actions\n",
    "        super().__init__([0], [n - 1], n)\n",
    "\n",
    "    def export_point(self, point):\n",
    "        return super().export_point(point).astype(int)\n",
    "\n",
    "\n",
    "def init_uniform_space(low, high, points):\n",
    "    dims = len(low)\n",
    "    points_in_each_axis = round(points**(1 / dims))\n",
    "\n",
    "    axis = []\n",
    "    for i in range(dims):\n",
    "        axis.append(list(np.linspace(low[i], high[i], points_in_each_axis)))\n",
    "    space = []\n",
    "    for _ in itertools.product(*axis):\n",
    "        space.append(list(_))\n",
    "    return np.array(space)\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "class WOLPAgent(object):\n",
    "    def __init__(self, nb_states, nb_actions, args):\n",
    "\n",
    "        if args2.seed > 0:\n",
    "            self.seed(args2.seed)\n",
    "\n",
    "        self.nb_states = nb_states\n",
    "        self.nb_actions= nb_actions\n",
    "\n",
    "        # Create Actor and Critic Network\n",
    "        net_cfg = {\n",
    "            'hidden1':args2.hidden1, \n",
    "            'hidden2':args2.hidden2, \n",
    "            'init_w':args2.init_w\n",
    "        }\n",
    "\n",
    "        ################################## Our Code Start ################################################\n",
    "        self.low = args2.low\n",
    "        self.high = args2.high\n",
    "        self.action_space = spaces.Box(\n",
    "            low=0,\n",
    "            high=1,\n",
    "            shape=(args5.solDim,)\n",
    "        )\n",
    "        self.k_nearest_neighbors = max(1, int(args2.max_actions * args2.k_ratio))\n",
    "        ################################## Our Code End ################################################        \n",
    "\n",
    "        self.actor = Actor(self.nb_states, self.nb_actions, **net_cfg)\n",
    "        self.actor_target = Actor(self.nb_states, self.nb_actions, **net_cfg)\n",
    "        self.actor_optim  = Adam(self.actor.parameters(), lr=args2.prate)\n",
    "        self.critic = Critic(self.nb_states, self.nb_actions, **net_cfg)\n",
    "        self.critic_target = Critic(self.nb_states, self.nb_actions, **net_cfg)\n",
    "        self.critic_optim  = Adam(self.critic.parameters(), lr=args2.rate)\n",
    "\n",
    "        hard_update(self.actor_target, self.actor) # Make sure target is with the same weight\n",
    "        hard_update(self.critic_target, self.critic)\n",
    "\n",
    "        #Create replay buffer\n",
    "        self.memory = SequentialMemory(limit=args2.rmsize, window_length=args2.window_length)\n",
    "        self.random_process = OrnsteinUhlenbeckProcess(size=nb_actions, theta=args2.ou_theta, mu=args2.ou_mu, sigma=args2.ou_sigma)\n",
    "\n",
    "        # Hyper-parameters\n",
    "        self.batch_size = args2.bsize\n",
    "        self.tau = args2.tau\n",
    "        self.discount = args2.discount\n",
    "        self.depsilon = 1.0 / args2.epsilon\n",
    "\n",
    "        # \n",
    "        self.epsilon = 1.0\n",
    "        self.s_t = None # Most recent state\n",
    "        self.a_t = None # Most recent action\n",
    "        self.is_training = True\n",
    "\n",
    "        # \n",
    "        if USE_CUDA: self.cuda()\n",
    "\n",
    "    def get_action_space(self):\n",
    "        return self.action_space    \n",
    "\n",
    "    def update_policy(self):\n",
    "        # Sample batch\n",
    "        state_batch, action_batch, reward_batch, \\\n",
    "        next_state_batch, terminal_batch = self.memory.sample_and_split(self.batch_size)\n",
    "\n",
    "        # Prepare for the target q batch\n",
    "        next_q_values = self.critic_target([\n",
    "            to_tensor(next_state_batch, volatile=True),\n",
    "            self.actor_target(to_tensor(next_state_batch, volatile=True)),\n",
    "        ])\n",
    "        next_q_values.volatile=False\n",
    "\n",
    "        target_q_batch = to_tensor(reward_batch) + \\\n",
    "            self.discount*to_tensor(terminal_batch.astype(np.float))*next_q_values\n",
    "\n",
    "        # Critic update\n",
    "        self.critic.zero_grad()\n",
    "        q_batch = self.critic([ to_tensor(state_batch), to_tensor(action_batch) ])\n",
    "\n",
    "        value_loss = criterion(q_batch, target_q_batch)\n",
    "        value_loss.backward()\n",
    "        self.critic_optim.step()\n",
    "\n",
    "        # Actor update\n",
    "        self.actor.zero_grad()\n",
    "\n",
    "        policy_loss = -self.critic([\n",
    "            to_tensor(state_batch),\n",
    "            self.actor(to_tensor(state_batch))\n",
    "        ])\n",
    "\n",
    "        policy_loss = policy_loss.mean()\n",
    "        policy_loss.backward()\n",
    "        self.actor_optim.step()\n",
    "\n",
    "        # Target update\n",
    "        soft_update(self.actor_target, self.actor, self.tau)\n",
    "        soft_update(self.critic_target, self.critic, self.tau)\n",
    "\n",
    "    def eval(self):\n",
    "        self.actor.eval()\n",
    "        self.actor_target.eval()\n",
    "        self.critic.eval()\n",
    "        self.critic_target.eval()\n",
    "\n",
    "    def cuda(self):\n",
    "        self.actor.cuda()\n",
    "        self.actor_target.cuda()\n",
    "        self.critic.cuda()\n",
    "        self.critic_target.cuda()\n",
    "\n",
    "    def observe(self, r_t, s_t1, done):\n",
    "        if self.is_training:\n",
    "            self.memory.append(self.s_t, self.a_t, r_t, done)\n",
    "            self.s_t = s_t1\n",
    "\n",
    "    def random_action(self):\n",
    "        action = np.random.uniform(0.,1.,self.nb_actions)\n",
    "        self.a_t = action\n",
    "        return action\n",
    "\n",
    "    def select_action(self, s_t, decay_epsilon=True):\n",
    "        proto_action = self.ddpg_select_action(s_t, decay_epsilon=decay_epsilon)\n",
    "        actions = proto_action\n",
    "        \n",
    "        states = np.array([[0.1]*args5.solDim])\n",
    "\n",
    "        a = [to_tensor(states), to_tensor(actions)]\n",
    "        # print(\"states: {}, actions: {}\".format(a[0].size(), a[1].size()))\n",
    "        actions_evaluation = self.critic([to_tensor(states), to_tensor(actions)])\n",
    "        # print(\"actions_evaluation: {}, actions_evaluation.size(): {}\".format(actions_evaluation, actions_evaluation.size()))\n",
    "        actions_evaluation_np = actions_evaluation.cpu().data.numpy()\n",
    "        max_index = np.argmax(actions_evaluation_np)\n",
    "\n",
    "        self.a_t = actions[max_index]\n",
    "        return self.a_t\n",
    "\n",
    "    def ddpg_select_action(self, s_t, decay_epsilon=True):\n",
    "        action = to_numpy(\n",
    "            self.actor(to_tensor(np.array([s_t])))\n",
    "        ).squeeze(0)            \n",
    "        #action += self.is_training*max(self.epsilon, 0)*self.random_process.sample()\n",
    "        action = np.clip(action, 0., 1.)\n",
    "        if decay_epsilon:\n",
    "            self.epsilon -= self.depsilon\n",
    "        return action\n",
    "    def select_action2(self, s_t, decay_epsilon=True):\n",
    "        proto_action = self.ddpg_select_action2(s_t, decay_epsilon=decay_epsilon)\n",
    "        actions = proto_action\n",
    "        \n",
    "        states = np.array([[0.1]*args5.solDim])\n",
    "\n",
    "        # print(\"states: {}, actions: {}\".format(a[0].size(), a[1].size()))\n",
    "        actions_evaluation = self.critic([to_tensor(states), to_tensor(actions.detach().numpy())])\n",
    "        # print(\"actions_evaluation: {}, actions_evaluation.size(): {}\".format(actions_evaluation, actions_evaluation.size()))\n",
    "        actions_evaluation_np = actions_evaluation.cpu().data.numpy()\n",
    "        max_index = np.argmax(actions_evaluation_np)\n",
    "\n",
    "        self.a_t = actions[max_index]\n",
    "        return self.a_t\n",
    "    def ddpg_select_action2(self, s_t, decay_epsilon=True):\n",
    "        action = self.actor(to_tensor(np.array([s_t])))          \n",
    "        #action += self.is_training*max(self.epsilon, 0)*self.random_process.sample()\n",
    "        action = torch.clamp(action, 0., 1.)\n",
    "        if decay_epsilon:\n",
    "            self.epsilon -= self.depsilon\n",
    "        return action    \n",
    "    def reset(self, obs):\n",
    "        self.s_t = obs\n",
    "        self.random_process.reset_states()\n",
    "\n",
    "    def load_weights(self, output):\n",
    "        if output is None: return\n",
    "\n",
    "        self.actor.load_state_dict(\n",
    "            torch.load('{}/actor.pkl'.format(output))\n",
    "        )\n",
    "\n",
    "        self.critic.load_state_dict(\n",
    "            torch.load('{}/critic.pkl'.format(output))\n",
    "        )\n",
    "\n",
    "\n",
    "    def save_model(self,output):\n",
    "        torch.save(\n",
    "            self.actor.state_dict(),\n",
    "            '{}/actor.pkl'.format(output)\n",
    "        )\n",
    "        torch.save(\n",
    "            self.critic.state_dict(),\n",
    "            '{}/critic.pkl'.format(output)\n",
    "        )\n",
    "\n",
    "    def seed(self,s):\n",
    "        torch.manual_seed(s)\n",
    "        if USE_CUDA:\n",
    "            torch.cuda.manual_seed(s)\n",
    "import math\n",
    "import gym\n",
    "from gym import spaces, logger\n",
    "from gym.utils import seeding\n",
    "import numpy as np\n",
    "\n",
    "   \n",
    "class Arguments(object):\n",
    "    def __init__(self):\n",
    "        self.mode = 'train'\n",
    "        self.env2 = \"InvertedPendulum-v2\"\n",
    "        self.hidden1 = 400\n",
    "        self.hidden2 = 300\n",
    "        self.rate = 0.001\n",
    "        self.prate = 0.0001\n",
    "        self.warmup = 100\n",
    "        self.discount = 0.99\n",
    "        self.bsize = 64\n",
    "        self.rmsize = 6000000\n",
    "        self.window_length = 1\n",
    "        self.tau = 0.001\n",
    "        self.ou_theta = 0.15\n",
    "        self.ou_sigma = 0.2\n",
    "        self.ou_mu = 0.0\n",
    "        self.validate_episodes = 20\n",
    "        self.max_episode_length = 500\n",
    "        self.validate_steps = 5000\n",
    "        self.output = 'output'\n",
    "        self.debug='debug'\n",
    "        self.init_w = 0.03\n",
    "        self.train_iter=20000\n",
    "        self.epsilon=50000\n",
    "        self.seed=-1\n",
    "        self.max_actions=1e6\n",
    "        self.resume='default'\n",
    "        self.k_ratio = 1e-6\n",
    "\n",
    "args2 = Arguments()\n",
    "args2.output = get_output_folder(args2.output, args2.env2)\n",
    "if args2.resume == 'default':\n",
    "    args2.resume = 'output/{}-run0'.format(args2.env2)\n",
    "class myenv2(gym.Env):\n",
    "    metadata = {\n",
    "        'render.modes': ['human', 'rgb_array'],\n",
    "        'video.frames_per_second': 50\n",
    "    }\n",
    "\n",
    "    def __init__(self):\n",
    "\n",
    "        # Angle limit set to 2 * theta_threshold_radians so failing observation\n",
    "        # is still within bounds\n",
    "\n",
    "        self.action_space =  spaces.Box(\n",
    "            low=0,\n",
    "            high=1,\n",
    "            shape=(args5.solDim,)\n",
    "        )\n",
    "        self.observation_space =  spaces.Box(\n",
    "            low=0,\n",
    "            high=1,\n",
    "            shape=(args5.solDim,)\n",
    "        )\n",
    "\n",
    "        self.seed()\n",
    "        self.viewer = None\n",
    "        self.state = None\n",
    "\n",
    "        self.steps_beyond_done = None\n",
    "\n",
    "        self.steps = torch.tensor(np.array([0.1]*args5.solDim))\n",
    "        self._max_episode_steps = 500\n",
    "        self.iteration=1\n",
    "        self.a = 10*(np.load('rateListUsersYahoo.npy')[0]-np.load('rateListUsersYahoo.npy')[0].min())\n",
    "        self.a /= np.linalg.norm(self.a, ord=2)\n",
    "        self.constraints=[]\n",
    "        self.constraint=0\n",
    "        self.history=np.ones(args5.solDim)\n",
    "    def seed(self, seed=None):\n",
    "        self.np_random, seed = seeding.np_random(seed)\n",
    "        return [seed]\n",
    "\n",
    "    def step(self, action,m):\n",
    "        self.iteration+=1 \n",
    "        if self.iteration==60:\n",
    "            self.constraint=10#10*sum(rewards)/len(rewards)\n",
    "            self.std=np.array(rewards).std()\n",
    "        if self.iteration>1 and self.iteration%60==0:\n",
    "            self.constraint=min(100*sum(rewards)/len(rewards),max(0,self.constraint + 0.005*(-0.0+sum(self.constraints)/len(self.constraints))))\n",
    "        done=1\n",
    "        if len(action)==1:\n",
    "            action=action[0]\n",
    "        action=[action[i]+0.01*np.sqrt(np.log(self.iteration)/self.history[i]) for i in range(args.solDim)]\n",
    "        action=np.array([1 if i in np.argsort(action)[-args.card:] else 0  for i in range(args.solDim)])\n",
    "        \n",
    "        self.constraints.append(checkFea(action))\n",
    "        #print(action,checkFea(action))\n",
    "  #      if sum(action)<10:\n",
    "   #         reward=100*m.forward(torch.FloatTensor(action).to(self.device)).detach().squeeze()-100*(sum(action)-10)**2+100000#+ 100*(1/(1+np.exp(-self.iteration))-0.5)*torch.log(torch.mean(wmc)).float()#0*(1/(1+np.exp(-self.iteration))-0.5)*torch.log(torch.mean(wmc)).float()+sum(action)#100*m.forward(torch.FloatTensor(action).to(self.device)).detach().squeeze()\n",
    "    #    else:\n",
    "        #m.forward(torch.FloatTensor(action).to(self.device)).detach().squeeze()\n",
    "        m.eval()\n",
    "        reward= m.forward(torch.FloatTensor(action)).detach().squeeze()-self.constraint*checkFea(action)##0*(1/(1+np.exp(-self.iteration))-0.5)*torch.log(torch.mean(wmc)).float()+sum(action)#100*m.forward(torch.FloatTensor(action).to(self.device)).detach().squeeze()\n",
    "        rewards.append(m.forward(torch.FloatTensor(action)).detach().squeeze())\n",
    "        constraints.append(checkFea(action))\n",
    "        self.history+=np.array(action)\n",
    "        #print(self.iteration,1*(self.a.dot(action))**1/(np.sqrt(10))**1,checkFea(action),action)\n",
    "        #print(m.forward(torch.FloatTensor(action).to(self.device)).detach().squeeze(),checkFea(action),torch.log(torch.mean(wmc)))\n",
    "        return action, reward, done, {}\n",
    "\n",
    "    def reset(self):\n",
    "        return  np.array([0.1]*args5.solDim).reshape([1,args5.solDim])\n",
    "\n",
    "    def render(self, mode='human'):\n",
    "        pass "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def inv_sherman_morrison(u, A_inv):\n",
    "    \"\"\"Inverse of a matrix with rank 1 update.\n",
    "    \"\"\"\n",
    "    Au = np.dot(A_inv, u)\n",
    "    A_inv -= np.outer(Au, Au)/(1+np.dot(u.T, Au))\n",
    "    return A_inv\n",
    "\n",
    "class Model(nn.Module):\n",
    "    \"\"\"Template for fully connected neural network for scalar approximation.\n",
    "    \"\"\"\n",
    "    def __init__(self, \n",
    "                 input_size=1, \n",
    "                 hidden_size=2,\n",
    "                 n_layers=4,\n",
    "                 activation='ReLU',\n",
    "                 p=0.0,\n",
    "                ):\n",
    "        super(Model, self).__init__()\n",
    "        \n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "        if self.n_layers == 1:\n",
    "            self.layers = [nn.Linear(input_size, 1)]\n",
    "        else:\n",
    "            size  = [input_size] + [hidden_size,] * (self.n_layers-1) + [1]\n",
    "            self.layers = [nn.Linear(size[i], size[i+1]) for i in range(self.n_layers)]\n",
    "        self.layers = nn.ModuleList(self.layers)\n",
    "        \n",
    "        # dropout layer\n",
    "        self.dropout = nn.Dropout(p=p)\n",
    "        \n",
    "        # activation function\n",
    "        if activation == 'sigmoid':\n",
    "            self.activation = nn.Sigmoid()\n",
    "        elif activation == 'ReLU':\n",
    "            self.activation = nn.ReLU()\n",
    "        elif activation == 'LeakyReLU':\n",
    "            self.activation = nn.LeakyReLU(negative_slope=0.1)\n",
    "        else:\n",
    "            raise Exception('{} not an available activation'.format(activation))\n",
    "            \n",
    "    def forward(self, x):\n",
    "        for i in range(self.n_layers-1):\n",
    "            x = self.activation(self.layers[i](x))\n",
    "        x = self.layers[-1](x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import abc\n",
    "from tqdm import tqdm\n",
    "class UCB(abc.ABC):\n",
    "    \"\"\"Base class for UBC methods.\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 bandit,\n",
    "                 reg_factor=1.0,\n",
    "                 confidence_scaling_factor=-1.0,\n",
    "                 delta=0.1,\n",
    "                 train_every=1,\n",
    "                 throttle=int(1e2),\n",
    "                ):\n",
    "        # bandit object, contains features and generated rewards\n",
    "        self.bandit = bandit\n",
    "        # L2 regularization strength\n",
    "        self.reg_factor = reg_factor\n",
    "        # Confidence bound with probability 1-delta\n",
    "        self.delta = delta\n",
    "        # multiplier for the confidence bound (default is bandit reward noise std dev)\n",
    "        if confidence_scaling_factor == -1.0:\n",
    "            confidence_scaling_factor = bandit.noise_std\n",
    "        self.confidence_scaling_factor = confidence_scaling_factor\n",
    "        \n",
    "        # train approximator only every few rounds\n",
    "        self.train_every = train_every\n",
    "        self.best_recommended_reward =-200\n",
    "        self.best_recommended_action =np.array([1]*args.card+[0]*(self.bandit.n_features-args.card))\n",
    "        self.rewards_list=[]\n",
    "        self.best_sample_rewards_list=[]\n",
    "        self.feasi=[]\n",
    "        # throttle tqdm updates\n",
    "        self.throttle = throttle\n",
    "        self.solverSol=None\n",
    "        self.reset()\n",
    "        \n",
    "    def reset_upper_confidence_bounds(self):\n",
    "        \"\"\"Initialize upper confidence bounds and related quantities.\n",
    "        \"\"\"\n",
    "        self.exploration_bonus = np.empty((self.bandit.n_arms))\n",
    "        self.mu_hat = np.empty((self.bandit.n_arms)) \n",
    "        self.meta_mu_hat = np.empty(( self.bandit.n_features)) \n",
    "        self.cross_mu_hat = np.empty(( self.bandit.n_features**2)) \n",
    "        self.upper_confidence_bounds = np.ones((self.bandit.n_arms))\n",
    "        self.meta_values= np.ones((self.bandit.n_features))\n",
    "        self.cross_values= np.ones((self.bandit.n_features**2))\n",
    "        self.knn_metabest=-100\n",
    "        self.best_rewards_oracle=-100\n",
    "\n",
    "    def reset_actions(self):\n",
    "        \"\"\"Initialize cache of actions.\n",
    "        \"\"\"\n",
    "        self.actions = np.empty(self.bandit.T).astype('int')\n",
    "    \n",
    "    def reset_A_inv(self):\n",
    "        \"\"\"Initialize n_arms square matrices representing the inverses\n",
    "        of exploration bonus matrices.\n",
    "        \"\"\"\n",
    "        self.A_inv = np.array(\n",
    "            [\n",
    "                np.eye(self.approximator_dim)/self.reg_factor for _ in self.bandit.arms\n",
    "            ]\n",
    "        )\n",
    "    \n",
    "    def reset_grad_approx(self):\n",
    "        \"\"\"Initialize the gradient of the approximator w.r.t its parameters.\n",
    "        \"\"\"\n",
    "        self.grad_approx = np.zeros((self.bandit.n_arms, self.approximator_dim))\n",
    "\n",
    "    def sample_action(self):\n",
    "        \"\"\"Return the action to play based on current estimates\n",
    "        \"\"\"\n",
    "        if self.iteration>=max(self.bandit.n_features*2,100):\n",
    "            return int(torch.argmax(self.upper_confidence_bounds).item())\n",
    "        else:\n",
    "            return np.random.choice(self.bandit.n_arms)\n",
    "    \n",
    "    @abc.abstractmethod\n",
    "    def reset(self):\n",
    "        \"\"\"Initialize variables of interest.\n",
    "        To be defined in children classes.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    @property\n",
    "    @abc.abstractmethod\n",
    "    def approximator_dim(self):\n",
    "        \"\"\"Number of parameters used in the approximator.\n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "    @property\n",
    "    @abc.abstractmethod\n",
    "    def confidence_multiplier(self):\n",
    "        \"\"\"Multiplier for the confidence exploration bonus.\n",
    "        To be defined in children classes.\n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "    @abc.abstractmethod\n",
    "    def update_confidence_bounds(self):\n",
    "        \"\"\"Update the confidence bounds for all arms at time t.\n",
    "        To be defined in children classes.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def update_output_gradient(self):\n",
    "        \"\"\"Compute output gradient of the approximator w.r.t its parameters.\n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "    @abc.abstractmethod\n",
    "    def train(self):\n",
    "        \"\"\"Update approximator.\n",
    "        To be defined in children classes.\n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "    @abc.abstractmethod\n",
    "    def predict(self):\n",
    "        \"\"\"Predict rewards based on an approximator.\n",
    "        To be defined in children classes.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def update_confidence_bounds(self):\n",
    "        \"\"\"Update confidence bounds and related quantities for all arms.\n",
    "        \"\"\"\n",
    "        if self.iteration>max(self.bandit.n_features*2,100):#self.bandit.n_features:\n",
    "            self.bandit.features[self.iteration][2]=self.best_recommended_action\n",
    "            self.bandit.features[self.iteration][0]=self.solverSol\n",
    "            self.bandit.features[self.iteration][1]=self.solverSol_quad\n",
    "            for j in range(3,self.bandit.n_arms//3):\n",
    "                tmp=[np.random.beta(min(1-0.1,max(0.1,mm)),1-min(1-0.1,max(0.1,mm))) for mm in self.solverSol]#[self.meta_values[i]+2*np.sqrt(np.log(self.iteration+1)/self.cnt[i]/self.iteration) for i in range(self.bandit.n_features)]\n",
    "                tmp2=sorted(tmp)[-args.card:] \n",
    "                self.bandit.features[self.iteration][j]= np.divide(np.array([1 if tmp[i] in tmp2 else 0 for i in range(self.bandit.n_features)]),np.sqrt(args.card))\n",
    "            for j in range(n_arms//3,self.bandit.n_arms//2):\n",
    "                tmp=[np.random.beta(min(1-0.1,max(0.1,mm)),1-min(1-0.1,max(0.1,mm))) for mm in self.solverSol_quad]#[self.meta_values[i]+2*np.sqrt(np.log(self.iteration+1)/self.cnt[i]/self.iteration) for i in range(self.bandit.n_features)]\n",
    "                tmp2=sorted(tmp)[-args.card:] \n",
    "                self.bandit.features[self.iteration][j]= np.divide(np.array([1 if tmp[i] in tmp2 else 0 for i in range(self.bandit.n_features)]),np.sqrt(args.card))\n",
    "            for j in range(n_arms//2,2*self.bandit.n_arms//3):\n",
    "                tmp=self.bandit.features[self.iteration][j]+np.random.random()*(self.best_recommended_action-self.bandit.features[self.iteration][j])\n",
    "                self.model.eval()\n",
    "                if self.model.forward(torch.FloatTensor(self.bandit.features[self.iteration][j]).to(self.device)).detach().squeeze()<self.model.forward(torch.FloatTensor(tmp).to(self.device)).detach().squeeze():\n",
    "                    tmp2=sorted(tmp)[-args.card:] \n",
    "                    self.bandit.features[self.iteration][j]= np.divide(np.array([1 if tmp[i] in tmp2 else 0 for i in range(self.bandit.n_features)]),np.sqrt(args.card))\n",
    "            if len(self.elite)>=5:\n",
    "                \n",
    "                for kk in range(min(len(self.elite),self.bandit.n_arms//2)):\n",
    "                    \n",
    "                    self.bandit.features[self.iteration][-kk]= self.elite[kk]\n",
    "            if np.random.random()>0.4:\n",
    "                a=np.random.choice(self.bandit.n_arms)\n",
    "                b=np.random.choice(self.bandit.n_arms)\n",
    "                if a!=b:\n",
    "                    self.model.eval()\n",
    "                    if  self.model.forward(torch.FloatTensor(self.bandit.features[self.iteration][a]).to(self.device)).detach().squeeze()>self.model.forward(torch.FloatTensor(self.bandit.features[self.iteration][b]).to(self.device)).detach().squeeze():\n",
    "                        tmp=self.bandit.features[self.iteration][b]+np.random.random()*(a-b)\n",
    "                        tmp2=sorted(tmp)[-args.card:] \n",
    "                        self.bandit.features[self.iteration][b]= np.divide(np.array([1 if tmp[i] in tmp2 else 0 for i in range(self.bandit.n_features)]),np.sqrt(args.card))\n",
    "\n",
    "\n",
    "\n",
    "        self.update_output_gradient()\n",
    "        \n",
    "        # UCB exploration bonus\n",
    "        self.exploration_bonus = np.array(\n",
    "            [\n",
    "                 -10.*checkFea(self.bandit.features[self.iteration][a]*np.sqrt(args.card))+0.01* np.sqrt(np.dot(self.grad_approx[a], np.dot(self.A_inv[a], self.grad_approx[a].T))) for a in self.bandit.arms\n",
    "            ]\n",
    "        )\n",
    "        self.model.eval()\n",
    "        self.mu_hat= self.model.forward(\n",
    "            torch.FloatTensor(self.bandit.features[self.iteration]).to(self.device)\n",
    "        ).detach().squeeze()\n",
    "        #print(self.bandit.features[self.iteration][0][:10],self.bandit.features[self.iteration][-1][:10])\n",
    "        self.meta_mu_hat = self.model.forward(\n",
    "            torch.FloatTensor(self.bandit.meta_features[self.iteration]).to(self.device)\n",
    "        ).detach().squeeze()\n",
    "        if self.iteration>=max(self.bandit.n_features*2,200):\n",
    "            self.cross_mu_hat= self.model.forward(\n",
    "                torch.FloatTensor(self.bandit.cross_features).to(self.device)\n",
    "            ).detach().squeeze()\n",
    "        self.bandit.rewards[self.iteration]=np.array([self.bandit.hwithoutconstraints(np.sqrt(args.card)*self.bandit.features[ self.iteration,k]) + self.bandit.noise_std*np.random.randn() for k in range(self.bandit.n_arms)])\n",
    "        # estimated combined bound for reward\n",
    "        self.meta_values=self.meta_mu_hat# + self.meta_exploration_bonus[self.iteration] \n",
    "        if self.iteration>=max(self.bandit.n_features*2,100):\n",
    "            self.cross_values=self.cross_mu_hat\n",
    "            for i in range(self.bandit.n_features):\n",
    "                for j in range(self.bandit.n_features):\n",
    "                    if i==j:\n",
    "                        self.cross_values[i*self.bandit.n_features+j]=self.meta_mu_hat[i]\n",
    "                    else:\n",
    "                        self.cross_values[i*self.bandit.n_features+j]=(self.cross_mu_hat[i*self.bandit.n_features+j]-self.meta_mu_hat[i]-self.meta_mu_hat[j])/2\n",
    "            self.cross_values=self.cross_values.reshape(self.bandit.n_features,self.bandit.n_features)\n",
    "        with gurobipy.Env(empty=True) as env:\n",
    "            env.setParam('OutputFlag', 0)\n",
    "            env.setParam('IterationLimit',600)\n",
    "            env.start()\n",
    "            with gurobipy.Model(env=env) as m:\n",
    "                self.solverSol=np.divide(solver(self.meta_values,m),np.sqrt(args.card))\n",
    "        if self.iteration>=max(self.bandit.n_features*2,100):\n",
    "            with gurobipy.Env(empty=True) as env:\n",
    "                env.setParam('OutputFlag', 0)\n",
    "                env.setParam('IterationLimit',600)\n",
    "                env.start()\n",
    "                if sum(sum(np.isnan(model.cross_values)))==0 and sum(sum(np.isinf(model.cross_values)))==0:\n",
    "                    with gurobipy.Model(env=env) as m:\n",
    "                        self.solverSol_quad=np.divide(solver_quad(self.cross_values,m),np.sqrt(args.card))\n",
    "                        #print('solverSol_quad',self.bandit.hwithoutconstraints(np.sqrt(args.card)*self.solverSol_quad))\n",
    "                else:\n",
    "                    self.solverSol_quad=self.bandit.features[self.iteration][0]\n",
    "        tmp=sorted(self.meta_values )[-args.card:]  \n",
    "        self.meta_values = np.array([1 if self.meta_values[i] in tmp else 0 for i in range(self.bandit.n_features)])\n",
    "        if len(np.argwhere(self.meta_values==1)):\n",
    "            metabest=np.argwhere(self.meta_values==1)[:args.card].reshape(args.card)\n",
    "        else:\n",
    "            metabest=list(range(self.bandit.n_features))\n",
    "            np.random.shuffle(metabest)\n",
    "            metabest=np.array(metabest[:args.card])\n",
    "        knn_metabest={}\n",
    "        import copy\n",
    "        for i in range(1):\n",
    "            tmp=metabest.copy()\n",
    "            tmp[np.random.choice(range(args.card))]=np.random.choice(range(self.bandit.n_features))\n",
    "            tmp=np.divide(np.array([1 if i in tmp else 0 for i in range(self.bandit.n_features)]),np.sqrt(args.card))\n",
    "            self.model.eval()\n",
    "            tmpvalue=self.model.forward(\n",
    "                    torch.FloatTensor(tmp).to(self.device)\n",
    "                ).detach().squeeze()\n",
    "            knn_metabest[tmpvalue]=tmp\n",
    "        \n",
    "        # estimated combined bound for reward\n",
    "        self.upper_confidence_bounds = self.mu_hat+ self.exploration_bonus\n",
    "        self.meta_values=np.divide(self.meta_values,np.sqrt(args.card))\n",
    "        self.knn_metabest=knn_metabest[max(knn_metabest.keys())]\n",
    "        self.bandit.knn_metabestValue =self.bandit.hwithoutconstraints(np.sqrt(args.card)*self.knn_metabest)\n",
    "        self.bandit.best_rewards_oracle =max( max(np.max(self.bandit.rewards, axis=1)),self.bandit.knn_metabestValue)\n",
    "        self.best_rewards_oracle=max(max(self.best_rewards_oracle,self.bandit.best_rewards_oracle),self.bandit.hwithoutconstraints(np.sqrt(args.card)*self.knn_metabest))\n",
    "    def update_A_inv(self):\n",
    "        self.A_inv[self.action] = inv_sherman_morrison(\n",
    "            self.grad_approx[self.action],\n",
    "            self.A_inv[self.action]\n",
    "        )\n",
    "        \n",
    "    def run(self):\n",
    "        \"\"\"Run an episode of bandit.\n",
    "        \"\"\"\n",
    "        postfix = {\n",
    "            'total regret': 0.0,\n",
    "            '% optimal arm': 0.0,\n",
    "        }\n",
    "        step=0\n",
    "        # env2 = Normalizedenv2(gym.make(args2.env2))\n",
    "        # env2 = gym.make(args2.env2)\n",
    "        env2 = myenv2()\n",
    "        #################################### Our Code ##############################\n",
    "        args2.low = env2.action_space.low\n",
    "        args2.high = env2.action_space.high\n",
    "        #################################### Our Code ##############################\n",
    "\n",
    "        if args2.seed > 0:\n",
    "            np.random.seed(args2.seed)\n",
    "            env2.seed(args2.seed)\n",
    "\n",
    "        nb_states =args.solDim\n",
    "        nb_actions =args.solDim\n",
    "\n",
    "\n",
    "        agent = WOLPAgent(nb_states, nb_actions, args2)\n",
    "\n",
    "        num_iterations=args2.train_iter\n",
    "        validate_steps=args2.validate_steps\n",
    "        output= args2.output\n",
    "        max_episode_length=args2.max_episode_length\n",
    "        debug=args2.debug\n",
    "        agent.is_training = True\n",
    "        step = episode = episode_steps = 0\n",
    "        episode_reward = 0.\n",
    "        observation = None\n",
    "        import numpy as np\n",
    "        import torch\n",
    "        import copy\n",
    "        eps=0.03\n",
    "        jitter=0.1\n",
    "        beta=0.2\n",
    "        rho=0.1\n",
    "        b=200\n",
    "        N=100\n",
    "        K=20\n",
    "        L=1000000\n",
    "        constraintMean=0.2\n",
    "        g2aactionvalues=np.zeros(50)\n",
    "        wolpertingeractionvalues=np.zeros(50)\n",
    "        def PPOobj(uNext,u):\n",
    "            obj=0.\n",
    "            for i in range(K):\n",
    "                for j in range(args.solDim):\n",
    "                    if actions[i][j]==1:\n",
    "                        obj+=(uNext[j]/u[j])*(b-rewardsCEM[i])\n",
    "                    else:\n",
    "                        obj+=((1-uNext[j])/(1-u[j]))*(b-rewardsCEM[i])\n",
    "                    obj-=(beta*(u[j]*torch.log(u[j]/uNext[j])))\n",
    "                    obj-=(beta*(u[j]*torch.log(u[j]/uNext[j])))\n",
    "            return obj\n",
    "\n",
    "        actions=np.zeros([K,args.solDim])\n",
    "        rewardsCEM=np.zeros(K)\n",
    "        actionsN=np.zeros([N,args.solDim])\n",
    "        rewardsN=np.zeros(N)\n",
    "        constraintN=np.zeros(N)\n",
    "        actionhistoryBest=np.zeros([5,args.solDim])\n",
    "        actionhistoryWorst=np.zeros([5,args.solDim])\n",
    "        rewardhistoryBest=np.zeros(5)\n",
    "        rewardhistoryWorst=np.zeros(5)\n",
    "        u=np.random.random([args.solDim])\n",
    "        uNext=u.copy()\n",
    "        action=[np.random.choice([0,1],size=1,p=[1-u[i],u[i]])[0] for i in range(args.solDim)]\n",
    "        reward=sum(action)-20*checkFea(action)\n",
    "        actions[0]=action\n",
    "        def feedback(x,m,constraintMean):\n",
    "            return m.forward(torch.FloatTensor(x)).detach().squeeze()-10*constraintMean*checkFea(x)\n",
    "        rewardsCEM[0]=feedback(action,self.model,constraintMean)\n",
    "        actionsN[0]=action\n",
    "        rewardsN[0]=feedback(action,self.model,constraintMean)\n",
    "        actionhistoryBest[0]=action\n",
    "        actionhistoryWorst[0]=action\n",
    "        rewardhistoryBest[0]=feedback(action,self.model,constraintMean)\n",
    "        rewardhistoryWorst[0]=feedback(action,self.model,constraintMean)\n",
    "        rnd=0\n",
    "        iteration=0\n",
    "        \n",
    "        actor=Actorg2a(args.solDim,args3)\n",
    "        critic=Criticg2a(args.solDim)\n",
    "        criterion = nn.MSELoss()\n",
    "        opt1 =torch.optim.Adam(actor.parameters(), lr=1e-3)\n",
    "        opt2 =torch.optim.Adam(critic.parameters(), lr=1e-3)\n",
    "        iteration=0\n",
    "        history=np.ones(args.solDim)\n",
    "        constraint=0\n",
    "        calactionhistoryBest=[]\n",
    "        \n",
    "        self.elite=[]\n",
    "        with tqdm(total=self.bandit.T, postfix=postfix) as pbar:\n",
    "            for t in range(self.bandit.T):\n",
    "                # update confidence of all arms based on observed features at time t\n",
    "                self.update_confidence_bounds()\n",
    "                # pick action with the highest boosted estimated reward\n",
    "                self.action = self.sample_action()\n",
    "                self.actions[t] = self.action\n",
    "                # update approximator\n",
    "                if t %2==0:# self.train_every == 0:\n",
    "                    self.train()\n",
    "                # update exploration indicator A_inv\n",
    "                self.update_A_inv()\n",
    "                #print(t,self.action,self.bandit.best_rewards_oracle[t],self.bandit.rewards[t, self.action])\n",
    "                print('hhhhh',self.bandit.hwithoutconstraints(self.bandit.features[self.iteration][self.action]*np.sqrt(args.card))-1*checkFea(torch.from_numpy(self.bandit.features[self.iteration][self.action])*np.sqrt(args.card)),checkFea(torch.from_numpy(self.bandit.features[self.iteration][self.action])*np.sqrt(args.card)))\n",
    "                if self.best_recommended_reward<self.bandit.hwithoutconstraints(np.sqrt(args.card)*self.bandit.features[self.iteration][self.action]):\n",
    "                    self.best_recommended_reward=self.bandit.hwithoutconstraints(np.sqrt(args.card)*self.bandit.features[self.iteration][self.action])\n",
    "                    self.best_recommended_action=self.bandit.features[self.iteration][self.action]\n",
    "                self.best_sample_rewards_list.append( self.best_rewards_oracle)\n",
    "                self.rewards_list.append(self.bandit.hwithoutconstraints(self.bandit.features[self.iteration][self.action]*np.sqrt(args.card)))\n",
    "                self.feasi.append(checkFea(self.bandit.features[self.iteration][self.action]*np.sqrt(args.card)))\n",
    "                # increment counter\n",
    "                self.iteration += 1\n",
    "                \n",
    "                \n",
    "                # log\n",
    "                # log\n",
    "                n_optimal_arm = np.sum(\n",
    "                    self.actions[:self.iteration]==self.bandit.best_actions_oracle[:self.iteration]\n",
    "                )\n",
    "                postfix['% optimal arm'] = '{:.2%}'.format(n_optimal_arm / self.iteration)\n",
    "                \n",
    "                if t % self.throttle == 0:\n",
    "                    pbar.set_postfix(postfix)\n",
    "                    pbar.update(self.throttle)\n",
    "                if t>=100 and t%20==0:\n",
    "                    self.elite=[]\n",
    "                    if t==100:\n",
    "                        MM=100\n",
    "                    else:\n",
    "                        MM=20\n",
    "                    for j in range(MM):\n",
    "                        \n",
    "                        iteration+=1\n",
    "                        if iteration<100:\n",
    "                            action=[1.]*args.card+[0]*(args.solDim-args.card)\n",
    "                            np.random.shuffle(action)\n",
    "                            xs=[action]\n",
    "                            action=np.array(action)\n",
    "                            ys=[self.model.forward(torch.FloatTensor(action)).detach().squeeze()-constraint*checkFea(action)]\n",
    "                            if j>=18 and checkFea(action)<0.3:\n",
    "                                self.elite.append(np.divide(action,np.sqrt(args.card)))\n",
    "                        else:\n",
    "                            action=actor(1)\n",
    "\n",
    "                            if iteration==60:\n",
    "                                constraint=10#100*sum(rewards)/len(rewards)\n",
    "                                std=np.array(rewards).std()\n",
    "                            if iteration>1 and iteration%60==0:\n",
    "                                constraint=min(100*sum(rewards)/len(rewards),max(0,constraint + 0.005*(-0.0+sum(constraints)/len(constraints))))\n",
    "                            done=1\n",
    "                            action2=[action[i]+0.0002*np.sqrt(np.log(iteration)/history[i]) for i in range(args.solDim)]\n",
    "                            action2=np.array([1 if i in np.argsort(action2)[-args.card:] else 0  for i in range(args.solDim)])\n",
    "                            history+=np.array(action2)\n",
    "                            loss=-critic(action)\n",
    "                            opt1.zero_grad()\n",
    "                            loss.backward()\n",
    "                            opt1.step()\n",
    "                            if j>=18 and checkFea(action2)<0.3:\n",
    "                                self.elite.append(np.divide(action2,np.sqrt(args.card)))\n",
    "                            xs=[action]\n",
    "                            ys=[self.model.forward(torch.FloatTensor(action2)).detach().squeeze()-constraint*checkFea(action2)]\n",
    "                            g2aactionvalues[(iteration-100)%50]=ys[0]\n",
    "                        if iteration>0:\n",
    "                            loss2=criterion(critic(torch.tensor(xs[-1])),torch.tensor(ys[-1]).float())\n",
    "                            opt2.zero_grad()\n",
    "                            loss2.backward()\n",
    "                            opt2.step()\n",
    "                        ##g2a co-training\n",
    "                        if iteration>150 and g2aactionvalues.std()<0.1:\n",
    "                            for _ in range(10):\n",
    "                                action=actor(1)\n",
    "                                loss=0\n",
    "                                for i in range(args.solDim):\n",
    "                                    loss-=(self.best_recommended_action[i]*torch.log(action[i])+(1-self.best_recommended_action[i])*torch.log(1-action[i]))\n",
    "                                loss/=args.solDim\n",
    "                                opt1.zero_grad()\n",
    "                                loss.backward()\n",
    "                                opt1.step()\n",
    "                                \n",
    "                        u=uNext.copy()\n",
    "                        action=[np.random.choice([0,1],size=1,p=[1-u[i],u[i]])[0] if 0<=u[i]<=1 else np.random.choice([0,1],size=1,p=[1/2,1/2])[0] for i in range(args.solDim)]\n",
    "                        reward=sum(action)-20*checkFea(action)\n",
    "                        actions[(rnd%K)]=action\n",
    "                        rewardsCEM[(rnd%K)]=feedback(action,self.model,constraintMean)\n",
    "                        actionsN[(rnd%N)]=action\n",
    "                        rewardsN[(rnd%N)]=feedback(action,self.model,constraintMean)\n",
    "                        constraintN[(rnd%N)]=checkFea(action)\n",
    "                        if reward>min(rewardhistoryBest) and checkFea(action)<0.1:\n",
    "                            actionhistoryBest[np.argwhere(rewardhistoryBest==min(rewardhistoryBest))]=action\n",
    "                            rewardhistoryBest[np.argwhere(rewardhistoryBest==min(rewardhistoryBest))]=feedback(action,self.model,constraintMean)\n",
    "                     #   if reward<max(rewardhistoryWorst) and feedback(action,self.model,constraintMean)<0.15:\n",
    "                          #      actionhistoryWorst[np.argwhere(rewardhistoryWorst==min(rewardhistoryWorst))]=action\n",
    "                           #     rewardhistoryWorst[np.argwhere(rewardhistoryWorst==min(rewardhistoryWorst))]=feedback(action,self.model,constraintMean)\n",
    "                        if rnd%K==0 and rnd%N!=0:\n",
    "                            b=rewardsCEM.mean()\n",
    "                            uNext=torch.tensor(uNext,requires_grad=True)\n",
    "                            optimizer=torch.optim.Adam([uNext],lr=2e-3)\n",
    "                            for tmp in range(5):\n",
    "                                pre=-PPOobj(uNext,u)\n",
    "                                #pre.requires_grad=True\n",
    "                                optimizer.zero_grad()\n",
    "                                pre.backward()\n",
    "                                optimizer.step() \n",
    "                            uNext=uNext.detach().numpy()\n",
    "                            uNext=np.clip(uNext,jitter,1-jitter)\n",
    "                        if rnd%N==0:\n",
    "                            cnt=0\n",
    "                            pointer=0\n",
    "                            elite=[]\n",
    "                            while cnt<int(N*rho) and pointer<N:\n",
    "                                if constraintN[np.argsort(rewardsN)[pointer]]<0.3:\n",
    "                                    elite.append(np.argsort(rewardsN)[pointer])              \n",
    "                                    cnt+=1\n",
    "                                pointer+=1\n",
    "                            if cnt<int(N*rho):\n",
    "                                while cnt<int(N*rho) and pointer<N:\n",
    "                                    if np.argsort(rewardsN)[pointer] not in elite:\n",
    "                                        elite.append(np.argsort(rewardsN)[pointer])\n",
    "                                    pointer+=1\n",
    "                            actionsN[elite]\n",
    "                            tmp=actionsN[elite].reshape([len(elite),args.solDim]).T\n",
    "                            for i in range(args.solDim):\n",
    "                                uNext[i]=max(min(np.exp(-t/L)*uNext[i] +(1-np.exp(-t/L))*np.sum(tmp[i])/len(elite),1-jitter),jitter)\n",
    "                        #    tmphistoryBest=actionhistoryBest.T\n",
    "                         #   for i in range(args.solDim):\n",
    "                          #      uNext[i]=max(min(uNext[i]/2+np.sum(tmphistoryBest[i])/len(elite)/2,1-jitter),jitter)\n",
    "                            constraintMean=constraintN.mean()\n",
    "                        rnd+=1  \n",
    "                        \n",
    "                        # reset if it is the start of episode\n",
    "                        if observation is None:\n",
    "                            observation = deepcopy(env2.reset())\n",
    "                            agent.reset(observation)\n",
    "                        if t==100:\n",
    "                                action = agent.random_action()\n",
    "                        else:    \n",
    "                            # agent pick action ...        \n",
    "                            action = agent.select_action(observation)\n",
    "                        observation2, reward, done, info = env2.step(action,self.model)\n",
    "                        wolpertingeractionvalues[(iteration-100)%50]=reward\n",
    "                        action=observation2.copy()\n",
    "                        if j>=18 and checkFea(action)<0.3:\n",
    "                            self.elite.append(np.divide(action,np.sqrt(args.card)))\n",
    "                            #print('gggggggg',1*(a.dot(action))**1/(np.sqrt(10))**1,checkFea(action))\n",
    "                        # env2 response with next_observation, reward, terminate_info\n",
    "                        if max_episode_length and episode_steps >= max_episode_length -1:\n",
    "                            done = True\n",
    "\n",
    "                        # agent observe and update policy\n",
    "                        agent.observe(reward, observation2, done)\n",
    "                        if step > args2.warmup:\n",
    "                            agent.update_policy()\n",
    "\n",
    "                        # [optional] evaluate\n",
    "                        \n",
    "                        # [optional] save intermideate model\n",
    "                        if step % int(num_iterations/3) == 0:\n",
    "                            agent.save_model(output)\n",
    "\n",
    "                        # update \n",
    "                        step += 1\n",
    "                        \n",
    "                        episode_steps += 1\n",
    "                        episode_reward += reward\n",
    "                        observation = observation2.copy()\n",
    "                        \n",
    "                        if done: # end of episode\n",
    "                            \n",
    "                            agent.memory.append(\n",
    "                                observation,\n",
    "                                agent.select_action([list(observation)]),\n",
    "                                0., False\n",
    "                            )\n",
    "\n",
    "                            # reset\n",
    "                            observation = None\n",
    "                            episode_steps = 0\n",
    "                            episode_reward = 0.\n",
    "                            episode += 1\n",
    "                        ##wolpertinger co-training\n",
    "                        if iteration>150 and wolpertingeractionvalues.std()<0.1:\n",
    "                            for _ in range(10):\n",
    "                                observation = observation2.copy()\n",
    "                                action = agent.select_action2([list(observation)])\n",
    "                                loss=0\n",
    "                                for i in range(args.solDim):\n",
    "                                    loss-=(self.best_recommended_action[i]*torch.log(action[i])+(1-self.best_recommended_action[i])*torch.log(1-action[i]))\n",
    "                                loss/=args.solDim\n",
    "                                agent.actor_optim.zero_grad()\n",
    "                                loss.backward()\n",
    "                                agent.actor_optim.step()\n",
    "                    calactionhistoryBest.append(len(actionhistoryBest))\n",
    "                    for i in actionhistoryBest:\n",
    "                        self.elite.append(np.divide(i,np.sqrt(args.card)))\n",
    "                        #print('--',1*(a.dot(action))**1/(np.sqrt(10))**1,checkFea(action))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import itertools\n",
    "\n",
    "\n",
    "class ContextualBandit():\n",
    "    def __init__(self,\n",
    "                 T,\n",
    "                 n_arms,\n",
    "                 n_features,\n",
    "                 hwithoutconstraints,\n",
    "                 noise_std=1.0,\n",
    "                ):\n",
    "        # number of rounds\n",
    "         # number of rounds\n",
    "        self.T = T\n",
    "        # number of arms\n",
    "        self.n_arms = n_arms\n",
    "        # number of features for each arm\n",
    "        self.n_features = n_features\n",
    "        # average reward function\n",
    "        # h : R^d -> R\n",
    "        self.hwithoutconstraints=hwithoutconstraints\n",
    "        self.knn_metabestValue=-100\n",
    "        self.best_rewards_oracle = -100\n",
    "        # standard deviation of Gaussian reward noise\n",
    "        self.noise_std = noise_std\n",
    "        # generate random features\n",
    "        self.reset()\n",
    "    @property\n",
    "    def arms(self):\n",
    "        \"\"\"Return [0, ...,n_arms-1]\n",
    "        \"\"\"\n",
    "        return range(self.n_arms)\n",
    "        \n",
    "    def reset(self):\n",
    "        \"\"\"Generate new features and new rewards.\n",
    "        \"\"\"\n",
    "        self.reset_features()\n",
    "        self.reset_rewards()\n",
    "\n",
    "    def reset_features(self):\n",
    "        \"\"\"Generate normalized random N(0,1) features.\n",
    "        \"\"\"\n",
    "        x1,x2=[],[]\n",
    "        for i in range(self.T):\n",
    "            tmp1,tmp2=[],[]\n",
    "            for j in range(self.n_arms):\n",
    "                a=np.array([0]*(self.n_features-args.card)+[1]*args.card)\n",
    "                np.random.shuffle(a)\n",
    "                a=np.divide(a,np.sqrt(args.card))\n",
    "                tmp1.append(a)\n",
    "            for j in range(self.n_features):\n",
    "                a=np.array([0.0]*j+[1.0]+[0.0]*(self.n_features-j-1))\n",
    "                tmp2.append(a)\n",
    "            x1.append(tmp1)\n",
    "            x2.append(tmp2)\n",
    "        x1,x2=np.array(x1),np.array(x2)\n",
    "        #x1 /= np.repeat(np.linalg.norm(x1, axis=-1, ord=2), self.n_features).reshape(self.T, self.n_arms, self.n_features)\n",
    "        self.features = x1\n",
    "        self.meta_features=x2\n",
    "        self.cross_features=[]\n",
    "        for i in range(self.n_features):\n",
    "            for j in range(self.n_features):\n",
    "                tmp=np.zeros(self.n_features)\n",
    "                tmp[i]=1\n",
    "                tmp[j]=1\n",
    "                self.cross_features.append(tmp)\n",
    "                \n",
    "\n",
    "    def reset_rewards(self):\n",
    "        \"\"\"Generate rewards for each arm and each round,\n",
    "        following the reward function h + Gaussian noise.\n",
    "        \"\"\"\n",
    "        \n",
    "        self.rewards = np.random.random([self.T, self.n_arms])\n",
    "        # to be used only to compute regret, NOT by the algorithm itself\n",
    "        self.best_rewards_oracle =max( max(np.max(self.rewards, axis=1)),self.knn_metabestValue)\n",
    "        self.best_actions_oracle = np.argmax(self.rewards, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralUCB(UCB):\n",
    "    \"\"\"Neural UCB.\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 bandit,\n",
    "                 hidden_size=20,\n",
    "                 n_layers=2,\n",
    "                 reg_factor=1.0,\n",
    "                 delta=0.01,\n",
    "                 confidence_scaling_factor=-1.0,\n",
    "                 training_window=100,\n",
    "                 p=0.0,\n",
    "                 learning_rate=0.01,\n",
    "                 epochs=1,\n",
    "                 train_every=1,\n",
    "                 throttle=1,\n",
    "                 use_cuda=False,\n",
    "                ):\n",
    "\n",
    "        # hidden size of the NN layers\n",
    "        self.hidden_size = hidden_size\n",
    "        # number of layers\n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "        # number of rewards in the training buffer\n",
    "        self.training_window = training_window\n",
    "        \n",
    "        # NN parameters\n",
    "        self.learning_rate = learning_rate\n",
    "        self.epochs = epochs\n",
    "        \n",
    "        self.use_cuda = use_cuda\n",
    "        if self.use_cuda:\n",
    "            raise Exception(\n",
    "                'Not yet CUDA compatible : TODO for later (not necessary to obtain good results')\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() and self.use_cuda else 'cpu')\n",
    "    \n",
    "        # dropout rate\n",
    "        self.p = p\n",
    "\n",
    "        # neural network\n",
    "        self.model = Model(input_size=bandit.n_features, \n",
    "                           hidden_size=self.hidden_size,\n",
    "                           n_layers=self.n_layers,\n",
    "                           p=self.p\n",
    "                          ).to(self.device)\n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=self.learning_rate)\n",
    "\n",
    "        super().__init__(bandit, \n",
    "                         reg_factor=reg_factor,\n",
    "                         confidence_scaling_factor=confidence_scaling_factor,\n",
    "                         delta=delta,\n",
    "                         throttle=throttle,\n",
    "                         train_every=train_every,\n",
    "                        )\n",
    "\n",
    "    @property\n",
    "    def approximator_dim(self):\n",
    "        \"\"\"Sum of the dimensions of all trainable layers in the network.\n",
    "        \"\"\"\n",
    "        return sum(w.numel() for w in self.model.parameters() if w.requires_grad)\n",
    "    \n",
    "    @property\n",
    "    def confidence_multiplier(self):\n",
    "        \"\"\"Constant equal to confidence_scaling_factor\n",
    "        \"\"\"\n",
    "        return self.confidence_scaling_factor\n",
    "    \n",
    "    def update_output_gradient(self):\n",
    "        \"\"\"Get gradient of network prediction w.r.t network weights.\n",
    "        \"\"\"\n",
    "        for a in self.bandit.arms:\n",
    "            x = torch.FloatTensor(\n",
    "                self.bandit.features[self.iteration, a].reshape(1,-1)\n",
    "            ).to(self.device)\n",
    "            \n",
    "            self.model.zero_grad()\n",
    "            y = self.model(x)\n",
    "            y.backward()\n",
    "            \n",
    "            self.grad_approx[a] = torch.cat(\n",
    "                [w.grad.detach().flatten() / np.sqrt(self.hidden_size) for w in self.model.parameters() if w.requires_grad]\n",
    "            ).to(self.device)\n",
    "            \n",
    "    def reset(self):\n",
    "        \"\"\"Reset the internal estimates.\n",
    "        \"\"\"\n",
    "        self.reset_upper_confidence_bounds()\n",
    "        self.reset_actions()\n",
    "        self.reset_A_inv()\n",
    "        self.reset_grad_approx()\n",
    "        self.iteration = 0\n",
    "\n",
    "    def train(self):\n",
    "        \"\"\"Train neural approximator.\n",
    "        \"\"\"\n",
    "        iterations_so_far = range(np.max([0, self.iteration-self.training_window]), self.iteration+1)\n",
    "        actions_so_far = self.actions[np.max([0, self.iteration-self.training_window]):self.iteration+1]\n",
    "\n",
    "        x_train = torch.FloatTensor(self.bandit.features[iterations_so_far, actions_so_far]).to(self.device)\n",
    "        y_train = torch.FloatTensor(self.bandit.rewards[iterations_so_far, actions_so_far]).squeeze().to(self.device)\n",
    "        \n",
    "        # train mode\n",
    "        self.model.train()\n",
    "        for _ in range(self.epochs):\n",
    "            y_pred = self.model.forward(x_train).squeeze()\n",
    "            loss = nn.MSELoss()(y_train, y_pred)\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "        \n",
    "        \n",
    "    def predict(self):\n",
    "        \"\"\"Predict reward.\n",
    "        \"\"\"\n",
    "        # eval mode\n",
    "        self.model.eval()\n",
    "        self.mu_hat= self.model.forward(\n",
    "            torch.FloatTensor(self.bandit.features[self.iteration]).to(self.device)\n",
    "        ).detach().squeeze()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "T = int(5e3)\n",
    "n_arms =10\n",
    "n_features = args.solDim\n",
    "noise_std = 0.5\n",
    "\n",
    "confidence_scaling_factor = noise_std\n",
    "\n",
    "n_sim = 1\n",
    "\n",
    "p = 0.2\n",
    "hidden_size = 4#16\n",
    "epochs = 100#100\n",
    "train_every = 10#10\n",
    "confidence_scaling_factor = 1.0\n",
    "use_cuda = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "bandit = ContextualBandit(T, n_arms, n_features,hwithoutconstraints, noise_std=noise_std)\n",
    "\n",
    "regrets = np.empty((n_sim, T))\n",
    "\n",
    "for i in range(n_sim):\n",
    "    bandit.reset_rewards()\n",
    "    model = NeuralUCB(bandit,\n",
    "                      hidden_size=hidden_size,\n",
    "                      reg_factor=1.0,\n",
    "                      delta=0.1,\n",
    "                      confidence_scaling_factor=confidence_scaling_factor,\n",
    "                      training_window=100,\n",
    "                      p=p,\n",
    "                      learning_rate=0.01,\n",
    "                      epochs=epochs,\n",
    "                      train_every=train_every,\n",
    "                      use_cuda=use_cuda\n",
    "                     )\n",
    "    model.run()\n",
    "    regrets[i] = np.cumsum(model.regrets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.save('samplingpercentage_fm.npy',model.actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('reward_mixpolicy_para10_realorderyahoo.npy',mofm.rewards_list)\n",
    "np.save('constraint_mixpolicy_para10_realorderyahoo.npy',mofm.feasi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with gurobipy.Env(empty=True) as env:\n",
    "    env.setParam('OutputFlag', 0)\n",
    "    env.setParam('IterationLimit',600)\n",
    "    env.start()\n",
    "    with gurobipy.Model(env=env) as m:\n",
    "        solverSol=np.divide(solver(np.array([1]*20),m),np.sqrt(args.card)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
