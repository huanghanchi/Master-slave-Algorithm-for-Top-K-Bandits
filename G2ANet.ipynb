{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "fixedItemSet=np.load(r\"hetrec2011-lastfm-2k\\frequentItemVec.npy\")\n",
    "userflow=np.load(r'hetrec2011-lastfm-2k\\userflow.npy')\n",
    "userclick=np.load(r'hetrec2011-lastfm-2k\\userclick.npy',allow_pickle=True)\n",
    "\n",
    "def hwithoutconstraints(action):\n",
    "    if type(action)==torch.Tensor:\n",
    "        action=action.detach().numpy()\n",
    "    items=np.argwhere(np.array(action)>0.5).reshape(args.card)\n",
    "    if iteration<args.card:\n",
    "        itemlist=userclick[0][:args.card]\n",
    "    else:\n",
    "        itemlist=userclick[0][(iteration-args.card):min((iteration+args.card),len(userclick[0]))]\n",
    "    cnt2=0\n",
    "    for i in items:\n",
    "        cnt2+=( itemlist.count(i))\n",
    "    return cnt2/(args.card)\n",
    "\n",
    "import argparse\n",
    "from copy import deepcopy\n",
    "import torch\n",
    "import gym\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\t\n",
    "import torch.nn.functional as F\t\n",
    "import torch.optim as optim\t\n",
    "import numpy as np\n",
    "\n",
    "import argparse\n",
    "import sys\n",
    "\n",
    "import copy\n",
    "import numpy as np\n",
    "from numpy.random import permutation\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "sys.path.insert(0,'/root/gurobi903/linux64/lib/python3.6_utf32/gurobipy')\n",
    "\n",
    "class para:\n",
    "    def __init__(self):\n",
    "        self.batch_size=16\n",
    "        self.wmc=10\n",
    "        self.solDim=40\n",
    "        self.card=10\n",
    "        self.test_batch_size=1000\n",
    "        self.epochs=10\n",
    "        self.lr=0.1\n",
    "        self.momentum=0.5\n",
    "        self.no_cuda='store_true'\n",
    "        self.seed=1\n",
    "        self.log_interval=10\n",
    "        self.hidden_dim=10\n",
    "args=para()\n",
    "thre=1\n",
    "legalList={}\n",
    "for i in range(40):\n",
    "    legalList[i]=[]\n",
    "for i in range(40):\n",
    "    for j in range(40):\n",
    "        if i!=j and np.linalg.norm(fixedItemSet[i]-fixedItemSet[j])<thre:\n",
    "            legalList[i].append(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import argparse\n",
    "from copy import deepcopy\n",
    "import torch\n",
    "import gym\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\t\n",
    "import torch.nn.functional as F\t\n",
    "import torch.optim as optim\t\n",
    "import numpy as np\n",
    "\n",
    "import argparse\n",
    "import sys\n",
    "\n",
    "import copy\n",
    "import numpy as np\n",
    "from numpy.random import permutation\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "sys.path.insert(0,'/root/gurobi903/linux64/lib/python3.6_utf32/gurobipy')\n",
    "\n",
    "\n",
    "def checkFea(x):\n",
    "    cnt=0\n",
    "    cntT=0\n",
    "    for i in range(len(legalList.keys())):\n",
    "        for j in legalList[i]:\n",
    "            cntT+=1\n",
    "            if x[j]+x[i]>1:\n",
    "                cnt+=1\n",
    "    return cnt/cntT/2+abs(sum(x)-args.card)/args.card/2\n",
    "def CB(alpha,x,M):\n",
    "    return alpha*np.sqrt(np.dot(np.dot(x.T,np.linalg.inv(M)),x))\n",
    "def solver(theta,MODEL):\n",
    "\n",
    "    import gurobipy\n",
    "    import time\n",
    "    s=time.time()\n",
    "    \n",
    "    variables=[]\n",
    "    for i in range(args.solDim):\n",
    "        variables.append(MODEL.addVar(vtype=gurobipy.GRB.BINARY, name='x'+'i'))\n",
    "    \n",
    "    MODEL.update()\n",
    "\n",
    "    \n",
    "    MODEL.setObjective(np.array(variables).dot(theta), sense=gurobipy.GRB.MAXIMIZE)\n",
    "\n",
    "    \n",
    "    cnt=0\n",
    "    for i in range(args.solDim):\n",
    "        if len(legalList)>0:\n",
    "            for j in legalList[i]:\n",
    "                MODEL.addConstr(-variables[i]-variables[j] >= -1, name=str(cnt))\n",
    "                cnt+=1\n",
    "    MODEL.addConstr(sum(variables) == args.card, name=str(cnt))\n",
    "    f=time.time()\n",
    "    \n",
    "    MODEL.optimize()\n",
    "    return np.array(MODEL.x)\n",
    "def solver_quad(Q,MODEL):\n",
    "\n",
    "    import gurobipy\n",
    "    import time\n",
    "    s=time.time()\n",
    "    \n",
    "    variables=[]\n",
    "    for i in range(args.solDim):\n",
    "        variables.append(MODEL.addVar(vtype=gurobipy.GRB.BINARY, name='x'+'i'))\n",
    "    \n",
    "    MODEL.update()\n",
    "\n",
    "    \n",
    "    MODEL.setObjective(np.array(variables).dot(Q).dot(np.array(variables)), sense=gurobipy.GRB.MAXIMIZE)\n",
    "\n",
    "    \n",
    "    cnt=0\n",
    "    for i in range(args.solDim):\n",
    "        if len(legalList)>0:\n",
    "            for j in legalList[i]:\n",
    "                MODEL.addConstr(-variables[i]-variables[j] >= -1, name=str(cnt))\n",
    "                cnt+=1\n",
    "    MODEL.addConstr(sum(variables) == args.card, name=str(cnt))\n",
    "    f=time.time()\n",
    "    \n",
    "    \n",
    "    print('??')\n",
    "    MODEL.optimize()\n",
    "    print('??')\n",
    "    return np.array(MODEL.x)\n",
    "class myenv2(gym.Env):\n",
    "    metadata = {\n",
    "        'render.modes': ['human', 'rgb_array'],\n",
    "        'video.frames_per_second': 50\n",
    "    }\n",
    "\n",
    "    def __init__(self):\n",
    "\n",
    "        \n",
    "        \n",
    "\n",
    "        self.steps = torch.tensor(np.array([0.1]*args.solDim))\n",
    "        self._max_episode_steps = 500\n",
    "        self.iteration=1\n",
    "        self.constraints=[]\n",
    "        self.constraint=0\n",
    "        self.history=np.ones(args.solDim)\n",
    "    def seed(self, seed=None):\n",
    "        self.np_random, seed = seeding.np_random(seed)\n",
    "        return [seed]\n",
    "\n",
    "    def step(self, action):\n",
    "        self.iteration+=1 \n",
    "        done=1\n",
    "        action=[action[i]+0.0*np.sqrt(np.log(self.iteration)/self.history[i]) for i in range(args.solDim)]\n",
    "        action=np.array([1 if i in np.argsort(action)[-args.card:] else 0  for i in range(args.solDim)])\n",
    "\n",
    "        \n",
    "  \n",
    "   \n",
    "    \n",
    "        \n",
    "        \n",
    "        reward=hwithoutconstraints(action)-10*checkFea(action)\n",
    "        rewards.append(hwithoutconstraints(action))\n",
    "        constraints.append(checkFea(action))\n",
    "        self.history+=np.array(action)\n",
    "        print(self.iteration,1*hwithoutconstraints(action),checkFea(action))\n",
    "        \n",
    "        return action, reward, done, {}\n",
    "\n",
    "    def reset(self):\n",
    "        return  np.array(np.array([0.1]*args.solDim)),\n",
    "\n",
    "    def render(self, mode='human'):\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.MSELoss()\n",
    "class g2apara:\n",
    "    def __init__(self):\n",
    "        self.rnn_hidden_dim=8\n",
    "        self.n_agents=args.solDim\n",
    "        self.attention_dim=8\n",
    "        self.cuda=False\n",
    "        self.n_actions=2\n",
    "        self.hard=False\n",
    "args3=g2apara()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "hids=torch.from_numpy(np.random.random([args.solDim,args3.rnn_hidden_dim])).float()\n",
    "rewards=[]\n",
    "constraints=[]\n",
    "import gym\n",
    "\n",
    "\n",
    "class Normalizedenv2(gym.ActionWrapper):\n",
    "    \"\"\" Wrap action \"\"\"\n",
    "\n",
    "    def action(self, action):\n",
    "        act_k = (self.action_space.high - self.action_space.low)/ 2.\n",
    "        act_b = (self.action_space.high + self.action_space.low)/ 2.\n",
    "        return act_k * action + act_b\n",
    "\n",
    "    def reverse_action(self, action):\n",
    "        act_k_inv = 2./(self.action_space.high - self.action_space.low)\n",
    "        act_b = (self.action_space.high + self.action_space.low)/ 2.\n",
    "        return act_k_inv * (action - act_b)\n",
    "import os\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "\n",
    "USE_CUDA = torch.cuda.is_available()\n",
    "FLOAT = torch.cuda.FloatTensor if USE_CUDA else torch.FloatTensor\n",
    "\n",
    "\n",
    "def to_numpy(var):\n",
    "    return var.cpu().data.numpy() if USE_CUDA else var.data.numpy()\n",
    "\n",
    "def to_tensor(ndarray, volatile=False, requires_grad=False, dtype=FLOAT):\n",
    "    return Variable(\n",
    "        torch.from_numpy(ndarray), volatile=volatile, requires_grad=requires_grad\n",
    "    ).type(dtype)\n",
    "\n",
    "def soft_update(target, source, tau):\n",
    "    for target_param, param in zip(target.parameters(), source.parameters()):\n",
    "        target_param.data.copy_(\n",
    "            target_param.data * (1.0 - tau) + param.data * tau\n",
    "        )\n",
    "\n",
    "def hard_update(target, source):\n",
    "    for target_param, param in zip(target.parameters(), source.parameters()):\n",
    "            target_param.data.copy_(param.data)\n",
    "\n",
    "def get_output_folder(parent_dir, env2_name):\n",
    "    \"\"\"Return save folder.\n",
    "    Assumes folders in the parent_dir have suffix -run{run\n",
    "    number}. Finds the highest run number and sets the output folder\n",
    "    to that number + 1. This is just convenient so that if you run the\n",
    "    same script multiple times tensorboard can plot all of the results\n",
    "    on the same plots with different names.\n",
    "    Parameters\n",
    "    ----------\n",
    "    parent_dir: str\n",
    "      Path of the directory containing all experiment runs.\n",
    "    Returns\n",
    "    -------\n",
    "    parent_dir/run_dir\n",
    "      Path to this run's save directory.\n",
    "    \"\"\"\n",
    "    os.makedirs(parent_dir, exist_ok=True)\n",
    "    experiment_id = 0\n",
    "    for folder_name in os.listdir(parent_dir):\n",
    "        if not os.path.isdir(os.path.join(parent_dir, folder_name)):\n",
    "            continue\n",
    "        try:\n",
    "            folder_name = int(folder_name.split('-run')[-1])\n",
    "            if folder_name > experiment_id:\n",
    "                experiment_id = folder_name\n",
    "        except:\n",
    "            pass\n",
    "    experiment_id += 1\n",
    "\n",
    "    parent_dir = os.path.join(parent_dir, env2_name)\n",
    "    parent_dir = parent_dir + '-run{}'.format(experiment_id)\n",
    "    os.makedirs(parent_dir, exist_ok=True)\n",
    "    return parent_dir\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.io import savemat\n",
    "class Evaluator(object):\n",
    "\n",
    "    def __init__(self, num_episodes, interval, save_path='', max_episode_length=None):\n",
    "        self.num_episodes = num_episodes\n",
    "        self.max_episode_length = max_episode_length\n",
    "        self.interval = interval\n",
    "        self.save_path = save_path\n",
    "        self.results = np.array([]).reshape(num_episodes,0)\n",
    "\n",
    "    def __call__(self, env2, policy, debug=False, visualize=False, save=True):\n",
    "\n",
    "        self.is_training = False\n",
    "        observation = None\n",
    "        result = []\n",
    "\n",
    "        for episode in range(self.num_episodes):\n",
    "\n",
    "            \n",
    "            observation = env2.reset()\n",
    "            episode_steps = 0\n",
    "            episode_reward = 0.\n",
    "\n",
    "            assert observation is not None\n",
    "\n",
    "            \n",
    "            done = False\n",
    "            while not done:\n",
    "                \n",
    "                action = policy(observation)\n",
    "\n",
    "                observation, reward, done, info = env2.step(action)\n",
    "                if self.max_episode_length and episode_steps >= self.max_episode_length -1:\n",
    "                    done = True\n",
    "\n",
    "                \n",
    "                episode_reward += reward\n",
    "                episode_steps += 1\n",
    "            result.append(episode_reward)\n",
    "\n",
    "        result = np.array(result).reshape(-1,1)\n",
    "        self.results = np.hstack([self.results, result])\n",
    "\n",
    "        if save:\n",
    "            self.save_results('{}/validate_reward'.format(self.save_path))\n",
    "        return np.mean(result)\n",
    "\n",
    "    def save_results(self, fn):\n",
    "\n",
    "        y = np.mean(self.results, axis=0)\n",
    "        error=np.std(self.results, axis=0)\n",
    "\n",
    "        x = range(0,self.results.shape[1]*self.interval,self.interval)\n",
    "        fig, ax = plt.subplots(1, 1, figsize=(6, 5))\n",
    "        plt.xlabel('Timestep')\n",
    "        plt.ylabel('Average Reward')\n",
    "        ax.errorbar(x, y, yerr=error, fmt='-o')\n",
    "        plt.savefig(fn+'.png')\n",
    "        savemat(fn+'.mat', {'reward':self.results})\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "\n",
    "def fanin_init(size, fanin=None):\n",
    "    fanin = fanin or size[0]\n",
    "    v = 1. / np.sqrt(fanin)\n",
    "    return torch.Tensor(size).uniform_(-v, v)\n",
    "\n",
    "class Actor(nn.Module):\n",
    "    def __init__(self, input_shape, args):\n",
    "        super(Actor, self).__init__()\n",
    "\n",
    "        \n",
    "        self.encoding = nn.Linear(input_shape, args.rnn_hidden_dim)  \n",
    "        self.h = nn.GRUCell(args.rnn_hidden_dim, args.rnn_hidden_dim)  \n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        self.hard_bi_GRU = nn.GRU(args.rnn_hidden_dim * 2, args.rnn_hidden_dim, bidirectional=True)\n",
    "        \n",
    "        self.hard_encoding = nn.Linear(args.rnn_hidden_dim * 2, 2)  \n",
    "\n",
    "        \n",
    "        self.q = nn.Linear(args.rnn_hidden_dim, args.attention_dim, bias=False)\n",
    "        self.k = nn.Linear(args.rnn_hidden_dim, args.attention_dim, bias=False)\n",
    "        self.v = nn.Linear(args.rnn_hidden_dim, args.attention_dim)\n",
    "\n",
    "        \n",
    "        self.decoding = nn.Linear(args.rnn_hidden_dim + args.attention_dim, args.n_actions)\n",
    "        self.args = args\n",
    "        self.input_shape = input_shape\n",
    "        self.softmax=F.softmax\n",
    "\n",
    "    def forward(self,state, obs=torch.tensor([1.]*args.solDim).view(args.solDim,1), hidden_state=hids):\n",
    "        size = obs.shape[0]  \n",
    "        \n",
    "        obs_encoding = F.relu(self.encoding(obs))\n",
    "        h_in = hidden_state.reshape(-1, self.args.rnn_hidden_dim)\n",
    "\n",
    "        \n",
    "        h_out = self.h(obs_encoding, h_in)  \n",
    "\n",
    "        \n",
    "        if self.args.hard:\n",
    "            \n",
    "            h = h_out.reshape(-1, self.args.n_agents, self.args.rnn_hidden_dim)  \n",
    "            input_hard = []\n",
    "            for i in range(self.args.n_agents):\n",
    "                h_i = h[:, i]  \n",
    "                h_hard_i = []\n",
    "                for j in range(self.args.n_agents):  \n",
    "                    if j != i:\n",
    "                        h_hard_i.append(torch.cat([h_i, h[:, j]], dim=-1))\n",
    "                \n",
    "                h_hard_i = torch.stack(h_hard_i, dim=0)\n",
    "                input_hard.append(h_hard_i)\n",
    "            \n",
    "            input_hard = torch.stack(input_hard, dim=-2)\n",
    "            \n",
    "            input_hard = input_hard.view(self.args.n_agents - 1, -1, self.args.rnn_hidden_dim * 2)\n",
    "\n",
    "            h_hard = torch.zeros((2 * 1, size, self.args.rnn_hidden_dim))  \n",
    "            if self.args.cuda:\n",
    "                h_hard = h_hard.cuda()\n",
    "            h_hard, _ = self.hard_bi_GRU(input_hard, h_hard)  \n",
    "            h_hard = h_hard.permute(1, 0, 2)  \n",
    "            h_hard = h_hard.reshape(-1, self.args.rnn_hidden_dim * 2)  \n",
    "\n",
    "            \n",
    "            hard_weights = self.hard_encoding(h_hard)\n",
    "            hard_weights = F.gumbel_softmax(hard_weights, tau=0.01)\n",
    "            \n",
    "            hard_weights = hard_weights[:, 1].view(-1, self.args.n_agents, 1, self.args.n_agents - 1)\n",
    "            hard_weights = hard_weights.permute(1, 0, 2, 3)\n",
    "\n",
    "        else:\n",
    "            hard_weights = torch.ones((self.args.n_agents, size // self.args.n_agents, 1, self.args.n_agents - 1))\n",
    "            if self.args.cuda:\n",
    "                hard_weights = hard_weights.cuda()\n",
    "\n",
    "        \n",
    "        q = self.q(h_out).reshape(-1, self.args.n_agents, self.args.attention_dim)  \n",
    "        k = self.k(h_out).reshape(-1, self.args.n_agents, self.args.attention_dim)  \n",
    "        v = F.relu(self.v(h_out)).reshape(-1, self.args.n_agents, self.args.attention_dim)  \n",
    "        x = []\n",
    "        for i in range(self.args.n_agents):\n",
    "            q_i = q[:, i].view(-1, 1, self.args.attention_dim)  \n",
    "            k_i = [k[:, j] for j in range(self.args.n_agents) if j != i]  \n",
    "            v_i = [v[:, j] for j in range(self.args.n_agents) if j != i]  \n",
    "\n",
    "            k_i = torch.stack(k_i, dim=0)  \n",
    "            k_i = k_i.permute(1, 2, 0)  \n",
    "            v_i = torch.stack(v_i, dim=0)\n",
    "            v_i = v_i.permute(1, 2, 0)\n",
    "\n",
    "            \n",
    "            score = torch.matmul(q_i, k_i)\n",
    "\n",
    "            \n",
    "            scaled_score = score / np.sqrt(self.args.attention_dim)\n",
    "\n",
    "            \n",
    "            soft_weight = F.softmax(scaled_score, dim=-1)  \n",
    "\n",
    "            \n",
    "            x_i = (v_i * soft_weight * hard_weights[i]).sum(dim=-1)\n",
    "            x.append(x_i)\n",
    "\n",
    "        \n",
    "        x = torch.stack(x, dim=1).reshape(-1, self.args.attention_dim)  \n",
    "        final_input = torch.cat([h_out, x], dim=-1)\n",
    "        output = self.softmax(1*self.decoding(final_input))\n",
    "        return torch.cat(([i[0].view(1) for i in output]),0)\n",
    "\n",
    "class Critic(nn.Module):\n",
    "    def __init__(self, nb_states, nb_actions, hidden1=400, hidden2=300, init_w=3e-3):\n",
    "        super(Critic, self).__init__()\n",
    "        self.fc1 = nn.Linear(nb_states, hidden1)\n",
    "        self.fc2 = nn.Linear(hidden1 +nb_actions, hidden2)\n",
    "        self.fc3 = nn.Linear(hidden2, 1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.init_weights(init_w)\n",
    "\n",
    "    def init_weights(self, init_w):\n",
    "        self.fc1.weight.data = fanin_init(self.fc1.weight.data.size())\n",
    "        self.fc2.weight.data = fanin_init(self.fc2.weight.data.size())\n",
    "        self.fc3.weight.data.uniform_(-init_w, init_w)\n",
    "\n",
    "    def forward(self, xs):\n",
    "        x, a = xs\n",
    "        out = self.fc1(x)\n",
    "        out = self.relu(out)\n",
    "        \n",
    "        if len(a.shape)==1:\n",
    "            a=a.view([1,a.shape[0]])\n",
    "        out = self.fc2(torch.cat([out,a],1))\n",
    "        out = self.relu(out)\n",
    "        out = self.fc3(out)\n",
    "        return out\n",
    "\n",
    "from collections import deque, namedtuple\n",
    "import warnings\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Experience = namedtuple('Experience', 'state0, action, reward, state1, terminal1')\n",
    "\n",
    "\n",
    "def sample_batch_indexes(low, high, size):\n",
    "    if high - low >= size:\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        try:\n",
    "            r = xrange(low, high)\n",
    "        except NameError:\n",
    "            r = range(low, high)\n",
    "        batch_idxs = random.sample(r, size)\n",
    "    else:\n",
    "        \n",
    "        \n",
    "        \n",
    "        warnings.warn('Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!')\n",
    "        batch_idxs = np.random.random_integers(low, high - 1, size=size)\n",
    "    assert len(batch_idxs) == size\n",
    "    return batch_idxs\n",
    "\n",
    "\n",
    "class RingBuffer(object):\n",
    "    def __init__(self, maxlen):\n",
    "        self.maxlen = maxlen\n",
    "        self.start = 0\n",
    "        self.length = 0\n",
    "        self.data = [None for _ in range(maxlen)]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if idx < 0 or idx >= self.length:\n",
    "            raise KeyError()\n",
    "        return self.data[(self.start + idx) % self.maxlen]\n",
    "\n",
    "    def append(self, v):\n",
    "        if self.length < self.maxlen:\n",
    "            \n",
    "            self.length += 1\n",
    "        elif self.length == self.maxlen:\n",
    "            \n",
    "            self.start = (self.start + 1) % self.maxlen\n",
    "        else:\n",
    "            \n",
    "            raise RuntimeError()\n",
    "        self.data[(self.start + self.length - 1) % self.maxlen] = v\n",
    "\n",
    "\n",
    "def zeroed_observation(observation):\n",
    "    if hasattr(observation, 'shape'):\n",
    "        return np.zeros(observation.shape)\n",
    "    elif hasattr(observation, '__iter__'):\n",
    "        out = []\n",
    "        for x in observation:\n",
    "            out.append(zeroed_observation(x))\n",
    "        return out\n",
    "    else:\n",
    "        return 0.\n",
    "\n",
    "\n",
    "class Memory(object):\n",
    "    def __init__(self, window_length, ignore_episode_boundaries=False):\n",
    "        self.window_length = window_length\n",
    "        self.ignore_episode_boundaries = ignore_episode_boundaries\n",
    "\n",
    "        self.recent_observations = deque(maxlen=window_length)\n",
    "        self.recent_terminals = deque(maxlen=window_length)\n",
    "\n",
    "    def sample(self, batch_size, batch_idxs=None):\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def append(self, observation, action, reward, terminal, training=True):\n",
    "        self.recent_observations.append(observation)\n",
    "        self.recent_terminals.append(terminal)\n",
    "\n",
    "    def get_recent_state(self, current_observation):\n",
    "        \n",
    "        \n",
    "        \n",
    "        state = [current_observation]\n",
    "        idx = len(self.recent_observations) - 1\n",
    "        for offset in range(0, self.window_length - 1):\n",
    "            current_idx = idx - offset\n",
    "            current_terminal = self.recent_terminals[current_idx - 1] if current_idx - 1 >= 0 else False\n",
    "            if current_idx < 0 or (not self.ignore_episode_boundaries and current_terminal):\n",
    "                \n",
    "                \n",
    "                break\n",
    "            state.insert(0, self.recent_observations[current_idx])\n",
    "        while len(state) < self.window_length:\n",
    "            state.insert(0, zeroed_observation(state[0]))\n",
    "        return state\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {\n",
    "            'window_length': self.window_length,\n",
    "            'ignore_episode_boundaries': self.ignore_episode_boundaries,\n",
    "        }\n",
    "        return config\n",
    "\n",
    "class SequentialMemory(Memory):\n",
    "    def __init__(self, limit, **kwargs):\n",
    "        super(SequentialMemory, self).__init__(**kwargs)\n",
    "\n",
    "        self.limit = limit\n",
    "\n",
    "        \n",
    "        \n",
    "        self.actions = RingBuffer(limit)\n",
    "        self.rewards = RingBuffer(limit)\n",
    "        self.terminals = RingBuffer(limit)\n",
    "        self.observations = RingBuffer(limit)\n",
    "\n",
    "    def sample(self, batch_size, batch_idxs=None):\n",
    "        if batch_idxs is None:\n",
    "            \n",
    "            \n",
    "            batch_idxs = sample_batch_indexes(0, self.nb_entries - 1, size=batch_size)\n",
    "        batch_idxs = np.array(batch_idxs) + 1\n",
    "        assert np.min(batch_idxs) >= 1\n",
    "        assert np.max(batch_idxs) < self.nb_entries\n",
    "        assert len(batch_idxs) == batch_size\n",
    "\n",
    "        \n",
    "        experiences = []\n",
    "        for idx in batch_idxs:\n",
    "            terminal0 = self.terminals[idx - 2] if idx >= 2 else False\n",
    "            while terminal0:\n",
    "                \n",
    "                \n",
    "                \n",
    "                idx = sample_batch_indexes(1, self.nb_entries, size=1)[0]\n",
    "                terminal0 = self.terminals[idx - 2] if idx >= 2 else False\n",
    "            assert 1 <= idx < self.nb_entries\n",
    "\n",
    "            \n",
    "            \n",
    "            \n",
    "            state0 = [self.observations[idx - 1]]\n",
    "            for offset in range(0, self.window_length - 1):\n",
    "                current_idx = idx - 2 - offset\n",
    "                current_terminal = self.terminals[current_idx - 1] if current_idx - 1 > 0 else False\n",
    "                if current_idx < 0 or (not self.ignore_episode_boundaries and current_terminal):\n",
    "                    \n",
    "                    \n",
    "                    break\n",
    "                state0.insert(0, self.observations[current_idx])\n",
    "            while len(state0) < self.window_length:\n",
    "                state0.insert(0, zeroed_observation(state0[0]))\n",
    "            action = self.actions[idx - 1]\n",
    "            reward = self.rewards[idx - 1]\n",
    "            terminal1 = self.terminals[idx - 1]\n",
    "\n",
    "            \n",
    "            \n",
    "            \n",
    "            state1 = [np.copy(x) for x in state0[1:]]\n",
    "            state1.append(self.observations[idx])\n",
    "\n",
    "            assert len(state0) == self.window_length\n",
    "            assert len(state1) == len(state0)\n",
    "            experiences.append(Experience(state0=state0, action=action, reward=reward,\n",
    "                                          state1=state1, terminal1=terminal1))\n",
    "        assert len(experiences) == batch_size\n",
    "        return experiences\n",
    "\n",
    "    def sample_and_split(self, batch_size, batch_idxs=None):\n",
    "        experiences = self.sample(batch_size, batch_idxs)\n",
    "\n",
    "        state0_batch = []\n",
    "        reward_batch = []\n",
    "        action_batch = []\n",
    "        terminal1_batch = []\n",
    "        state1_batch = []\n",
    "        for e in experiences:\n",
    "            state0_batch.append(e.state0)\n",
    "            state1_batch.append(e.state1)\n",
    "            reward_batch.append(e.reward)\n",
    "            action_batch.append(e.action)\n",
    "            terminal1_batch.append(0. if e.terminal1 else 1.)\n",
    "\n",
    "        \n",
    "        state0_batch = np.array(state0_batch).reshape(batch_size,-1)\n",
    "        state1_batch = np.array(state1_batch).reshape(batch_size,-1)\n",
    "        terminal1_batch = np.array(terminal1_batch).reshape(batch_size,-1)\n",
    "        reward_batch = np.array(reward_batch).reshape(batch_size,-1)\n",
    "        action_batch = np.array(action_batch).reshape(batch_size,-1)\n",
    "\n",
    "        return state0_batch, action_batch, reward_batch, state1_batch, terminal1_batch\n",
    "\n",
    "\n",
    "    def append(self, observation, action, reward, terminal, training=True):\n",
    "        super(SequentialMemory, self).append(observation, action, reward, terminal, training=training)\n",
    "\n",
    "        \n",
    "        \n",
    "        if training:\n",
    "            self.observations.append(observation)\n",
    "            self.actions.append(action)\n",
    "            self.rewards.append(reward)\n",
    "            self.terminals.append(terminal)\n",
    "\n",
    "    @property\n",
    "    def nb_entries(self):\n",
    "        return len(self.observations)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super(SequentialMemory, self).get_config()\n",
    "        config['limit'] = self.limit\n",
    "        return config\n",
    "\n",
    "\n",
    "class EpisodeParameterMemory(Memory):\n",
    "    def __init__(self, limit, **kwargs):\n",
    "        super(EpisodeParameterMemory, self).__init__(**kwargs)\n",
    "        self.limit = limit\n",
    "\n",
    "        self.params = RingBuffer(limit)\n",
    "        self.intermediate_rewards = []\n",
    "        self.total_rewards = RingBuffer(limit)\n",
    "\n",
    "    def sample(self, batch_size, batch_idxs=None):\n",
    "        if batch_idxs is None:\n",
    "            batch_idxs = sample_batch_indexes(0, self.nb_entries, size=batch_size)\n",
    "        assert len(batch_idxs) == batch_size\n",
    "\n",
    "        batch_params = []\n",
    "        batch_total_rewards = []\n",
    "        for idx in batch_idxs:\n",
    "            batch_params.append(self.params[idx])\n",
    "            batch_total_rewards.append(self.total_rewards[idx])\n",
    "        return batch_params, batch_total_rewards\n",
    "\n",
    "    def append(self, observation, action, reward, terminal, training=True):\n",
    "        super(EpisodeParameterMemory, self).append(observation, action, reward, terminal, training=training)\n",
    "        if training:\n",
    "            self.intermediate_rewards.append(reward)\n",
    "\n",
    "    def finalize_episode(self, params):\n",
    "        total_reward = sum(self.intermediate_rewards)\n",
    "        self.total_rewards.append(total_reward)\n",
    "        self.params.append(params)\n",
    "        self.intermediate_rewards = []\n",
    "\n",
    "    @property\n",
    "    def nb_entries(self):\n",
    "        return len(self.total_rewards)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super(SequentialMemory, self).get_config()\n",
    "        config['limit'] = self.limit\n",
    "        return config\n",
    "import numpy as np \n",
    "\n",
    "\n",
    "\n",
    "class RandomProcess(object):\n",
    "    def reset_states(self):\n",
    "        pass\n",
    "\n",
    "class AnnealedGaussianProcess(RandomProcess):\n",
    "    def __init__(self, mu, sigma, sigma_min, n_steps_annealing):\n",
    "        self.mu = mu\n",
    "        self.sigma = sigma\n",
    "        self.n_steps = 0\n",
    "\n",
    "        if sigma_min is not None:\n",
    "            self.m = -float(sigma - sigma_min) / float(n_steps_annealing)\n",
    "            self.c = sigma\n",
    "            self.sigma_min = sigma_min\n",
    "        else:\n",
    "            self.m = 0.\n",
    "            self.c = sigma\n",
    "            self.sigma_min = sigma\n",
    "\n",
    "    @property\n",
    "    def current_sigma(self):\n",
    "        sigma = max(self.sigma_min, self.m * float(self.n_steps) + self.c)\n",
    "        return sigma\n",
    "\n",
    "\n",
    "\n",
    "class OrnsteinUhlenbeckProcess(AnnealedGaussianProcess):\n",
    "    def __init__(self, theta, mu=0., sigma=1., dt=1e-2, x0=None, size=1, sigma_min=None, n_steps_annealing=1000):\n",
    "        super(OrnsteinUhlenbeckProcess, self).__init__(mu=mu, sigma=sigma, sigma_min=sigma_min, n_steps_annealing=n_steps_annealing)\n",
    "        self.theta = theta\n",
    "        self.mu = mu\n",
    "        self.dt = dt\n",
    "        self.x0 = x0\n",
    "        self.size = size\n",
    "        self.reset_states()\n",
    "\n",
    "    def sample(self):\n",
    "        x = self.x_prev + self.theta * (self.mu - self.x_prev) * self.dt + self.current_sigma * np.sqrt(self.dt) * np.random.normal(size=self.size)\n",
    "        self.x_prev = x\n",
    "        self.n_steps += 1\n",
    "        return x\n",
    "\n",
    "    def reset_states(self):\n",
    "        self.x_prev = self.x0 if self.x0 is not None else np.zeros(self.size)\n",
    "\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "class DDPG(object):\n",
    "    def __init__(self, nb_states, nb_actions, args2):\n",
    "\n",
    "        if args2.seed > 0:\n",
    "            self.seed(args2.seed)\n",
    "\n",
    "        self.nb_states = nb_states\n",
    "        self.nb_actions= nb_actions\n",
    "\n",
    "        \n",
    "        net_cfg = {\n",
    "            'hidden1':args2.hidden1, \n",
    "            'hidden2':args2.hidden2, \n",
    "            'init_w':args2.init_w\n",
    "        }\n",
    "        self.actor = Actor(1,args3)\n",
    "        self.actor_target = Actor(1,args3)\n",
    "        self.actor_optim  = Adam(self.actor.parameters(), lr=args2.prate)\n",
    "\n",
    "        self.critic = Critic(self.nb_states, self.nb_actions, **net_cfg)\n",
    "        self.critic_target = Critic(self.nb_states, self.nb_actions, **net_cfg)\n",
    "        self.critic_optim  = Adam(self.critic.parameters(), lr=args2.rate)\n",
    "\n",
    "        hard_update(self.actor_target, self.actor) \n",
    "        hard_update(self.critic_target, self.critic)\n",
    "\n",
    "        \n",
    "        self.memory = SequentialMemory(limit=args2.rmsize, window_length=args2.window_length)\n",
    "        self.random_process = OrnsteinUhlenbeckProcess(size=nb_actions, theta=args2.ou_theta, mu=args2.ou_mu, sigma=args2.ou_sigma)\n",
    "\n",
    "        \n",
    "        self.batch_size = args2.bsize\n",
    "        self.tau = args2.tau\n",
    "        self.discount = args2.discount\n",
    "        self.depsilon = 1.0 / args2.epsilon\n",
    "\n",
    "        \n",
    "        self.epsilon = 1.0\n",
    "        self.s_t = None \n",
    "        self.a_t = None \n",
    "        self.is_training = True\n",
    "\n",
    "        \n",
    "        if USE_CUDA: self.cuda()\n",
    "\n",
    "    def update_policy(self):\n",
    "        \n",
    "        state_batch, action_batch, reward_batch, \\\n",
    "        next_state_batch, terminal_batch = self.memory.sample_and_split(self.batch_size)\n",
    "\n",
    "        \n",
    "        next_q_values = self.critic_target([\n",
    "            to_tensor(next_state_batch, volatile=True),\n",
    "            torch.cat([self.actor_target(to_tensor(next_state_batch[i], volatile=True)) for i in range(len(next_state_batch))],0).view([len(next_state_batch),-1]),\n",
    "        ])\n",
    "        next_q_values.volatile=False\n",
    "\n",
    "        target_q_batch = to_tensor(reward_batch) + \\\n",
    "            self.discount*to_tensor(terminal_batch.astype(np.float))*next_q_values\n",
    "\n",
    "        \n",
    "        self.critic.zero_grad()\n",
    "\n",
    "        q_batch = self.critic([ to_tensor(state_batch), to_tensor(action_batch) ])\n",
    "\n",
    "        value_loss = criterion(q_batch, target_q_batch)\n",
    "        value_loss.backward()\n",
    "        self.critic_optim.step()\n",
    "\n",
    "        \n",
    "        self.actor.zero_grad()\n",
    "\n",
    "        policy_loss = -self.critic([\n",
    "            to_tensor(state_batch),\n",
    "            torch.cat([self.actor(to_tensor(next_state_batch[i], volatile=True)) for i in range(len(next_state_batch))],0).view([len(state_batch),-1]), ])\n",
    "        \n",
    "\n",
    "        policy_loss = policy_loss.mean()\n",
    "        policy_loss.backward()\n",
    "        self.actor_optim.step()\n",
    "\n",
    "        \n",
    "        soft_update(self.actor_target, self.actor, self.tau)\n",
    "        soft_update(self.critic_target, self.critic, self.tau)\n",
    "\n",
    "    def eval(self):\n",
    "        self.actor.eval()\n",
    "        self.actor_target.eval()\n",
    "        self.critic.eval()\n",
    "        self.critic_target.eval()\n",
    "\n",
    "    def cuda(self):\n",
    "        self.actor.cuda()\n",
    "        self.actor_target.cuda()\n",
    "        self.critic.cuda()\n",
    "        self.critic_target.cuda()\n",
    "\n",
    "    def observe(self, r_t, s_t1, done):\n",
    "        if self.is_training:\n",
    "            self.memory.append(self.s_t, self.a_t, r_t, done)\n",
    "            self.s_t = s_t1\n",
    "\n",
    "    def random_action(self):\n",
    "        action = np.random.uniform(0.,1.,self.nb_actions)\n",
    "        self.a_t = action\n",
    "        return action\n",
    "\n",
    "    def select_action(self, s_t, decay_epsilon=True):\n",
    "        action = to_numpy(\n",
    "            self.actor(to_tensor(np.array([s_t])))\n",
    "        )\n",
    "        \n",
    "        action = np.clip(action, 0., 1.)\n",
    "\n",
    "        if decay_epsilon:\n",
    "            self.epsilon -= self.depsilon\n",
    "\n",
    "        self.a_t = action\n",
    "\n",
    "        return action\n",
    "\n",
    "    def reset(self, obs):\n",
    "        self.s_t = obs\n",
    "        self.random_process.reset_states()\n",
    "\n",
    "    def load_weights(self, output):\n",
    "        if output is None: return\n",
    "\n",
    "        self.actor.load_state_dict(\n",
    "            torch.load('{}/actor.pkl'.format(output))\n",
    "        )\n",
    "\n",
    "        self.critic.load_state_dict(\n",
    "            torch.load('{}/critic.pkl'.format(output))\n",
    "        )\n",
    "\n",
    "\n",
    "    def save_model(self,output):\n",
    "        torch.save(\n",
    "            self.actor.state_dict(),\n",
    "            '{}/actor.pkl'.format(output)\n",
    "        )\n",
    "        torch.save(\n",
    "            self.critic.state_dict(),\n",
    "            '{}/critic.pkl'.format(output)\n",
    "        )\n",
    "\n",
    "    def seed(self,s):\n",
    "        torch.manual_seed(s)\n",
    "        if USE_CUDA:\n",
    "            torch.cuda.manual_seed(s)\n",
    "import pyflann\n",
    "from gym.spaces import Box\n",
    "import numpy as np\n",
    "import itertools\n",
    "\n",
    "class Space:\n",
    "\n",
    "    def __init__(self, low, high, points):\n",
    "\n",
    "        self._low = np.array(low)\n",
    "        self._high = np.array(high)\n",
    "        self._range = self._high - self._low\n",
    "        self._dimensions = len(low)\n",
    "        self.__space = init_uniform_space([0] * self._dimensions,\n",
    "                                          [1] * self._dimensions,\n",
    "                                          points)\n",
    "        self._flann = pyflann.FLANN()\n",
    "        self.rebuild_flann()\n",
    "\n",
    "    def rebuild_flann(self):\n",
    "        self._index = self._flann.build_index(self.__space, algorithm='kdtree')\n",
    "        \n",
    "\n",
    "    def search_point(self, point, k):\n",
    "        p_in = self.import_point(point).reshape(1, -1).astype('float64')\n",
    "        \n",
    "        search_res, _ = self._flann.nn_index(p_in, k)\n",
    "        knns = self.__space[search_res]\n",
    "        p_out = []\n",
    "        for p in knns:\n",
    "            p_out.append(self.export_point(p))\n",
    "\n",
    "        if k == 1:\n",
    "            p_out = [p_out]\n",
    "        \n",
    "        return np.array(p_out)\n",
    "\n",
    "    def import_point(self, point):\n",
    "        return (point - self._low) / self._range\n",
    "\n",
    "    def export_point(self, point):\n",
    "        return self._low + point * self._range\n",
    "\n",
    "    def get_space(self):\n",
    "        return self.__space\n",
    "\n",
    "    def shape(self):\n",
    "        return self.__space.shape\n",
    "\n",
    "    def get_number_of_actions(self):\n",
    "        return self.shape()[0]\n",
    "\n",
    "    def plot_space(self, additional_points=None):\n",
    "\n",
    "        dims = self._dimensions\n",
    "\n",
    "        if dims > 3:\n",
    "            print(\n",
    "                'Cannot plot a {}-dimensional space. Max 3 dimensions'.format(dims))\n",
    "            return\n",
    "\n",
    "        space = self.get_space()\n",
    "        if additional_points is not None:\n",
    "            for i in additional_points:\n",
    "                space = np.append(space, additional_points, axis=0)\n",
    "\n",
    "        if dims == 1:\n",
    "            for x in space:\n",
    "                plt.plot([x], [0], 'o')\n",
    "\n",
    "            plt.show()\n",
    "        elif dims == 2:\n",
    "            for x, y in space:\n",
    "                plt.plot([x], [y], 'o')\n",
    "\n",
    "            plt.show()\n",
    "        else:\n",
    "            plot_3d_points(space)\n",
    "\n",
    "\n",
    "class Discrete_space(Space):\n",
    "    \"\"\"\n",
    "        Discrete action space with n actions (the integers in the range [0, n))\n",
    "        0, 1, 2, ..., n-2, n-1\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n):  \n",
    "        super().__init__([0], [n - 1], n)\n",
    "\n",
    "    def export_point(self, point):\n",
    "        return super().export_point(point).astype(int)\n",
    "\n",
    "\n",
    "def init_uniform_space(low, high, points):\n",
    "    dims = len(low)\n",
    "    points_in_each_axis = round(points**(1 / dims))\n",
    "\n",
    "    axis = []\n",
    "    for i in range(dims):\n",
    "        axis.append(list(np.linspace(low[i], high[i], points_in_each_axis)))\n",
    "    print('points_in_each_axis')\n",
    "    space = []\n",
    "    for _ in itertools.product(*axis):\n",
    "        print('points_in_each_axis')\n",
    "        space.append(list(_))\n",
    "    print('points_in_each_axis')\n",
    "    return np.array(space)\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "class WOLPAgent(object):\n",
    "    def __init__(self, nb_states, nb_actions, args2):\n",
    "\n",
    "        if args2.seed > 0:\n",
    "            self.seed(args2.seed)\n",
    "\n",
    "        self.nb_states = nb_states\n",
    "        self.nb_actions= nb_actions\n",
    "\n",
    "        \n",
    "        net_cfg = {\n",
    "            'hidden1':args2.hidden1, \n",
    "            'hidden2':args2.hidden2, \n",
    "            'init_w':args2.init_w\n",
    "        }\n",
    "\n",
    "        \n",
    "        self.low = args2.low\n",
    "        self.high = args2.high\n",
    "        self.action_space = spaces.Box(\n",
    "            low=0,\n",
    "            high=1,\n",
    "            shape=(args.solDim,)\n",
    "        )\n",
    "        self.k_nearest_neighbors = max(1, int(args2.max_actions * args2.k_ratio))\n",
    "        \n",
    "\n",
    "        self.actor =Actor(1,args3)\n",
    "        self.actor_target = Actor(1,args3)\n",
    "        self.actor_optim  = Adam(self.actor.parameters(), lr=args2.prate)\n",
    "        self.critic = Critic(self.nb_states, self.nb_actions, **net_cfg)\n",
    "        self.critic_target = Critic(self.nb_states, self.nb_actions, **net_cfg)\n",
    "        self.critic_optim  = Adam(self.critic.parameters(), lr=args2.rate)\n",
    "\n",
    "        hard_update(self.actor_target, self.actor) \n",
    "        hard_update(self.critic_target, self.critic)\n",
    "\n",
    "        \n",
    "        self.memory = SequentialMemory(limit=args2.rmsize, window_length=args2.window_length)\n",
    "        self.random_process = OrnsteinUhlenbeckProcess(size=nb_actions, theta=args2.ou_theta, mu=args2.ou_mu, sigma=args2.ou_sigma)\n",
    "\n",
    "        \n",
    "        self.batch_size = args2.bsize\n",
    "        self.tau = args2.tau\n",
    "        self.discount = args2.discount\n",
    "        self.depsilon = 1.0 / args2.epsilon\n",
    "\n",
    "        \n",
    "        self.epsilon = 1.0\n",
    "        self.s_t = None \n",
    "        self.a_t = None \n",
    "        self.is_training = True\n",
    "\n",
    "        \n",
    "        if USE_CUDA: self.cuda()\n",
    "\n",
    "    def get_action_space(self):\n",
    "        return self.action_space    \n",
    "\n",
    "    def update_policy(self):\n",
    "        \n",
    "        state_batch, action_batch, reward_batch, \\\n",
    "        next_state_batch, terminal_batch = self.memory.sample_and_split(self.batch_size)\n",
    "\n",
    "        \n",
    "        next_q_values = self.critic_target([\n",
    "            to_tensor(next_state_batch, volatile=True),\n",
    "            torch.cat([self.actor_target(to_tensor(next_state_batch[i], volatile=True)) for i in range(len(next_state_batch))],0).view([len(next_state_batch),-1]),\n",
    "        ])\n",
    "        next_q_values.volatile=False\n",
    "\n",
    "        target_q_batch = to_tensor(reward_batch) + \\\n",
    "            self.discount*to_tensor(terminal_batch.astype(np.float))*next_q_values\n",
    "\n",
    "        \n",
    "        self.critic.zero_grad()\n",
    "        q_batch = self.critic([ to_tensor(state_batch), to_tensor(action_batch) ])\n",
    "\n",
    "        value_loss = criterion(q_batch, target_q_batch)\n",
    "        value_loss.backward()\n",
    "        self.critic_optim.step()\n",
    "\n",
    "        \n",
    "        self.actor.zero_grad()\n",
    "\n",
    "        policy_loss = -self.critic([\n",
    "            to_tensor(state_batch),\n",
    "            torch.cat([self.actor(to_tensor(next_state_batch[i], volatile=True)) for i in range(len(next_state_batch))],0).view([len(state_batch),-1]), ])\n",
    "\n",
    "        policy_loss = policy_loss.mean()\n",
    "        policy_loss.backward()\n",
    "        self.actor_optim.step()\n",
    "\n",
    "        \n",
    "        soft_update(self.actor_target, self.actor, self.tau)\n",
    "        soft_update(self.critic_target, self.critic, self.tau)\n",
    "\n",
    "    def eval(self):\n",
    "        self.actor.eval()\n",
    "        self.actor_target.eval()\n",
    "        self.critic.eval()\n",
    "        self.critic_target.eval()\n",
    "\n",
    "    def cuda(self):\n",
    "        self.actor.cuda()\n",
    "        self.actor_target.cuda()\n",
    "        self.critic.cuda()\n",
    "        self.critic_target.cuda()\n",
    "\n",
    "    def observe(self, r_t, s_t1, done):\n",
    "        if self.is_training:\n",
    "            self.memory.append(self.s_t, self.a_t, r_t, done)\n",
    "            self.s_t = s_t1\n",
    "\n",
    "    def random_action(self):\n",
    "        action = np.random.uniform(0.,1.,self.nb_actions)\n",
    "        self.a_t = action\n",
    "        return action\n",
    "\n",
    "    def select_action(self, s_t, decay_epsilon=True):\n",
    "\n",
    "        return self.ddpg_select_action(s_t, decay_epsilon=decay_epsilon)\n",
    "\n",
    "    def ddpg_select_action(self, s_t, decay_epsilon=True):\n",
    "        action = to_numpy(\n",
    "            self.actor(to_tensor(np.array([s_t])))\n",
    "        )\n",
    "        \n",
    "        action = np.clip(action, 0., 1.)\n",
    "        if decay_epsilon:\n",
    "            self.epsilon -= self.depsilon\n",
    "\n",
    "        return action\n",
    "\n",
    "    def reset(self, obs):\n",
    "        self.s_t = obs\n",
    "        self.random_process.reset_states()\n",
    "\n",
    "    def load_weights(self, output):\n",
    "        if output is None: return\n",
    "\n",
    "        self.actor.load_state_dict(\n",
    "            torch.load('{}/actor.pkl'.format(output))\n",
    "        )\n",
    "\n",
    "        self.critic.load_state_dict(\n",
    "            torch.load('{}/critic.pkl'.format(output))\n",
    "        )\n",
    "\n",
    "\n",
    "    def save_model(self,output):\n",
    "        torch.save(\n",
    "            self.actor.state_dict(),\n",
    "            '{}/actor.pkl'.format(output)\n",
    "        )\n",
    "        torch.save(\n",
    "            self.critic.state_dict(),\n",
    "            '{}/critic.pkl'.format(output)\n",
    "        )\n",
    "\n",
    "    def seed(self,s):\n",
    "        torch.manual_seed(s)\n",
    "        if USE_CUDA:\n",
    "            torch.cuda.manual_seed(s)\n",
    "import math\n",
    "import gym\n",
    "from gym import spaces, logger\n",
    "from gym.utils import seeding\n",
    "import numpy as np\n",
    "\n",
    "def test(num_episodes, agent, env2, evaluate, model_path, visualize=True, debug=False):\n",
    "\n",
    "    agent.load_weights(model_path)\n",
    "    agent.is_training = False\n",
    "    agent.eval()\n",
    "    policy = lambda x: agent.select_action(x, decay_epsilon=False)\n",
    "\n",
    "    for i in range(num_episodes):\n",
    "        validate_reward = evaluate(env2, policy, debug=debug, visualize=visualize, save=False)\n",
    "class Arguments(object):\n",
    "    def __init__(self):\n",
    "        self.mode = 'train'\n",
    "        self.env2 = \"InvertedPendulum-v2\"\n",
    "        self.hidden1 = 400\n",
    "        self.hidden2 = 300\n",
    "        self.rate = 0.001\n",
    "        self.prate = 0.0001\n",
    "        self.warmup = 100\n",
    "        self.discount = 0.99\n",
    "        self.bsize = 64\n",
    "        self.rmsize = 6000000\n",
    "        self.window_length = 1\n",
    "        self.tau = 0.001\n",
    "        self.ou_theta = 0.15\n",
    "        self.ou_sigma = 0.2\n",
    "        self.ou_mu = 0.0\n",
    "        self.validate_episodes = 20\n",
    "        self.max_episode_length = 500\n",
    "        self.validate_steps = 50000\n",
    "        self.output = 'output'\n",
    "        self.debug='debug'\n",
    "        self.init_w = 0.03\n",
    "        self.train_iter=5000\n",
    "        self.epsilon=50000\n",
    "        self.seed=-1\n",
    "        self.max_actions=1e6\n",
    "        self.resume='default'\n",
    "        self.k_ratio = 1e-6\n",
    "\n",
    "args2 = Arguments()\n",
    "args2.output = get_output_folder(args2.output, args2.env2)\n",
    "if args2.resume == 'default':\n",
    "    args2.resume = 'output/{}-run0'.format(args2.env2)\n",
    "class myenv2(gym.Env):\n",
    "    metadata = {\n",
    "        'render.modes': ['human', 'rgb_array'],\n",
    "        'video.frames_per_second': 50\n",
    "    }\n",
    "\n",
    "    def __init__(self):\n",
    "\n",
    "        \n",
    "        \n",
    "\n",
    "        self.action_space =  spaces.Box(\n",
    "            low=0,\n",
    "            high=1,\n",
    "            shape=(args.solDim,)\n",
    "        )\n",
    "        self.observation_space =  spaces.Box(\n",
    "            low=0,\n",
    "            high=1,\n",
    "            shape=(args.solDim,)\n",
    "        )\n",
    "\n",
    "        self.seed()\n",
    "        self.viewer = None\n",
    "        self.state = None\n",
    "\n",
    "        self.steps_beyond_done = None\n",
    "\n",
    "        self.steps = torch.tensor(np.array([0.1]*args.solDim))\n",
    "        self._max_episode_steps = 500\n",
    "        self.iteration=1\n",
    "        self.constraints=[]\n",
    "        self.constraint=0\n",
    "        self.history=np.ones(args.solDim)\n",
    "    def seed(self, seed=None):\n",
    "        self.np_random, seed = seeding.np_random(seed)\n",
    "        return [seed]\n",
    "\n",
    "    def step(self, action):\n",
    "        self.iteration+=1 \n",
    "        if self.iteration==60:\n",
    "            self.constraint=10\n",
    "            self.std=np.array(rewards).std()\n",
    "        if self.iteration>1 and self.iteration%10==0:\n",
    "            self.constraint=min(100*sum(rewards)/len(rewards),max(0,self.constraint + 0.005*(-0.02+sum(self.constraints)/len(self.constraints))))\n",
    "        done=1\n",
    "        action=[action[i]+0.01*np.sqrt(np.log(self.iteration)/self.history[i]) for i in range(args.solDim)]\n",
    "        action=np.array([1 if i in np.argsort(action)[-args.card:] else 0  for i in range(args.solDim)])\n",
    "\n",
    "        self.constraints.append(checkFea(action))\n",
    "        \n",
    "  \n",
    "   \n",
    "    \n",
    "        \n",
    "        \n",
    "        reward=hwithoutconstraints(action)-self.constraint*checkFea(action)\n",
    "        rewards.append(hwithoutconstraints(action))\n",
    "        constraints.append(checkFea(action))\n",
    "        self.history+=np.array(action)\n",
    "        print(self.iteration,1*hwithoutconstraints(action),checkFea(action))\n",
    "        \n",
    "        return action, reward, done, {}\n",
    "\n",
    "    def reset(self):\n",
    "        return  np.array(np.array([0.1]*args.solDim)),\n",
    "\n",
    "    def render(self, mode='human'):\n",
    "        pass\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "env2 = myenv2()\n",
    "\n",
    "args2.low = env2.action_space.low\n",
    "args2.high = env2.action_space.high\n",
    "\n",
    "\n",
    "if args2.seed > 0:\n",
    "    np.random.seed(args2.seed)\n",
    "    env2.seed(args2.seed)\n",
    "\n",
    "nb_states =args.solDim\n",
    "nb_actions =args.solDim\n",
    "agent = WOLPAgent(nb_states, nb_actions, args2)\n",
    "\n",
    "evaluate = Evaluator(args2.validate_episodes, \n",
    "    args2.validate_steps, args2.output, max_episode_length=args2.max_episode_length)\n",
    "num_iterations=args2.train_iter\n",
    "validate_steps=args2.validate_steps\n",
    "output= args2.output\n",
    "max_episode_length=args2.max_episode_length\n",
    "debug=args2.debug\n",
    "agent.is_training = True\n",
    "step = episode = episode_steps = 0\n",
    "episode_reward = 0.\n",
    "observation = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actor(nn.Module):\n",
    "    def __init__(self, input_shape, args):\n",
    "        super(Actor, self).__init__()\n",
    "\n",
    "        \n",
    "        self.encoding = nn.Linear(input_shape, args.rnn_hidden_dim)  \n",
    "        self.h = nn.GRUCell(args.rnn_hidden_dim, args.rnn_hidden_dim)  \n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        self.hard_bi_GRU = nn.GRU(args.rnn_hidden_dim * 2, args.rnn_hidden_dim, bidirectional=True)\n",
    "        \n",
    "        self.hard_encoding = nn.Linear(args.rnn_hidden_dim * 2, 2)  \n",
    "\n",
    "        \n",
    "        self.q = nn.Linear(args.rnn_hidden_dim, args.attention_dim, bias=False)\n",
    "        self.k = nn.Linear(args.rnn_hidden_dim, args.attention_dim, bias=False)\n",
    "        self.v = nn.Linear(args.rnn_hidden_dim, args.attention_dim)\n",
    "\n",
    "        \n",
    "        self.decoding = nn.Linear(args.rnn_hidden_dim + args.attention_dim, args.n_actions)\n",
    "        self.args = args\n",
    "        self.input_shape = input_shape\n",
    "        self.softmax=F.softmax\n",
    "\n",
    "    def forward(self,state, obs=torch.diag(torch.ones(args.solDim)), hidden_state=hids):\n",
    "        size = obs.shape[0]  \n",
    "        \n",
    "        obs_encoding = F.relu(self.encoding(obs))\n",
    "        h_in = hidden_state.reshape(-1, self.args.rnn_hidden_dim)\n",
    "\n",
    "        \n",
    "        h_out = self.h(obs_encoding, h_in)  \n",
    "\n",
    "        \n",
    "        if self.args.hard:\n",
    "            \n",
    "            h = h_out.reshape(-1, self.args.n_agents, self.args.rnn_hidden_dim)  \n",
    "            input_hard = []\n",
    "            for i in range(self.args.n_agents):\n",
    "                h_i = h[:, i]  \n",
    "                h_hard_i = []\n",
    "                for j in range(self.args.n_agents):  \n",
    "                    if j != i:\n",
    "                        h_hard_i.append(torch.cat([h_i, h[:, j]], dim=-1))\n",
    "                \n",
    "                h_hard_i = torch.stack(h_hard_i, dim=0)\n",
    "                input_hard.append(h_hard_i)\n",
    "            \n",
    "            input_hard = torch.stack(input_hard, dim=-2)\n",
    "            \n",
    "            input_hard = input_hard.view(self.args.n_agents - 1, -1, self.args.rnn_hidden_dim * 2)\n",
    "\n",
    "            h_hard = torch.zeros((2 * 1, size, self.args.rnn_hidden_dim))  \n",
    "            if self.args.cuda:\n",
    "                h_hard = h_hard.cuda()\n",
    "            h_hard, _ = self.hard_bi_GRU(input_hard, h_hard)  \n",
    "            h_hard = h_hard.permute(1, 0, 2)  \n",
    "            h_hard = h_hard.reshape(-1, self.args.rnn_hidden_dim * 2)  \n",
    "\n",
    "            \n",
    "            hard_weights = self.hard_encoding(h_hard)\n",
    "            hard_weights = F.gumbel_softmax(hard_weights, tau=0.01)\n",
    "            \n",
    "            hard_weights = hard_weights[:, 1].view(-1, self.args.n_agents, 1, self.args.n_agents - 1)\n",
    "            hard_weights = hard_weights.permute(1, 0, 2, 3)\n",
    "\n",
    "        else:\n",
    "            hard_weights = torch.ones((self.args.n_agents, size // self.args.n_agents, 1, self.args.n_agents - 1))\n",
    "            if self.args.cuda:\n",
    "                hard_weights = hard_weights.cuda()\n",
    "\n",
    "        \n",
    "        q = self.q(h_out).reshape(-1, self.args.n_agents, self.args.attention_dim)  \n",
    "        k = self.k(h_out).reshape(-1, self.args.n_agents, self.args.attention_dim)  \n",
    "        v = F.relu(self.v(h_out)).reshape(-1, self.args.n_agents, self.args.attention_dim)  \n",
    "        x = []\n",
    "        for i in range(self.args.n_agents):\n",
    "            q_i = q[:, i].view(-1, 1, self.args.attention_dim)  \n",
    "            k_i = [k[:, j] for j in range(self.args.n_agents) if j != i]  \n",
    "            v_i = [v[:, j] for j in range(self.args.n_agents) if j != i]  \n",
    "\n",
    "            k_i = torch.stack(k_i, dim=0)  \n",
    "            k_i = k_i.permute(1, 2, 0)  \n",
    "            v_i = torch.stack(v_i, dim=0)\n",
    "            v_i = v_i.permute(1, 2, 0)\n",
    "\n",
    "            \n",
    "            score = torch.matmul(q_i, k_i)\n",
    "\n",
    "            \n",
    "            scaled_score = score / np.sqrt(self.args.attention_dim)\n",
    "\n",
    "            \n",
    "            soft_weight = F.softmax(scaled_score, dim=-1)  \n",
    "\n",
    "            \n",
    "            x_i = (v_i * soft_weight * hard_weights[i]).sum(dim=-1)\n",
    "            x.append(x_i)\n",
    "\n",
    "        \n",
    "        x = torch.stack(x, dim=1).reshape(-1, self.args.attention_dim)  \n",
    "        final_input = torch.cat([h_out, x], dim=-1)\n",
    "        output = self.softmax(1*self.decoding(final_input))\n",
    "        return torch.cat(([i[0].view(1) for i in output]),0)\n",
    "\n",
    "class Critic(nn.Module):\n",
    "    def __init__(self, nb_actions, hidden1=400, hidden2=300, init_w=3e-3):\n",
    "        super(Critic, self).__init__()\n",
    "        self.fc1 = nn.Linear(nb_actions, hidden1)\n",
    "        self.fc2 = nn.Linear(hidden1, hidden2)\n",
    "        self.fc3 = nn.Linear(hidden2, 1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.init_weights(init_w)\n",
    "\n",
    "    def init_weights(self, init_w):\n",
    "        self.fc1.weight.data = fanin_init(self.fc1.weight.data.size())\n",
    "        self.fc2.weight.data = fanin_init(self.fc2.weight.data.size())\n",
    "        self.fc3.weight.data.uniform_(-init_w, init_w)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc3(out)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "actor=Actor(args.solDim,args3)\n",
    "critic=Critic(args.solDim)\n",
    "criterion = nn.MSELoss()\n",
    "opt1 =torch.optim.Adam(actor.parameters(), lr=1e-3)\n",
    "opt2 =torch.optim.Adam(critic.parameters(), lr=1e-3)\n",
    "iteration=0\n",
    "history=np.ones(args.solDim)\n",
    "constraints=[]\n",
    "constraint=0\n",
    "xs=[]\n",
    "ys=[]\n",
    "for iteration in range(5001):\n",
    "    if iteration<100:\n",
    "        action=[1.]*args.card+[0]*(args.solDim-args.card)\n",
    "        np.random.shuffle(action)\n",
    "        xs=[action]\n",
    "        action=np.array(action)\n",
    "        ys=[hwithoutconstraints(action)-constraint*checkFea(action)]\n",
    "        rewards.append(hwithoutconstraints(action))\n",
    "        constraints.append(checkFea(action))\n",
    "    else:\n",
    "        action=actor(1)\n",
    "\n",
    "        if iteration==60:\n",
    "            constraint=10#100*sum(rewards)/len(rewards)\n",
    "            std=np.array(rewards).std()\n",
    "        if iteration>1 and iteration%60==0:\n",
    "            constraint=min(100*sum(rewards)/len(rewards),max(0,constraint + 0.005*(-0.0+sum(constraints)/len(constraints))))\n",
    "        done=1\n",
    "        action2=[action[i]+0.0002*np.sqrt(np.log(iteration)/history[i]) for i in range(args.solDim)]\n",
    "        action2=np.array([1 if i in np.argsort(action2)[-args.card:] else 0  for i in range(args.solDim)])\n",
    "        print(iteration,hwithoutconstraints(action2)-10*checkFea(action2))\n",
    "        rewards.append(hwithoutconstraints(action2))\n",
    "        constraints.append(checkFea(action2))\n",
    "        history+=np.array(action2)\n",
    "        loss=-critic(action)\n",
    "        opt1.zero_grad()\n",
    "        loss.backward()\n",
    "        opt1.step()\n",
    "        xs=[action]\n",
    "        ys=[hwithoutconstraints(action2)-constraint*checkFea(action2)]\n",
    "    if iteration>0:\n",
    "        loss2=criterion(critic(torch.tensor(xs[-1])),torch.tensor(ys[-1]).float())\n",
    "        opt2.zero_grad()\n",
    "        loss2.backward()\n",
    "        opt2.step()\n",
    "    if iteration%1000==0:\n",
    "        np.save('reward_g2a_realorderfm.npy',rewards)\n",
    "        np.save('constraint_g2a_realorderfm.npy',constraints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('reward_g2a_realorderfm.npy',rewards+[rewards[-1]]*(5000-len(rewards)))\n",
    "\n",
    "np.save('constraint_g2a_realorderfm.npy',constraints+[constraints[-1]]*(5000-len(constraints)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hwithoutconstraints = lambda x:10*((x.dot(a))**2)  #100*(x.dot(a))**1+100*x.dot(Q).dot(x) # 、100*x.dot(Q).dot(x)\n",
    "\n",
    "actor=Actor(args.solDim,args3)\n",
    "critic=Critic(args.solDim)\n",
    "criterion = nn.MSELoss()\n",
    "opt1 =torch.optim.Adam(actor.parameters(), lr=1e-3)\n",
    "opt2 =torch.optim.Adam(critic.parameters(), lr=1e-3)\n",
    "iteration=0\n",
    "history=np.ones(args.solDim)\n",
    "constraints=[]\n",
    "constraint=0\n",
    "xs=[]\n",
    "ys=[]\n",
    "for iteration in range(5001):\n",
    "    if iteration<100:\n",
    "        action=[1.]*args.card+[0]*(args.solDim-args.card)\n",
    "        np.random.shuffle(action)\n",
    "        xs=[action]\n",
    "        action=np.array(action)\n",
    "        ys=[hwithoutconstraints(action)-constraint*checkFea(action)]\n",
    "        rewards.append(hwithoutconstraints(action))\n",
    "        constraints.append(checkFea(action))\n",
    "    else:\n",
    "        action=actor(1)\n",
    "\n",
    "        if iteration==60:\n",
    "            constraint=10#100*sum(rewards)/len(rewards)\n",
    "            std=np.array(rewards).std()\n",
    "        if iteration>1 and iteration%60==0:\n",
    "            constraint=min(100*sum(rewards)/len(rewards),max(0,constraint + 0.005*(-0.0+sum(constraints)/len(constraints))))\n",
    "        done=1\n",
    "        action2=[action[i]+0.0002*np.sqrt(np.log(iteration)/history[i]) for i in range(args.solDim)]\n",
    "        action2=np.array([1 if i in np.argsort(action2)[-args.card:] else 0  for i in range(args.solDim)])\n",
    "        print(iteration,hwithoutconstraints(action2)-10*checkFea(action2))\n",
    "        rewards.append(hwithoutconstraints(action2))\n",
    "        constraints.append(checkFea(action2))\n",
    "        history+=np.array(action2)\n",
    "        loss=-critic(action)\n",
    "        opt1.zero_grad()\n",
    "        loss.backward()\n",
    "        opt1.step()\n",
    "        xs=[action]\n",
    "        ys=[hwithoutconstraints(action2)-constraint*checkFea(action2)]\n",
    "    if iteration>0:\n",
    "        loss2=criterion(critic(torch.tensor(xs[-1])),torch.tensor(ys[-1]).float())\n",
    "        opt2.zero_grad()\n",
    "        loss2.backward()\n",
    "        opt2.step()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aa=np.random.random(10)\n",
    "aa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def top_k(x, k):\n",
    "    return np.argpartition(x, k)[..., -k:]\n",
    "\n",
    "def sample_k(logits, k):\n",
    "    u = np.random.uniform(size=np.shape(logits))\n",
    "    z = -np.log(-np.log(u))\n",
    "    return top_k(logits +z, k)\n",
    "sample_k(aa,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
