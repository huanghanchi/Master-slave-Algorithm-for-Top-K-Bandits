{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#real data with real Feedback\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "fixedItemSet=np.load(r\"hetrec2011-movielens-2k\\frequentItemVec.npy\")\n",
    "userflow=np.load(r'hetrec2011-movielens-2k\\userflow.npy')\n",
    "userclick=np.load(r'hetrec2011-movielens-2k\\userclick.npy',allow_pickle=True)\n",
    "\n",
    "def hwithoutconstraints(action):\n",
    "    if type(action)==torch.Tensor:\n",
    "        action=action.detach().numpy()\n",
    "    items=np.argwhere(np.array(action)>0.5).reshape(args.card)\n",
    "    if step<args.card:\n",
    "        itemlist=userclick[0][:args.card]\n",
    "    else:\n",
    "        itemlist=userclick[0][(step-args.card):min((step+args.card),len(userclick[0]))]\n",
    "    cnt2=0\n",
    "    for i in items:\n",
    "        cnt2+=( itemlist.count(i))\n",
    "    return cnt2/(args.card)\n",
    "\n",
    "import argparse\n",
    "from copy import deepcopy\n",
    "import torch\n",
    "import gym\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\t# 各种层类型的实现\n",
    "import torch.nn.functional as F\t# 各中层函数的实现，与层类型对应，如：卷积函数、池化函数、归一化函数等等\n",
    "import torch.optim as optim\t# 实现各种优化算法的包\n",
    "import numpy as np\n",
    "\n",
    "import argparse\n",
    "import sys\n",
    "\n",
    "import copy\n",
    "import numpy as np\n",
    "from numpy.random import permutation\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "sys.path.insert(0,'/root/gurobi903/linux64/lib/python3.6_utf32/gurobipy')\n",
    "\n",
    "class para:\n",
    "    def __init__(self):\n",
    "        self.batch_size=16\n",
    "        self.wmc=10\n",
    "        self.solDim=40\n",
    "        self.card=10\n",
    "        self.test_batch_size=1000\n",
    "        self.epochs=10\n",
    "        self.lr=0.1\n",
    "        self.momentum=0.5\n",
    "        self.no_cuda='store_true'\n",
    "        self.seed=1\n",
    "        self.log_interval=10\n",
    "        self.hidden_dim=10\n",
    "args=para()\n",
    "thre=1\n",
    "legalList={}\n",
    "for i in range(40):\n",
    "    legalList[i]=[]\n",
    "for i in range(40):\n",
    "    for j in range(40):\n",
    "        if i!=j and np.linalg.norm(fixedItemSet[i]-fixedItemSet[j])<thre:\n",
    "            legalList[i].append(j)\n",
    "def checkFea(x):\n",
    "    cnt=0\n",
    "    cntT=0\n",
    "    for i in range(len(legalList.keys())):\n",
    "        for j in legalList[i]:\n",
    "            cntT+=1\n",
    "            if x[j]+x[i]>1:\n",
    "                cnt+=1\n",
    "    return cnt/cntT/2+abs(sum(x)-args.card)/args.card/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "rewards=[]\n",
    "constraints=[]\n",
    "import gym\n",
    "\n",
    "# https://github.com/openai/gym/blob/master/gym/core.py\n",
    "class Normalizedenv2(gym.ActionWrapper):\n",
    "    \"\"\" Wrap action \"\"\"\n",
    "\n",
    "    def action(self, action):\n",
    "        act_k = (self.action_space.high - self.action_space.low)/ 2.\n",
    "        act_b = (self.action_space.high + self.action_space.low)/ 2.\n",
    "        return act_k * action + act_b\n",
    "\n",
    "    def reverse_action(self, action):\n",
    "        act_k_inv = 2./(self.action_space.high - self.action_space.low)\n",
    "        act_b = (self.action_space.high + self.action_space.low)/ 2.\n",
    "        return act_k_inv * (action - act_b)\n",
    "import os\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "\n",
    "USE_CUDA = torch.cuda.is_available()\n",
    "FLOAT = torch.cuda.FloatTensor if USE_CUDA else torch.FloatTensor\n",
    "\n",
    "\n",
    "def to_numpy(var):\n",
    "    return var.cpu().data.numpy() if USE_CUDA else var.data.numpy()\n",
    "\n",
    "def to_tensor(ndarray, volatile=False, requires_grad=False, dtype=FLOAT):\n",
    "    return Variable(\n",
    "        torch.from_numpy(ndarray), volatile=volatile, requires_grad=requires_grad\n",
    "    ).type(dtype)\n",
    "\n",
    "def soft_update(target, source, tau):\n",
    "    for target_param, param in zip(target.parameters(), source.parameters()):\n",
    "        target_param.data.copy_(\n",
    "            target_param.data * (1.0 - tau) + param.data * tau\n",
    "        )\n",
    "\n",
    "def hard_update(target, source):\n",
    "    for target_param, param in zip(target.parameters(), source.parameters()):\n",
    "            target_param.data.copy_(param.data)\n",
    "\n",
    "def get_output_folder(parent_dir, env2_name):\n",
    "    \"\"\"Return save folder.\n",
    "    Assumes folders in the parent_dir have suffix -run{run\n",
    "    number}. Finds the highest run number and sets the output folder\n",
    "    to that number + 1. This is just convenient so that if you run the\n",
    "    same script multiple times tensorboard can plot all of the results\n",
    "    on the same plots with different names.\n",
    "    Parameters\n",
    "    ----------\n",
    "    parent_dir: str\n",
    "      Path of the directory containing all experiment runs.\n",
    "    Returns\n",
    "    -------\n",
    "    parent_dir/run_dir\n",
    "      Path to this run's save directory.\n",
    "    \"\"\"\n",
    "    os.makedirs(parent_dir, exist_ok=True)\n",
    "    experiment_id = 0\n",
    "    for folder_name in os.listdir(parent_dir):\n",
    "        if not os.path.isdir(os.path.join(parent_dir, folder_name)):\n",
    "            continue\n",
    "        try:\n",
    "            folder_name = int(folder_name.split('-run')[-1])\n",
    "            if folder_name > experiment_id:\n",
    "                experiment_id = folder_name\n",
    "        except:\n",
    "            pass\n",
    "    experiment_id += 1\n",
    "\n",
    "    parent_dir = os.path.join(parent_dir, env2_name)\n",
    "    parent_dir = parent_dir + '-run{}'.format(experiment_id)\n",
    "    os.makedirs(parent_dir, exist_ok=True)\n",
    "    return parent_dir\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.io import savemat\n",
    "class Evaluator(object):\n",
    "\n",
    "    def __init__(self, num_episodes, interval, save_path='', max_episode_length=None):\n",
    "        self.num_episodes = num_episodes\n",
    "        self.max_episode_length = max_episode_length\n",
    "        self.interval = interval\n",
    "        self.save_path = save_path\n",
    "        self.results = np.array([]).reshape(num_episodes,0)\n",
    "\n",
    "    def __call__(self, env2, policy, debug=False, visualize=False, save=True):\n",
    "\n",
    "        self.is_training = False\n",
    "        observation = None\n",
    "        result = []\n",
    "\n",
    "        for episode in range(self.num_episodes):\n",
    "\n",
    "            # reset at the start of episode\n",
    "            observation = env2.reset()\n",
    "            episode_steps = 0\n",
    "            episode_reward = 0.\n",
    "\n",
    "            assert observation is not None\n",
    "\n",
    "            # start episode\n",
    "            done = False\n",
    "            while not done:\n",
    "                # basic operation, action ,reward, blablabla ...\n",
    "                action = policy(observation)\n",
    "\n",
    "                observation, reward, done, info = env2.step(action)\n",
    "                if self.max_episode_length and episode_steps >= self.max_episode_length -1:\n",
    "                    done = True\n",
    "\n",
    "                # update\n",
    "                episode_reward += reward\n",
    "                episode_steps += 1\n",
    "            result.append(episode_reward)\n",
    "\n",
    "        result = np.array(result).reshape(-1,1)\n",
    "        self.results = np.hstack([self.results, result])\n",
    "\n",
    "        if save:\n",
    "            self.save_results('{}/validate_reward'.format(self.save_path))\n",
    "        return np.mean(result)\n",
    "\n",
    "    def save_results(self, fn):\n",
    "\n",
    "        y = np.mean(self.results, axis=0)\n",
    "        error=np.std(self.results, axis=0)\n",
    "\n",
    "        x = range(0,self.results.shape[1]*self.interval,self.interval)\n",
    "        fig, ax = plt.subplots(1, 1, figsize=(6, 5))\n",
    "        plt.xlabel('Timestep')\n",
    "        plt.ylabel('Average Reward')\n",
    "        ax.errorbar(x, y, yerr=error, fmt='-o')\n",
    "        plt.savefig(fn+'.png')\n",
    "        savemat(fn+'.mat', {'reward':self.results})\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "#from ipdb import set_trace as debug\n",
    "\n",
    "def fanin_init(size, fanin=None):\n",
    "    fanin = fanin or size[0]\n",
    "    v = 1. / np.sqrt(fanin)\n",
    "    return torch.Tensor(size).uniform_(-v, v)\n",
    "\n",
    "class Actor(nn.Module):\n",
    "    def __init__(self, nb_states, nb_actions, hidden1=400, hidden2=300, init_w=3e-3):\n",
    "        super(Actor, self).__init__()\n",
    "        self.fc1 = nn.Linear(nb_states, hidden1)\n",
    "        self.fc2 = nn.Linear(hidden1, hidden2)\n",
    "        self.fc3 = nn.Linear(hidden2, nb_actions)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.init_weights(init_w)\n",
    "        self.sigmoid=nn.Sigmoid()\n",
    "    def init_weights(self, init_w):\n",
    "        self.fc1.weight.data = fanin_init(self.fc1.weight.data.size())\n",
    "        self.fc2.weight.data = fanin_init(self.fc2.weight.data.size())\n",
    "        self.fc3.weight.data.uniform_(-init_w, init_w)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc3(out)\n",
    "        out = self.tanh(out)\n",
    "        out=self.sigmoid(out)\n",
    "        return out\n",
    "\n",
    "class Critic(nn.Module):\n",
    "    def __init__(self, nb_states, nb_actions, hidden1=400, hidden2=300, init_w=3e-3):\n",
    "        super(Critic, self).__init__()\n",
    "        self.fc1 = nn.Linear(nb_states, hidden1)\n",
    "        self.fc2 = nn.Linear(hidden1 +nb_actions, hidden2)\n",
    "        self.fc3 = nn.Linear(hidden2, 1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.init_weights(init_w)\n",
    "\n",
    "    def init_weights(self, init_w):\n",
    "        self.fc1.weight.data = fanin_init(self.fc1.weight.data.size())\n",
    "        self.fc2.weight.data = fanin_init(self.fc2.weight.data.size())\n",
    "        self.fc3.weight.data.uniform_(-init_w, init_w)\n",
    "\n",
    "    def forward(self, xs):\n",
    "        x, a = xs\n",
    "        out = self.fc1(x)\n",
    "        out = self.relu(out)\n",
    "        # debug()\n",
    "        if len(a.shape)==1:\n",
    "            a=a.view([1,a.shape[0]])\n",
    "        out = self.fc2(torch.cat([out,a],1))\n",
    "        out = self.relu(out)\n",
    "        out = self.fc3(out)\n",
    "        return out\n",
    "\n",
    "from collections import deque, namedtuple\n",
    "import warnings\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# [reference] https://github.com/matthiasplappert/keras-rl/blob/master/rl/memory.py\n",
    "\n",
    "# This is to be understood as a transition: Given `state0`, performing `action`\n",
    "# yields `reward` and results in `state1`, which might be `terminal`.\n",
    "Experience = namedtuple('Experience', 'state0, action, reward, state1, terminal1')\n",
    "\n",
    "\n",
    "def sample_batch_indexes(low, high, size):\n",
    "    if high - low >= size:\n",
    "        # We have enough data. Draw without replacement, that is each index is unique in the\n",
    "        # batch. We cannot use `np.random.choice` here because it is horribly inefficient as\n",
    "        # the memory grows. See https://github.com/numpy/numpy/issues/2764 for a discussion.\n",
    "        # `random.sample` does the same thing (drawing without replacement) and is way faster.\n",
    "        try:\n",
    "            r = xrange(low, high)\n",
    "        except NameError:\n",
    "            r = range(low, high)\n",
    "        batch_idxs = random.sample(r, size)\n",
    "    else:\n",
    "        # Not enough data. Help ourselves with sampling from the range, but the same index\n",
    "        # can occur multiple times. This is not good and should be avoided by picking a\n",
    "        # large enough warm-up phase.\n",
    "        warnings.warn('Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!')\n",
    "        batch_idxs = np.random.random_integers(low, high - 1, size=size)\n",
    "    assert len(batch_idxs) == size\n",
    "    return batch_idxs\n",
    "\n",
    "\n",
    "class RingBuffer(object):\n",
    "    def __init__(self, maxlen):\n",
    "        self.maxlen = maxlen\n",
    "        self.start = 0\n",
    "        self.length = 0\n",
    "        self.data = [None for _ in range(maxlen)]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if idx < 0 or idx >= self.length:\n",
    "            raise KeyError()\n",
    "        return self.data[(self.start + idx) % self.maxlen]\n",
    "\n",
    "    def append(self, v):\n",
    "        if self.length < self.maxlen:\n",
    "            # We have space, simply increase the length.\n",
    "            self.length += 1\n",
    "        elif self.length == self.maxlen:\n",
    "            # No space, \"remove\" the first item.\n",
    "            self.start = (self.start + 1) % self.maxlen\n",
    "        else:\n",
    "            # This should never happen.\n",
    "            raise RuntimeError()\n",
    "        self.data[(self.start + self.length - 1) % self.maxlen] = v\n",
    "\n",
    "\n",
    "def zeroed_observation(observation):\n",
    "    if hasattr(observation, 'shape'):\n",
    "        return np.zeros(observation.shape)\n",
    "    elif hasattr(observation, '__iter__'):\n",
    "        out = []\n",
    "        for x in observation:\n",
    "            out.append(zeroed_observation(x))\n",
    "        return out\n",
    "    else:\n",
    "        return 0.\n",
    "\n",
    "\n",
    "class Memory(object):\n",
    "    def __init__(self, window_length, ignore_episode_boundaries=False):\n",
    "        self.window_length = window_length\n",
    "        self.ignore_episode_boundaries = ignore_episode_boundaries\n",
    "\n",
    "        self.recent_observations = deque(maxlen=window_length)\n",
    "        self.recent_terminals = deque(maxlen=window_length)\n",
    "\n",
    "    def sample(self, batch_size, batch_idxs=None):\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def append(self, observation, action, reward, terminal, training=True):\n",
    "        self.recent_observations.append(observation)\n",
    "        self.recent_terminals.append(terminal)\n",
    "\n",
    "    def get_recent_state(self, current_observation):\n",
    "        # This code is slightly complicated by the fact that subsequent observations might be\n",
    "        # from different episodes. We ensure that an experience never spans multiple episodes.\n",
    "        # This is probably not that important in practice but it seems cleaner.\n",
    "        state = [current_observation]\n",
    "        idx = len(self.recent_observations) - 1\n",
    "        for offset in range(0, self.window_length - 1):\n",
    "            current_idx = idx - offset\n",
    "            current_terminal = self.recent_terminals[current_idx - 1] if current_idx - 1 >= 0 else False\n",
    "            if current_idx < 0 or (not self.ignore_episode_boundaries and current_terminal):\n",
    "                # The previously handled observation was terminal, don't add the current one.\n",
    "                # Otherwise we would leak into a different episode.\n",
    "                break\n",
    "            state.insert(0, self.recent_observations[current_idx])\n",
    "        while len(state) < self.window_length:\n",
    "            state.insert(0, zeroed_observation(state[0]))\n",
    "        return state\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {\n",
    "            'window_length': self.window_length,\n",
    "            'ignore_episode_boundaries': self.ignore_episode_boundaries,\n",
    "        }\n",
    "        return config\n",
    "\n",
    "class SequentialMemory(Memory):\n",
    "    def __init__(self, limit, **kwargs):\n",
    "        super(SequentialMemory, self).__init__(**kwargs)\n",
    "\n",
    "        self.limit = limit\n",
    "\n",
    "        # Do not use deque to implement the memory. This data structure may seem convenient but\n",
    "        # it is way too slow on random access. Instead, we use our own ring buffer implementation.\n",
    "        self.actions = RingBuffer(limit)\n",
    "        self.rewards = RingBuffer(limit)\n",
    "        self.terminals = RingBuffer(limit)\n",
    "        self.observations = RingBuffer(limit)\n",
    "\n",
    "    def sample(self, batch_size, batch_idxs=None):\n",
    "        if batch_idxs is None:\n",
    "            # Draw random indexes such that we have at least a single entry before each\n",
    "            # index.\n",
    "            batch_idxs = sample_batch_indexes(0, self.nb_entries - 1, size=batch_size)\n",
    "        batch_idxs = np.array(batch_idxs) + 1\n",
    "        assert np.min(batch_idxs) >= 1\n",
    "        assert np.max(batch_idxs) < self.nb_entries\n",
    "        assert len(batch_idxs) == batch_size\n",
    "\n",
    "        # Create experiences\n",
    "        experiences = []\n",
    "        for idx in batch_idxs:\n",
    "            terminal0 = self.terminals[idx - 2] if idx >= 2 else False\n",
    "            while terminal0:\n",
    "                # Skip this transition because the env2ironment was reset here. Select a new, random\n",
    "                # transition and use this instead. This may cause the batch to contain the same\n",
    "                # transition twice.\n",
    "                idx = sample_batch_indexes(1, self.nb_entries, size=1)[0]\n",
    "                terminal0 = self.terminals[idx - 2] if idx >= 2 else False\n",
    "            assert 1 <= idx < self.nb_entries\n",
    "\n",
    "            # This code is slightly complicated by the fact that subsequent observations might be\n",
    "            # from different episodes. We ensure that an experience never spans multiple episodes.\n",
    "            # This is probably not that important in practice but it seems cleaner.\n",
    "            state0 = [self.observations[idx - 1]]\n",
    "            for offset in range(0, self.window_length - 1):\n",
    "                current_idx = idx - 2 - offset\n",
    "                current_terminal = self.terminals[current_idx - 1] if current_idx - 1 > 0 else False\n",
    "                if current_idx < 0 or (not self.ignore_episode_boundaries and current_terminal):\n",
    "                    # The previously handled observation was terminal, don't add the current one.\n",
    "                    # Otherwise we would leak into a different episode.\n",
    "                    break\n",
    "                state0.insert(0, self.observations[current_idx])\n",
    "            while len(state0) < self.window_length:\n",
    "                state0.insert(0, zeroed_observation(state0[0]))\n",
    "            action = self.actions[idx - 1]\n",
    "            reward = self.rewards[idx - 1]\n",
    "            terminal1 = self.terminals[idx - 1]\n",
    "\n",
    "            # Okay, now we need to create the follow-up state. This is state0 shifted on timestep\n",
    "            # to the right. Again, we need to be careful to not include an observation from the next\n",
    "            # episode if the last state is terminal.\n",
    "            state1 = [np.copy(x) for x in state0[1:]]\n",
    "            state1.append(self.observations[idx])\n",
    "\n",
    "            assert len(state0) == self.window_length\n",
    "            assert len(state1) == len(state0)\n",
    "            experiences.append(Experience(state0=state0, action=action, reward=reward,\n",
    "                                          state1=state1, terminal1=terminal1))\n",
    "        assert len(experiences) == batch_size\n",
    "        return experiences\n",
    "\n",
    "    def sample_and_split(self, batch_size, batch_idxs=None):\n",
    "        experiences = self.sample(batch_size, batch_idxs)\n",
    "\n",
    "        state0_batch = []\n",
    "        reward_batch = []\n",
    "        action_batch = []\n",
    "        terminal1_batch = []\n",
    "        state1_batch = []\n",
    "        for e in experiences:\n",
    "            state0_batch.append(e.state0)\n",
    "            state1_batch.append(e.state1)\n",
    "            reward_batch.append(e.reward)\n",
    "            action_batch.append(e.action)\n",
    "            terminal1_batch.append(0. if e.terminal1 else 1.)\n",
    "\n",
    "        # Prepare and validate parameters.\n",
    "        state0_batch = np.array(state0_batch).reshape(batch_size,-1)\n",
    "        state1_batch = np.array(state1_batch).reshape(batch_size,-1)\n",
    "        terminal1_batch = np.array(terminal1_batch).reshape(batch_size,-1)\n",
    "        reward_batch = np.array(reward_batch).reshape(batch_size,-1)\n",
    "        action_batch = np.array(action_batch).reshape(batch_size,-1)\n",
    "\n",
    "        return state0_batch, action_batch, reward_batch, state1_batch, terminal1_batch\n",
    "\n",
    "\n",
    "    def append(self, observation, action, reward, terminal, training=True):\n",
    "        super(SequentialMemory, self).append(observation, action, reward, terminal, training=training)\n",
    "\n",
    "        # This needs to be understood as follows: in `observation`, take `action`, obtain `reward`\n",
    "        # and weather the next state is `terminal` or not.\n",
    "        if training:\n",
    "            self.observations.append(observation)\n",
    "            self.actions.append(action)\n",
    "            self.rewards.append(reward)\n",
    "            self.terminals.append(terminal)\n",
    "\n",
    "    @property\n",
    "    def nb_entries(self):\n",
    "        return len(self.observations)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super(SequentialMemory, self).get_config()\n",
    "        config['limit'] = self.limit\n",
    "        return config\n",
    "\n",
    "\n",
    "class EpisodeParameterMemory(Memory):\n",
    "    def __init__(self, limit, **kwargs):\n",
    "        super(EpisodeParameterMemory, self).__init__(**kwargs)\n",
    "        self.limit = limit\n",
    "\n",
    "        self.params = RingBuffer(limit)\n",
    "        self.intermediate_rewards = []\n",
    "        self.total_rewards = RingBuffer(limit)\n",
    "\n",
    "    def sample(self, batch_size, batch_idxs=None):\n",
    "        if batch_idxs is None:\n",
    "            batch_idxs = sample_batch_indexes(0, self.nb_entries, size=batch_size)\n",
    "        assert len(batch_idxs) == batch_size\n",
    "\n",
    "        batch_params = []\n",
    "        batch_total_rewards = []\n",
    "        for idx in batch_idxs:\n",
    "            batch_params.append(self.params[idx])\n",
    "            batch_total_rewards.append(self.total_rewards[idx])\n",
    "        return batch_params, batch_total_rewards\n",
    "\n",
    "    def append(self, observation, action, reward, terminal, training=True):\n",
    "        super(EpisodeParameterMemory, self).append(observation, action, reward, terminal, training=training)\n",
    "        if training:\n",
    "            self.intermediate_rewards.append(reward)\n",
    "\n",
    "    def finalize_episode(self, params):\n",
    "        total_reward = sum(self.intermediate_rewards)\n",
    "        self.total_rewards.append(total_reward)\n",
    "        self.params.append(params)\n",
    "        self.intermediate_rewards = []\n",
    "\n",
    "    @property\n",
    "    def nb_entries(self):\n",
    "        return len(self.total_rewards)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super(SequentialMemory, self).get_config()\n",
    "        config['limit'] = self.limit\n",
    "        return config\n",
    "import numpy as np \n",
    "\n",
    "# [reference] https://github.com/matthiasplappert/keras-rl/blob/master/rl/random.py\n",
    "\n",
    "class RandomProcess(object):\n",
    "    def reset_states(self):\n",
    "        pass\n",
    "\n",
    "class AnnealedGaussianProcess(RandomProcess):\n",
    "    def __init__(self, mu, sigma, sigma_min, n_steps_annealing):\n",
    "        self.mu = mu\n",
    "        self.sigma = sigma\n",
    "        self.n_steps = 0\n",
    "\n",
    "        if sigma_min is not None:\n",
    "            self.m = -float(sigma - sigma_min) / float(n_steps_annealing)\n",
    "            self.c = sigma\n",
    "            self.sigma_min = sigma_min\n",
    "        else:\n",
    "            self.m = 0.\n",
    "            self.c = sigma\n",
    "            self.sigma_min = sigma\n",
    "\n",
    "    @property\n",
    "    def current_sigma(self):\n",
    "        sigma = max(self.sigma_min, self.m * float(self.n_steps) + self.c)\n",
    "        return sigma\n",
    "\n",
    "\n",
    "# Based on http://math.stackexchange.com/questions/1287634/implementing-ornstein-uhlenbeck-in-matlab\n",
    "class OrnsteinUhlenbeckProcess(AnnealedGaussianProcess):\n",
    "    def __init__(self, theta, mu=0., sigma=1., dt=1e-2, x0=None, size=1, sigma_min=None, n_steps_annealing=1000):\n",
    "        super(OrnsteinUhlenbeckProcess, self).__init__(mu=mu, sigma=sigma, sigma_min=sigma_min, n_steps_annealing=n_steps_annealing)\n",
    "        self.theta = theta\n",
    "        self.mu = mu\n",
    "        self.dt = dt\n",
    "        self.x0 = x0\n",
    "        self.size = size\n",
    "        self.reset_states()\n",
    "\n",
    "    def sample(self):\n",
    "        x = self.x_prev + self.theta * (self.mu - self.x_prev) * self.dt + self.current_sigma * np.sqrt(self.dt) * np.random.normal(size=self.size)\n",
    "        self.x_prev = x\n",
    "        self.n_steps += 1\n",
    "        return x\n",
    "\n",
    "    def reset_states(self):\n",
    "        self.x_prev = self.x0 if self.x0 is not None else np.zeros(self.size)\n",
    "# from ipdb import set_trace as debug\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "class DDPG(object):\n",
    "    def __init__(self, nb_states, nb_actions, args2):\n",
    "\n",
    "        if args2.seed > 0:\n",
    "            self.seed(args2.seed)\n",
    "\n",
    "        self.nb_states = nb_states\n",
    "        self.nb_actions= nb_actions\n",
    "\n",
    "        # Create Actor and Critic Network\n",
    "        net_cfg = {\n",
    "            'hidden1':args2.hidden1, \n",
    "            'hidden2':args2.hidden2, \n",
    "            'init_w':args2.init_w\n",
    "        }\n",
    "        self.actor = Actor(self.nb_states, self.nb_actions, **net_cfg)\n",
    "        self.actor_target = Actor(self.nb_states, self.nb_actions, **net_cfg)\n",
    "        self.actor_optim  = Adam(self.actor.parameters(), lr=args2.prate)\n",
    "\n",
    "        self.critic = Critic(self.nb_states, self.nb_actions, **net_cfg)\n",
    "        self.critic_target = Critic(self.nb_states, self.nb_actions, **net_cfg)\n",
    "        self.critic_optim  = Adam(self.critic.parameters(), lr=args2.rate)\n",
    "\n",
    "        hard_update(self.actor_target, self.actor) # Make sure target is with the same weight\n",
    "        hard_update(self.critic_target, self.critic)\n",
    "\n",
    "        #Create replay buffer\n",
    "        self.memory = SequentialMemory(limit=args2.rmsize, window_length=args2.window_length)\n",
    "        self.random_process = OrnsteinUhlenbeckProcess(size=nb_actions, theta=args2.ou_theta, mu=args2.ou_mu, sigma=args2.ou_sigma)\n",
    "\n",
    "        # Hyper-parameters\n",
    "        self.batch_size = args2.bsize\n",
    "        self.tau = args2.tau\n",
    "        self.discount = args2.discount\n",
    "        self.depsilon = 1.0 / args2.epsilon\n",
    "\n",
    "        # \n",
    "        self.epsilon = 1.0\n",
    "        self.s_t = None # Most recent state\n",
    "        self.a_t = None # Most recent action\n",
    "        self.is_training = True\n",
    "\n",
    "        # \n",
    "        if USE_CUDA: self.cuda()\n",
    "\n",
    "    def update_policy(self):\n",
    "        # Sample batch\n",
    "        state_batch, action_batch, reward_batch, \\\n",
    "        next_state_batch, terminal_batch = self.memory.sample_and_split(self.batch_size)\n",
    "\n",
    "        # Prepare for the target q batch\n",
    "        next_q_values = self.critic_target([\n",
    "            to_tensor(next_state_batch, volatile=True),\n",
    "            self.actor_target(to_tensor(next_state_batch, volatile=True)),\n",
    "        ])\n",
    "        next_q_values.volatile=False\n",
    "\n",
    "        target_q_batch = to_tensor(reward_batch) + \\\n",
    "            self.discount*to_tensor(terminal_batch.astype(np.float))*next_q_values\n",
    "\n",
    "        # Critic update\n",
    "        self.critic.zero_grad()\n",
    "\n",
    "        q_batch = self.critic([ to_tensor(state_batch), to_tensor(action_batch) ])\n",
    "\n",
    "        value_loss = criterion(q_batch, target_q_batch)\n",
    "        value_loss.backward()\n",
    "        self.critic_optim.step()\n",
    "\n",
    "        # Actor update\n",
    "        self.actor.zero_grad()\n",
    "\n",
    "        policy_loss = -self.critic([\n",
    "            to_tensor(state_batch),\n",
    "            self.actor(to_tensor(state_batch))\n",
    "        ])\n",
    "\n",
    "        policy_loss = policy_loss.mean()\n",
    "        policy_loss.backward()\n",
    "        self.actor_optim.step()\n",
    "\n",
    "        # Target update\n",
    "        soft_update(self.actor_target, self.actor, self.tau)\n",
    "        soft_update(self.critic_target, self.critic, self.tau)\n",
    "\n",
    "    def eval(self):\n",
    "        self.actor.eval()\n",
    "        self.actor_target.eval()\n",
    "        self.critic.eval()\n",
    "        self.critic_target.eval()\n",
    "\n",
    "    def cuda(self):\n",
    "        self.actor.cuda()\n",
    "        self.actor_target.cuda()\n",
    "        self.critic.cuda()\n",
    "        self.critic_target.cuda()\n",
    "\n",
    "    def observe(self, r_t, s_t1, done):\n",
    "        if self.is_training:\n",
    "            self.memory.append(self.s_t, self.a_t, r_t, done)\n",
    "            self.s_t = s_t1\n",
    "\n",
    "    def random_action(self):\n",
    "        action = np.random.uniform(0.,1.,self.nb_actions)\n",
    "        self.a_t = action\n",
    "        return action\n",
    "\n",
    "    def select_action(self, s_t, decay_epsilon=True):\n",
    "        action = to_numpy(\n",
    "            self.actor(to_tensor(np.array([s_t])))\n",
    "        ).squeeze(0)\n",
    "        #action += self.is_training*max(self.epsilon, 0)*self.random_process.sample()\n",
    "        action = np.clip(action, 0., 1.)\n",
    "\n",
    "        if decay_epsilon:\n",
    "            self.epsilon -= self.depsilon\n",
    "\n",
    "        self.a_t = action\n",
    "\n",
    "        return action\n",
    "\n",
    "    def reset(self, obs):\n",
    "        self.s_t = obs\n",
    "        self.random_process.reset_states()\n",
    "\n",
    "    def load_weights(self, output):\n",
    "        if output is None: return\n",
    "\n",
    "        self.actor.load_state_dict(\n",
    "            torch.load('{}/actor.pkl'.format(output))\n",
    "        )\n",
    "\n",
    "        self.critic.load_state_dict(\n",
    "            torch.load('{}/critic.pkl'.format(output))\n",
    "        )\n",
    "\n",
    "\n",
    "    def save_model(self,output):\n",
    "        torch.save(\n",
    "            self.actor.state_dict(),\n",
    "            '{}/actor.pkl'.format(output)\n",
    "        )\n",
    "        torch.save(\n",
    "            self.critic.state_dict(),\n",
    "            '{}/critic.pkl'.format(output)\n",
    "        )\n",
    "\n",
    "    def seed(self,s):\n",
    "        torch.manual_seed(s)\n",
    "        if USE_CUDA:\n",
    "            torch.cuda.manual_seed(s)\n",
    "import pyflann\n",
    "from gym.spaces import Box\n",
    "import numpy as np\n",
    "import itertools\n",
    "\n",
    "class Space:\n",
    "\n",
    "    def __init__(self, low, high, points):\n",
    "\n",
    "        self._low = np.array(low)\n",
    "        self._high = np.array(high)\n",
    "        self._range = self._high - self._low\n",
    "        self._dimensions = len(low)\n",
    "        self.__space = init_uniform_space([0] * self._dimensions,\n",
    "                                          [1] * self._dimensions,\n",
    "                                          points)\n",
    "        self._flann = pyflann.FLANN()\n",
    "        self.rebuild_flann()\n",
    "\n",
    "    def rebuild_flann(self):\n",
    "        self._index = self._flann.build_index(self.__space, algorithm='kdtree')\n",
    "        # print(\"Index type: {}\".format(type(self._index)))\n",
    "\n",
    "    def search_point(self, point, k):\n",
    "        p_in = self.import_point(point).reshape(1, -1).astype('float64')\n",
    "        # print(\"p_in: {}, p_in.shape: {}, p_in.dtype: {}\".format(p_in, p_in.shape, p_in.dtype))\n",
    "        search_res, _ = self._flann.nn_index(p_in, k)\n",
    "        knns = self.__space[search_res]\n",
    "        p_out = []\n",
    "        for p in knns:\n",
    "            p_out.append(self.export_point(p))\n",
    "\n",
    "        if k == 1:\n",
    "            p_out = [p_out]\n",
    "        #print(np.array(p_out),'-------------------')\n",
    "        return np.array(p_out)\n",
    "\n",
    "    def import_point(self, point):\n",
    "        return (point - self._low) / self._range\n",
    "\n",
    "    def export_point(self, point):\n",
    "        return self._low + point * self._range\n",
    "\n",
    "    def get_space(self):\n",
    "        return self.__space\n",
    "\n",
    "    def shape(self):\n",
    "        return self.__space.shape\n",
    "\n",
    "    def get_number_of_actions(self):\n",
    "        return self.shape()[0]\n",
    "\n",
    "    def plot_space(self, additional_points=None):\n",
    "\n",
    "        dims = self._dimensions\n",
    "\n",
    "        if dims > 3:\n",
    "            print(\n",
    "                'Cannot plot a {}-dimensional space. Max 3 dimensions'.format(dims))\n",
    "            return\n",
    "\n",
    "        space = self.get_space()\n",
    "        if additional_points is not None:\n",
    "            for i in additional_points:\n",
    "                space = np.append(space, additional_points, axis=0)\n",
    "\n",
    "        if dims == 1:\n",
    "            for x in space:\n",
    "                plt.plot([x], [0], 'o')\n",
    "\n",
    "            plt.show()\n",
    "        elif dims == 2:\n",
    "            for x, y in space:\n",
    "                plt.plot([x], [y], 'o')\n",
    "\n",
    "            plt.show()\n",
    "        else:\n",
    "            plot_3d_points(space)\n",
    "\n",
    "\n",
    "class Discrete_space(Space):\n",
    "    \"\"\"\n",
    "        Discrete action space with n actions (the integers in the range [0, n))\n",
    "        0, 1, 2, ..., n-2, n-1\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n):  # n: the number of the discrete actions\n",
    "        super().__init__([0], [n - 1], n)\n",
    "\n",
    "    def export_point(self, point):\n",
    "        return super().export_point(point).astype(int)\n",
    "\n",
    "\n",
    "def init_uniform_space(low, high, points):\n",
    "    dims = len(low)\n",
    "    points_in_each_axis = round(points**(1 / dims))\n",
    "\n",
    "    axis = []\n",
    "    for i in range(dims):\n",
    "        axis.append(list(np.linspace(low[i], high[i], points_in_each_axis)))\n",
    "    print('points_in_each_axis')\n",
    "    space = []\n",
    "    for _ in itertools.product(*axis):\n",
    "        print('points_in_each_axis')\n",
    "        space.append(list(_))\n",
    "    print('points_in_each_axis')\n",
    "    return np.array(space)\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "class WOLPAgent(object):\n",
    "    def __init__(self, nb_states, nb_actions, args2):\n",
    "\n",
    "        if args2.seed > 0:\n",
    "            self.seed(args2.seed)\n",
    "\n",
    "        self.nb_states = nb_states\n",
    "        self.nb_actions= nb_actions\n",
    "\n",
    "        # Create Actor and Critic Network\n",
    "        net_cfg = {\n",
    "            'hidden1':args2.hidden1, \n",
    "            'hidden2':args2.hidden2, \n",
    "            'init_w':args2.init_w\n",
    "        }\n",
    "\n",
    "        ################################## Our Code Start ################################################\n",
    "        self.low = args2.low\n",
    "        self.high = args2.high\n",
    "        self.action_space = spaces.Box(\n",
    "            low=0,\n",
    "            high=1,\n",
    "            shape=(args.solDim,)\n",
    "        )\n",
    "        self.k_nearest_neighbors = max(1, int(args2.max_actions * args2.k_ratio))\n",
    "        ################################## Our Code End ################################################        \n",
    "\n",
    "        self.actor = Actor(self.nb_states, self.nb_actions, **net_cfg)\n",
    "        self.actor_target = Actor(self.nb_states, self.nb_actions, **net_cfg)\n",
    "        self.actor_optim  = Adam(self.actor.parameters(), lr=args2.prate)\n",
    "        self.critic = Critic(self.nb_states, self.nb_actions, **net_cfg)\n",
    "        self.critic_target = Critic(self.nb_states, self.nb_actions, **net_cfg)\n",
    "        self.critic_optim  = Adam(self.critic.parameters(), lr=args2.rate)\n",
    "\n",
    "        hard_update(self.actor_target, self.actor) # Make sure target is with the same weight\n",
    "        hard_update(self.critic_target, self.critic)\n",
    "\n",
    "        #Create replay buffer\n",
    "        self.memory = SequentialMemory(limit=args2.rmsize, window_length=args2.window_length)\n",
    "        self.random_process = OrnsteinUhlenbeckProcess(size=nb_actions, theta=args2.ou_theta, mu=args2.ou_mu, sigma=args2.ou_sigma)\n",
    "\n",
    "        # Hyper-parameters\n",
    "        self.batch_size = args2.bsize\n",
    "        self.tau = args2.tau\n",
    "        self.discount = args2.discount\n",
    "        self.depsilon = 1.0 / args2.epsilon\n",
    "\n",
    "        # \n",
    "        self.epsilon = 1.0\n",
    "        self.s_t = None # Most recent state\n",
    "        self.a_t = None # Most recent action\n",
    "        self.is_training = True\n",
    "\n",
    "        # \n",
    "        if USE_CUDA: self.cuda()\n",
    "\n",
    "    def get_action_space(self):\n",
    "        return self.action_space    \n",
    "\n",
    "    def update_policy(self):\n",
    "        # Sample batch\n",
    "        state_batch, action_batch, reward_batch, \\\n",
    "        next_state_batch, terminal_batch = self.memory.sample_and_split(self.batch_size)\n",
    "\n",
    "        # Prepare for the target q batch\n",
    "        next_q_values = self.critic_target([\n",
    "            to_tensor(next_state_batch, volatile=True),\n",
    "            self.actor_target(to_tensor(next_state_batch, volatile=True)),\n",
    "        ])\n",
    "        next_q_values.volatile=False\n",
    "\n",
    "        target_q_batch = to_tensor(reward_batch) + \\\n",
    "            self.discount*to_tensor(terminal_batch.astype(np.float))*next_q_values\n",
    "\n",
    "        # Critic update\n",
    "        self.critic.zero_grad()\n",
    "        q_batch = self.critic([ to_tensor(state_batch), to_tensor(action_batch) ])\n",
    "\n",
    "        value_loss = criterion(q_batch, target_q_batch)\n",
    "        value_loss.backward()\n",
    "        self.critic_optim.step()\n",
    "\n",
    "        # Actor update\n",
    "        self.actor.zero_grad()\n",
    "\n",
    "        policy_loss = -self.critic([\n",
    "            to_tensor(state_batch),\n",
    "            self.actor(to_tensor(state_batch))\n",
    "        ])\n",
    "\n",
    "        policy_loss = policy_loss.mean()\n",
    "        policy_loss.backward()\n",
    "        self.actor_optim.step()\n",
    "\n",
    "        # Target update\n",
    "        soft_update(self.actor_target, self.actor, self.tau)\n",
    "        soft_update(self.critic_target, self.critic, self.tau)\n",
    "\n",
    "    def eval(self):\n",
    "        self.actor.eval()\n",
    "        self.actor_target.eval()\n",
    "        self.critic.eval()\n",
    "        self.critic_target.eval()\n",
    "\n",
    "    def cuda(self):\n",
    "        self.actor.cuda()\n",
    "        self.actor_target.cuda()\n",
    "        self.critic.cuda()\n",
    "        self.critic_target.cuda()\n",
    "\n",
    "    def observe(self, r_t, s_t1, done):\n",
    "        if self.is_training:\n",
    "            self.memory.append(self.s_t, self.a_t, r_t, done)\n",
    "            self.s_t = s_t1\n",
    "\n",
    "    def random_action(self):\n",
    "        action = np.random.uniform(0.,1.,self.nb_actions)\n",
    "        self.a_t = action\n",
    "        return action\n",
    "\n",
    "    def select_action(self, s_t, decay_epsilon=True):\n",
    "        proto_action = self.ddpg_select_action(s_t, decay_epsilon=decay_epsilon)\n",
    "        # print(\"Proto action: {}, proto action.shape: {}\".format(proto_action, proto_action.shape))\n",
    "        #print(sum(proto_action>0.5))\n",
    "        actions = proto_action\n",
    "        # print(\"len(actions): {}\".format(len(actions)))\n",
    "        states = np.array([[0.1]*args.solDim])\n",
    "\n",
    "        a = [to_tensor(states), to_tensor(actions)]\n",
    "        # print(\"states: {}, actions: {}\".format(a[0].size(), a[1].size()))\n",
    "        actions_evaluation = self.critic([to_tensor(states), to_tensor(actions)])\n",
    "        # print(\"actions_evaluation: {}, actions_evaluation.size(): {}\".format(actions_evaluation, actions_evaluation.size()))\n",
    "        actions_evaluation_np = actions_evaluation.detach().cpu().numpy()\n",
    "        max_index = np.argmax(actions_evaluation_np)\n",
    "\n",
    "        self.a_t = actions[max_index]\n",
    "        return self.a_t\n",
    "\n",
    "    def ddpg_select_action(self, s_t, decay_epsilon=True):\n",
    "        action = to_numpy(\n",
    "            self.actor(to_tensor(np.array([s_t])))\n",
    "        ).squeeze(0)\n",
    "        #action += self.is_training*max(self.epsilon, 0)*self.random_process.sample()\n",
    "        action = np.clip(action, 0., 1.)\n",
    "        if decay_epsilon:\n",
    "            self.epsilon -= self.depsilon\n",
    "\n",
    "        return action\n",
    "\n",
    "    def reset(self, obs):\n",
    "        self.s_t = obs\n",
    "        self.random_process.reset_states()\n",
    "\n",
    "    def load_weights(self, output):\n",
    "        if output is None: return\n",
    "\n",
    "        self.actor.load_state_dict(\n",
    "            torch.load('{}/actor.pkl'.format(output))\n",
    "        )\n",
    "\n",
    "        self.critic.load_state_dict(\n",
    "            torch.load('{}/critic.pkl'.format(output))\n",
    "        )\n",
    "\n",
    "\n",
    "    def save_model(self,output):\n",
    "        torch.save(\n",
    "            self.actor.state_dict(),\n",
    "            '{}/actor.pkl'.format(output)\n",
    "        )\n",
    "        torch.save(\n",
    "            self.critic.state_dict(),\n",
    "            '{}/critic.pkl'.format(output)\n",
    "        )\n",
    "\n",
    "    def seed(self,s):\n",
    "        torch.manual_seed(s)\n",
    "        if USE_CUDA:\n",
    "            torch.cuda.manual_seed(s)\n",
    "import math\n",
    "import gym\n",
    "from gym import spaces, logger\n",
    "from gym.utils import seeding\n",
    "import numpy as np\n",
    "\n",
    "def test(num_episodes, agent, env2, evaluate, model_path, visualize=True, debug=False):\n",
    "\n",
    "    agent.load_weights(model_path)\n",
    "    agent.is_training = False\n",
    "    agent.eval()\n",
    "    policy = lambda x: agent.select_action(x, decay_epsilon=False)\n",
    "\n",
    "    for i in range(num_episodes):\n",
    "        validate_reward = evaluate(env2, policy, debug=debug, visualize=visualize, save=False)\n",
    "class Arguments(object):\n",
    "    def __init__(self):\n",
    "        self.mode = 'train'\n",
    "        self.env2 = \"InvertedPendulum-v2\"\n",
    "        self.hidden1 = 400\n",
    "        self.hidden2 = 300\n",
    "        self.rate = 0.001\n",
    "        self.prate = 0.0001\n",
    "        self.warmup = 100\n",
    "        self.discount = 0.99\n",
    "        self.bsize = 64\n",
    "        self.rmsize = 6000000\n",
    "        self.window_length = 1\n",
    "        self.tau = 0.001\n",
    "        self.ou_theta = 0.15\n",
    "        self.ou_sigma = 0.2\n",
    "        self.ou_mu = 0.0\n",
    "        self.validate_episodes = 20\n",
    "        self.max_episode_length = 500\n",
    "        self.validate_steps = 50000\n",
    "        self.output = 'output'\n",
    "        self.debug='debug'\n",
    "        self.init_w = 0.03\n",
    "        self.train_iter=5000\n",
    "        self.epsilon=50000\n",
    "        self.seed=-1\n",
    "        self.max_actions=1e6\n",
    "        self.resume='default'\n",
    "        self.k_ratio = 1e-6\n",
    "\n",
    "args2 = Arguments()\n",
    "args2.output = get_output_folder(args2.output, args2.env2)\n",
    "if args2.resume == 'default':\n",
    "    args2.resume = 'output/{}-run0'.format(args2.env2)\n",
    "class myenv2(gym.Env):\n",
    "    metadata = {\n",
    "        'render.modes': ['human', 'rgb_array'],\n",
    "        'video.frames_per_second': 50\n",
    "    }\n",
    "\n",
    "    def __init__(self):\n",
    "\n",
    "        # Angle limit set to 2 * theta_threshold_radians so failing observation\n",
    "        # is still within bounds\n",
    "\n",
    "        self.action_space =  spaces.Box(\n",
    "            low=0,\n",
    "            high=1,\n",
    "            shape=(args.solDim,)\n",
    "        )\n",
    "        self.observation_space =  spaces.Box(\n",
    "            low=0,\n",
    "            high=1,\n",
    "            shape=(args.solDim,)\n",
    "        )\n",
    "\n",
    "        self.seed()\n",
    "        self.viewer = None\n",
    "        self.state = None\n",
    "\n",
    "        self.steps_beyond_done = None\n",
    "\n",
    "        self.steps = torch.tensor(np.array([0.1]*args.solDim))\n",
    "        self._max_episode_steps = 500\n",
    "        self.iteration=1\n",
    "        self.constraints=[]\n",
    "        self.constraint=0\n",
    "        self.history=np.ones(args.solDim)\n",
    "    def seed(self, seed=None):\n",
    "        self.np_random, seed = seeding.np_random(seed)\n",
    "        return [seed]\n",
    "\n",
    "    def step(self, action):\n",
    "        self.iteration+=1 \n",
    "        done=1\n",
    "        action=np.array([1 if i in np.argsort(action)[-args.card:] else 0  for i in range(args.solDim)])\n",
    "        #print(action,checkFea(action))\n",
    "  #      if sum(action)<10:\n",
    "   #         reward=100*( self.a.dot(action))**2+hwithoutconstraints(action-100*(sum(action)-10)**2+100000#+ 100*(1/(1+np.exp(-self.iteration))-0.5)*torch.log(torch.mean(wmc)).float()#0*(1/(1+np.exp(-self.iteration))-0.5)*torch.log(torch.mean(wmc)).float()+sum(action)#100*( self.a.dot(action))**2+hwithoutconstraints(action\n",
    "    #    else:\n",
    "        #( self.a.dot(action))**2+hwithoutconstraints(action\n",
    "        #reward=(self.a.dot(action))**2+hwithoutconstraints(action-self.constraint*checkFea(action)##0*(1/(1+np.exp(-self.iteration))-0.5)*torch.log(torch.mean(wmc)).float()+sum(action)#100*( self.a.dot(action))**2+hwithoutconstraints(action\n",
    "        reward=hwithoutconstraints(action)-10*checkFea(action)##0*(1/(1+np.exp(-self.iteration))-0.5)*torch.log(torch.mean(wmc)).float()+sum(action)#100*( self.a.dot(action))**2+hwithoutconstraints(action\n",
    "        rewards.append(hwithoutconstraints(action))\n",
    "        constraints.append(checkFea(action))\n",
    "        self.history+=np.array(action)\n",
    "        print(self.iteration,1*hwithoutconstraints(action),checkFea(action))\n",
    "        #print(( self.a.dot(action))**2+hwithoutconstraints(action,checkFea(action),torch.log(torch.mean(wmc)))\n",
    "        return action, reward, done, {}\n",
    "\n",
    "    def reset(self):\n",
    "        return  np.array(np.array([0.1]*args.solDim)),\n",
    "\n",
    "    def render(self, mode='human'):\n",
    "        pass\n",
    "\n",
    "\n",
    "\n",
    "# env2 = Normalizedenv2(gym.make(args2.env2))\n",
    "# env2 = gym.make(args2.env2)\n",
    "env2 = myenv2()\n",
    "#################################### Our Code ##############################\n",
    "args2.low = env2.action_space.low\n",
    "args2.high = env2.action_space.high\n",
    "#################################### Our Code ##############################\n",
    "\n",
    "if args2.seed > 0:\n",
    "    np.random.seed(args2.seed)\n",
    "    env2.seed(args2.seed)\n",
    "\n",
    "nb_states =args.solDim\n",
    "nb_actions =args.solDim\n",
    "#yahoo data with synthetic feedback\n",
    "agent = WOLPAgent(nb_states, nb_actions, args2)\n",
    "\n",
    "evaluate = Evaluator(args2.validate_episodes, \n",
    "    args2.validate_steps, args2.output, max_episode_length=args2.max_episode_length)\n",
    "num_iterations=args2.train_iter\n",
    "validate_steps=args2.validate_steps\n",
    "output= args2.output\n",
    "max_episode_length=args2.max_episode_length\n",
    "debug=args2.debug\n",
    "agent.is_training = True\n",
    "step = episode = episode_steps = 0\n",
    "episode_reward = 0.\n",
    "observation = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "while step < num_iterations:\n",
    "    # reset if it is the start of episode\n",
    "    if observation is None:\n",
    "        observation = deepcopy(env2.reset())\n",
    "        agent.reset(observation)\n",
    "\n",
    "    # agent pick action ...        \n",
    "\n",
    "    if step <= args2.warmup:\n",
    "        action = agent.random_action()\n",
    "    else:\n",
    "        action = agent.select_action(observation)\n",
    "\n",
    "    # env2 response with next_observation, reward, terminate_info\n",
    "    observation2, reward, done, info = env2.step(action)\n",
    "    observation2 = deepcopy(observation2)\n",
    "    if max_episode_length and episode_steps >= max_episode_length -1:\n",
    "        done = True\n",
    "\n",
    "    # agent observe and update policy\n",
    "    agent.observe(reward, observation2, done)\n",
    "    if step > args2.warmup:\n",
    "        agent.update_policy()\n",
    "\n",
    "    # [optional] evaluate\n",
    "    if evaluate is not None and validate_steps > 0 and step % validate_steps == 0:\n",
    "        policy = lambda x: agent.select_action(x, decay_epsilon=False)\n",
    "        validate_reward = evaluate(env2, policy, debug=False, visualize=False)\n",
    "\n",
    "    # [optional] save intermideate model\n",
    "    if step % int(num_iterations/3) == 0:\n",
    "        agent.save_model(output)\n",
    "\n",
    "    # update \n",
    "    step += 1\n",
    "    episode_steps += 1\n",
    "    episode_reward += reward\n",
    "    observation = deepcopy(observation2)\n",
    "    \n",
    "    if done: # end of episode\n",
    "\n",
    "        agent.memory.append(\n",
    "            observation,\n",
    "            agent.select_action(observation),\n",
    "            0., False\n",
    "        )\n",
    "\n",
    "        # reset\n",
    "        observation = None\n",
    "        episode_steps = 0\n",
    "        episode_reward = 0.\n",
    "        episode += 1\n",
    "    if step%1000==0:\n",
    "        np.save('reward_rl_realorderml.npy',rewards)\n",
    "        np.save('constraint_rl_realorderml.npy',constraints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
